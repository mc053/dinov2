submitit INFO (2024-12-05 08:13:22,485) - Starting with JobEnvironment(job_id=2997507, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-05 08:13:22,485) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset/2997507_submitted.pkl
I20241205 08:13:30 2997508 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 08:13:30 2997508 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_pixelated_C/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAPixelatedTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 08:13:30 2997508 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 08:13:30 2997508 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 08:13:31 2997508 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 08:14:00 2997508 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 08:14:05 2997508 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 08:14:05 2997508 dinov2 loaders.py:94] using dataset: "CelebAPixelatedTrain"
Load image list
I20241205 08:14:12 2997508 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 08:14:12 2997508 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 08:14:15 2997508 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 08:14:15 2997508 dinov2 knn.py:260] Extracting features for train set...
I20241205 08:14:15 2997508 dinov2 loaders.py:157] sampler: distributed
I20241205 08:14:15 2997508 dinov2 loaders.py:216] using PyTorch data loader
W20241205 08:14:15 2997508 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 08:14:15 2997508 dinov2 loaders.py:229] # of batches: 634
I20241205 08:14:50 2997508 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 08:14:50 2997508 dinov2 helpers.py:102]   [  0/634]  eta: 6:09:27    time: 34.963905  data: 13.511917  max mem: 3463
I20241205 08:15:01 2997508 dinov2 helpers.py:102]   [ 10/634]  eta: 0:43:30    time: 4.183193  data: 1.230064  max mem: 4109
I20241205 08:15:21 2997508 dinov2 helpers.py:102]   [ 20/634]  eta: 0:32:14    time: 1.559197  data: 0.001190  max mem: 4109
I20241205 08:15:59 2997508 dinov2 helpers.py:102]   [ 30/634]  eta: 0:33:50    time: 2.909598  data: 0.000681  max mem: 4109
I20241205 08:16:38 2997508 dinov2 helpers.py:102]   [ 40/634]  eta: 0:34:41    time: 3.876412  data: 0.001674  max mem: 4109
I20241205 08:17:18 2997508 dinov2 helpers.py:102]   [ 50/634]  eta: 0:34:58    time: 3.951838  data: 0.001994  max mem: 4109
I20241205 08:17:58 2997508 dinov2 helpers.py:102]   [ 60/634]  eta: 0:34:58    time: 3.965232  data: 0.001164  max mem: 4109
I20241205 08:18:38 2997508 dinov2 helpers.py:102]   [ 70/634]  eta: 0:34:48    time: 3.982017  data: 0.001418  max mem: 4109
I20241205 08:19:18 2997508 dinov2 helpers.py:102]   [ 80/634]  eta: 0:34:31    time: 3.991523  data: 0.001826  max mem: 4109
I20241205 08:19:58 2997508 dinov2 helpers.py:102]   [ 90/634]  eta: 0:34:08    time: 3.991118  data: 0.001214  max mem: 4109
I20241205 08:20:37 2997508 dinov2 helpers.py:102]   [100/634]  eta: 0:33:42    time: 3.989847  data: 0.000637  max mem: 4109
I20241205 08:21:17 2997508 dinov2 helpers.py:102]   [110/634]  eta: 0:33:14    time: 3.991325  data: 0.000731  max mem: 4109
I20241205 08:21:57 2997508 dinov2 helpers.py:102]   [120/634]  eta: 0:32:44    time: 3.992509  data: 0.001652  max mem: 4109
I20241205 08:22:37 2997508 dinov2 helpers.py:102]   [130/634]  eta: 0:32:12    time: 3.990705  data: 0.001919  max mem: 4109
I20241205 08:23:17 2997508 dinov2 helpers.py:102]   [140/634]  eta: 0:31:39    time: 3.986077  data: 0.001175  max mem: 4109
I20241205 08:23:57 2997508 dinov2 helpers.py:102]   [150/634]  eta: 0:31:05    time: 3.982414  data: 0.000743  max mem: 4109
I20241205 08:24:37 2997508 dinov2 helpers.py:102]   [160/634]  eta: 0:30:30    time: 3.982434  data: 0.000736  max mem: 4109
I20241205 08:25:16 2997508 dinov2 helpers.py:102]   [170/634]  eta: 0:29:55    time: 3.981673  data: 0.001445  max mem: 4109
I20241205 08:25:56 2997508 dinov2 helpers.py:102]   [180/634]  eta: 0:29:19    time: 3.977776  data: 0.001463  max mem: 4109
I20241205 08:26:36 2997508 dinov2 helpers.py:102]   [190/634]  eta: 0:28:42    time: 3.975795  data: 0.000981  max mem: 4109
I20241205 08:27:16 2997508 dinov2 helpers.py:102]   [200/634]  eta: 0:28:06    time: 3.979696  data: 0.000986  max mem: 4109
I20241205 08:27:56 2997508 dinov2 helpers.py:102]   [210/634]  eta: 0:27:29    time: 3.980502  data: 0.000829  max mem: 4109
I20241205 08:28:35 2997508 dinov2 helpers.py:102]   [220/634]  eta: 0:26:51    time: 3.976066  data: 0.000666  max mem: 4109
I20241205 08:29:15 2997508 dinov2 helpers.py:102]   [230/634]  eta: 0:26:14    time: 3.974417  data: 0.000825  max mem: 4109
I20241205 08:29:55 2997508 dinov2 helpers.py:102]   [240/634]  eta: 0:25:36    time: 3.976998  data: 0.001934  max mem: 4109
I20241205 08:30:35 2997508 dinov2 helpers.py:102]   [250/634]  eta: 0:24:59    time: 3.978610  data: 0.002628  max mem: 4109
I20241205 08:31:14 2997508 dinov2 helpers.py:102]   [260/634]  eta: 0:24:20    time: 3.976078  data: 0.002747  max mem: 4109
I20241205 08:31:54 2997508 dinov2 helpers.py:102]   [270/634]  eta: 0:23:42    time: 3.977194  data: 0.002001  max mem: 4109
I20241205 08:32:34 2997508 dinov2 helpers.py:102]   [280/634]  eta: 0:23:04    time: 3.976934  data: 0.000989  max mem: 4109
I20241205 08:33:14 2997508 dinov2 helpers.py:102]   [290/634]  eta: 0:22:26    time: 3.975792  data: 0.000883  max mem: 4109
I20241205 08:33:54 2997508 dinov2 helpers.py:102]   [300/634]  eta: 0:21:47    time: 3.978626  data: 0.001886  max mem: 4109
I20241205 08:34:33 2997508 dinov2 helpers.py:102]   [310/634]  eta: 0:21:09    time: 3.977092  data: 0.002086  max mem: 4109
I20241205 08:35:13 2997508 dinov2 helpers.py:102]   [320/634]  eta: 0:20:30    time: 3.979762  data: 0.001012  max mem: 4109
I20241205 08:35:53 2997508 dinov2 helpers.py:102]   [330/634]  eta: 0:19:52    time: 3.979732  data: 0.000855  max mem: 4109
I20241205 08:36:33 2997508 dinov2 helpers.py:102]   [340/634]  eta: 0:19:13    time: 3.977034  data: 0.001173  max mem: 4109
I20241205 08:37:12 2997508 dinov2 helpers.py:102]   [350/634]  eta: 0:18:34    time: 3.978651  data: 0.002066  max mem: 4109
I20241205 08:37:52 2997508 dinov2 helpers.py:102]   [360/634]  eta: 0:17:55    time: 3.976192  data: 0.001884  max mem: 4109
I20241205 08:38:32 2997508 dinov2 helpers.py:102]   [370/634]  eta: 0:17:16    time: 3.976219  data: 0.001367  max mem: 4109
I20241205 08:39:12 2997508 dinov2 helpers.py:102]   [380/634]  eta: 0:16:37    time: 3.977759  data: 0.001288  max mem: 4109
I20241205 08:39:51 2997508 dinov2 helpers.py:102]   [390/634]  eta: 0:15:58    time: 3.976048  data: 0.000801  max mem: 4109
I20241205 08:40:31 2997508 dinov2 helpers.py:102]   [400/634]  eta: 0:15:19    time: 3.975962  data: 0.000911  max mem: 4109
I20241205 08:41:11 2997508 dinov2 helpers.py:102]   [410/634]  eta: 0:14:40    time: 3.977758  data: 0.000915  max mem: 4109
I20241205 08:41:51 2997508 dinov2 helpers.py:102]   [420/634]  eta: 0:14:01    time: 3.976059  data: 0.001080  max mem: 4109
I20241205 08:42:31 2997508 dinov2 helpers.py:102]   [430/634]  eta: 0:13:22    time: 3.976913  data: 0.001499  max mem: 4109
I20241205 08:43:10 2997508 dinov2 helpers.py:102]   [440/634]  eta: 0:12:43    time: 3.976996  data: 0.001314  max mem: 4109
I20241205 08:43:50 2997508 dinov2 helpers.py:102]   [450/634]  eta: 0:12:04    time: 3.977002  data: 0.001366  max mem: 4109
I20241205 08:44:30 2997508 dinov2 helpers.py:102]   [460/634]  eta: 0:11:25    time: 3.978578  data: 0.001567  max mem: 4109
I20241205 08:45:10 2997508 dinov2 helpers.py:102]   [470/634]  eta: 0:10:45    time: 3.977563  data: 0.001330  max mem: 4109
I20241205 08:45:50 2997508 dinov2 helpers.py:102]   [480/634]  eta: 0:10:06    time: 3.983104  data: 0.001433  max mem: 4109
I20241205 08:46:29 2997508 dinov2 helpers.py:102]   [490/634]  eta: 0:09:27    time: 3.982431  data: 0.002232  max mem: 4109
I20241205 08:47:09 2997508 dinov2 helpers.py:102]   [500/634]  eta: 0:08:48    time: 3.977009  data: 0.001793  max mem: 4109
I20241205 08:47:49 2997508 dinov2 helpers.py:102]   [510/634]  eta: 0:08:08    time: 3.978725  data: 0.001124  max mem: 4109
I20241205 08:48:29 2997508 dinov2 helpers.py:102]   [520/634]  eta: 0:07:29    time: 3.980773  data: 0.001578  max mem: 4109
I20241205 08:49:08 2997508 dinov2 helpers.py:102]   [530/634]  eta: 0:06:50    time: 3.979757  data: 0.001482  max mem: 4109
I20241205 08:49:48 2997508 dinov2 helpers.py:102]   [540/634]  eta: 0:06:10    time: 3.978637  data: 0.001232  max mem: 4109
I20241205 08:50:28 2997508 dinov2 helpers.py:102]   [550/634]  eta: 0:05:31    time: 3.981346  data: 0.001017  max mem: 4109
I20241205 08:51:08 2997508 dinov2 helpers.py:102]   [560/634]  eta: 0:04:51    time: 3.983192  data: 0.001532  max mem: 4109
I20241205 08:51:48 2997508 dinov2 helpers.py:102]   [570/634]  eta: 0:04:12    time: 3.979718  data: 0.001700  max mem: 4109
I20241205 08:52:28 2997508 dinov2 helpers.py:102]   [580/634]  eta: 0:03:33    time: 3.981408  data: 0.001098  max mem: 4109
I20241205 08:53:07 2997508 dinov2 helpers.py:102]   [590/634]  eta: 0:02:53    time: 3.983345  data: 0.000897  max mem: 4109
I20241205 08:53:47 2997508 dinov2 helpers.py:102]   [600/634]  eta: 0:02:14    time: 3.983473  data: 0.001552  max mem: 4109
I20241205 08:54:27 2997508 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.987842  data: 0.002324  max mem: 4109
I20241205 08:55:07 2997508 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.986013  data: 0.001473  max mem: 4109
I20241205 08:55:46 2997508 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.939909  data: 0.001058  max mem: 4109
I20241205 08:56:05 2997508 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.313529  data: 0.000979  max mem: 4109
I20241205 08:56:06 2997508 dinov2 helpers.py:130]  Total time: 0:41:50 (3.960262 s / it)
I20241205 08:56:06 2997508 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:56:06 2997508 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:56:07 2997508 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:56:07 2997508 dinov2 loaders.py:157] sampler: distributed
I20241205 08:56:07 2997508 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:56:07 2997508 dinov2 loaders.py:229] # of batches: 78
I20241205 08:56:07 2997508 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:56:16 2997508 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:02    time: 8.494856  data: 4.401136  max mem: 4109
I20241205 08:56:55 2997508 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:55    time: 4.345808  data: 0.406395  max mem: 4109
I20241205 08:57:29 2997508 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:46    time: 3.673221  data: 0.010056  max mem: 4109
I20241205 08:58:09 2997508 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:08    time: 3.695369  data: 0.009390  max mem: 4109
I20241205 08:58:49 2997508 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:30    time: 3.996335  data: 0.006245  max mem: 4109
I20241205 08:59:29 2997508 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:50    time: 4.016684  data: 0.008186  max mem: 4109
I20241205 09:00:09 2997508 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 4.011958  data: 0.009045  max mem: 4109
I20241205 09:00:47 2997508 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 3.896778  data: 0.005654  max mem: 4109
I20241205 09:01:10 2997508 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.630230  data: 0.002613  max mem: 4109
I20241205 09:01:10 2997508 dinov2 helpers.py:130] Test: Total time: 0:05:02 (3.880752 s / it)
I20241205 09:01:10 2997508 dinov2 utils.py:79] Averaged stats: 
I20241205 09:01:11 2997508 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 84.32
I20241205 09:01:11 2997508 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 84.73
I20241205 09:01:11 2997508 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.05
I20241205 09:01:11 2997508 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.08
submitit INFO (2024-12-05 09:01:11,774) - Job completed successfully
I20241205 09:01:11 2997508 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 09:01:11,776) - Exiting after successful completion
I20241205 09:01:11 2997508 submitit submission.py:61] Exiting after successful completion
