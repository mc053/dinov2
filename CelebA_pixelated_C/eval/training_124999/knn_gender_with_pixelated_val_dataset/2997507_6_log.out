submitit INFO (2024-12-05 08:13:22,484) - Starting with JobEnvironment(job_id=2997507, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-05 08:13:22,484) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset/2997507_submitted.pkl
I20241205 08:13:31 2997514 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 08:13:31 2997514 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_pixelated_C/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAPixelatedTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 08:13:31 2997514 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 08:13:31 2997514 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 08:13:31 2997514 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 08:14:06 2997514 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 08:14:11 2997514 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 08:14:12 2997514 dinov2 loaders.py:94] using dataset: "CelebAPixelatedTrain"
Load image list
I20241205 08:14:26 2997514 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 08:14:26 2997514 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 08:14:32 2997514 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 08:14:32 2997514 dinov2 knn.py:260] Extracting features for train set...
I20241205 08:14:32 2997514 dinov2 loaders.py:157] sampler: distributed
I20241205 08:14:32 2997514 dinov2 loaders.py:216] using PyTorch data loader
W20241205 08:14:32 2997514 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 08:14:32 2997514 dinov2 loaders.py:229] # of batches: 634
I20241205 08:15:24 2997514 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 08:15:24 2997514 dinov2 helpers.py:102]   [  0/634]  eta: 9:07:19    time: 51.796677  data: 12.032346  max mem: 3463
I20241205 08:15:56 2997514 dinov2 helpers.py:102]   [ 10/634]  eta: 1:19:21    time: 7.630284  data: 1.096705  max mem: 4109
I20241205 08:16:35 2997514 dinov2 helpers.py:102]   [ 20/634]  eta: 1:00:07    time: 3.578545  data: 0.001962  max mem: 4109
I20241205 08:17:15 2997514 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:55    time: 3.952667  data: 0.000664  max mem: 4109
I20241205 08:17:55 2997514 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:56    time: 3.967699  data: 0.000706  max mem: 4109
I20241205 08:18:34 2997514 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:18    time: 3.981436  data: 0.000738  max mem: 4109
I20241205 08:19:14 2997514 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:18    time: 3.989808  data: 0.002095  max mem: 4109
I20241205 08:19:54 2997514 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:41    time: 3.990444  data: 0.002405  max mem: 4109
I20241205 08:20:34 2997514 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:18    time: 3.990714  data: 0.000959  max mem: 4109
I20241205 08:21:14 2997514 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:04    time: 3.990728  data: 0.000740  max mem: 4109
I20241205 08:21:54 2997514 dinov2 helpers.py:102]   [100/634]  eta: 0:38:57    time: 3.991687  data: 0.000790  max mem: 4109
I20241205 08:22:34 2997514 dinov2 helpers.py:102]   [110/634]  eta: 0:37:55    time: 3.990707  data: 0.001581  max mem: 4109
I20241205 08:23:14 2997514 dinov2 helpers.py:102]   [120/634]  eta: 0:36:56    time: 3.985973  data: 0.002096  max mem: 4109
I20241205 08:23:54 2997514 dinov2 helpers.py:102]   [130/634]  eta: 0:36:01    time: 3.983346  data: 0.001301  max mem: 4109
I20241205 08:24:33 2997514 dinov2 helpers.py:102]   [140/634]  eta: 0:35:07    time: 3.985314  data: 0.000677  max mem: 4109
I20241205 08:25:13 2997514 dinov2 helpers.py:102]   [150/634]  eta: 0:34:15    time: 3.985878  data: 0.000991  max mem: 4109
I20241205 08:25:53 2997514 dinov2 helpers.py:102]   [160/634]  eta: 0:33:25    time: 3.981239  data: 0.001324  max mem: 4109
I20241205 08:26:33 2997514 dinov2 helpers.py:102]   [170/634]  eta: 0:32:36    time: 3.977796  data: 0.000998  max mem: 4109
I20241205 08:27:13 2997514 dinov2 helpers.py:102]   [180/634]  eta: 0:31:48    time: 3.978738  data: 0.000783  max mem: 4109
I20241205 08:27:52 2997514 dinov2 helpers.py:102]   [190/634]  eta: 0:31:01    time: 3.980633  data: 0.000732  max mem: 4109
I20241205 08:28:32 2997514 dinov2 helpers.py:102]   [200/634]  eta: 0:30:14    time: 3.978799  data: 0.000722  max mem: 4109
I20241205 08:29:12 2997514 dinov2 helpers.py:102]   [210/634]  eta: 0:29:28    time: 3.975250  data: 0.000818  max mem: 4109
I20241205 08:29:52 2997514 dinov2 helpers.py:102]   [220/634]  eta: 0:28:43    time: 3.974422  data: 0.000873  max mem: 4109
I20241205 08:30:31 2997514 dinov2 helpers.py:102]   [230/634]  eta: 0:27:58    time: 3.975874  data: 0.000735  max mem: 4109
I20241205 08:31:11 2997514 dinov2 helpers.py:102]   [240/634]  eta: 0:27:13    time: 3.978609  data: 0.000762  max mem: 4109
I20241205 08:31:51 2997514 dinov2 helpers.py:102]   [250/634]  eta: 0:26:29    time: 3.979928  data: 0.000818  max mem: 4109
I20241205 08:32:31 2997514 dinov2 helpers.py:102]   [260/634]  eta: 0:25:46    time: 3.976982  data: 0.000874  max mem: 4109
I20241205 08:33:11 2997514 dinov2 helpers.py:102]   [270/634]  eta: 0:25:02    time: 3.975772  data: 0.001033  max mem: 4109
I20241205 08:33:50 2997514 dinov2 helpers.py:102]   [280/634]  eta: 0:24:19    time: 3.978659  data: 0.000839  max mem: 4109
I20241205 08:34:30 2997514 dinov2 helpers.py:102]   [290/634]  eta: 0:23:36    time: 3.979837  data: 0.001813  max mem: 4109
I20241205 08:35:10 2997514 dinov2 helpers.py:102]   [300/634]  eta: 0:22:53    time: 3.977079  data: 0.003236  max mem: 4109
I20241205 08:35:50 2997514 dinov2 helpers.py:102]   [310/634]  eta: 0:22:11    time: 3.976969  data: 0.002231  max mem: 4109
I20241205 08:36:29 2997514 dinov2 helpers.py:102]   [320/634]  eta: 0:21:28    time: 3.977929  data: 0.001354  max mem: 4109
I20241205 08:37:09 2997514 dinov2 helpers.py:102]   [330/634]  eta: 0:20:46    time: 3.976003  data: 0.001565  max mem: 4109
I20241205 08:37:49 2997514 dinov2 helpers.py:102]   [340/634]  eta: 0:20:04    time: 3.977057  data: 0.001347  max mem: 4109
I20241205 08:38:29 2997514 dinov2 helpers.py:102]   [350/634]  eta: 0:19:22    time: 3.977988  data: 0.001063  max mem: 4109
I20241205 08:39:09 2997514 dinov2 helpers.py:102]   [360/634]  eta: 0:18:40    time: 3.977782  data: 0.000811  max mem: 4109
I20241205 08:39:48 2997514 dinov2 helpers.py:102]   [370/634]  eta: 0:17:59    time: 3.976932  data: 0.000822  max mem: 4109
I20241205 08:40:28 2997514 dinov2 helpers.py:102]   [380/634]  eta: 0:17:17    time: 3.976821  data: 0.000723  max mem: 4109
I20241205 08:41:08 2997514 dinov2 helpers.py:102]   [390/634]  eta: 0:16:36    time: 3.978668  data: 0.000801  max mem: 4109
I20241205 08:41:48 2997514 dinov2 helpers.py:102]   [400/634]  eta: 0:15:54    time: 3.978671  data: 0.000764  max mem: 4109
I20241205 08:42:27 2997514 dinov2 helpers.py:102]   [410/634]  eta: 0:15:13    time: 3.977031  data: 0.001123  max mem: 4109
I20241205 08:43:07 2997514 dinov2 helpers.py:102]   [420/634]  eta: 0:14:31    time: 3.977421  data: 0.001233  max mem: 4109
I20241205 08:43:47 2997514 dinov2 helpers.py:102]   [430/634]  eta: 0:13:50    time: 3.976099  data: 0.000689  max mem: 4109
I20241205 08:44:27 2997514 dinov2 helpers.py:102]   [440/634]  eta: 0:13:09    time: 3.976472  data: 0.000798  max mem: 4109
I20241205 08:45:07 2997514 dinov2 helpers.py:102]   [450/634]  eta: 0:12:28    time: 3.978419  data: 0.000882  max mem: 4109
I20241205 08:45:46 2997514 dinov2 helpers.py:102]   [460/634]  eta: 0:11:47    time: 3.980592  data: 0.000680  max mem: 4109
I20241205 08:46:26 2997514 dinov2 helpers.py:102]   [470/634]  eta: 0:11:06    time: 3.983231  data: 0.000568  max mem: 4109
I20241205 08:47:06 2997514 dinov2 helpers.py:102]   [480/634]  eta: 0:10:25    time: 3.984132  data: 0.000576  max mem: 4109
I20241205 08:47:46 2997514 dinov2 helpers.py:102]   [490/634]  eta: 0:09:44    time: 3.982357  data: 0.000697  max mem: 4109
I20241205 08:48:26 2997514 dinov2 helpers.py:102]   [500/634]  eta: 0:09:03    time: 3.979599  data: 0.000911  max mem: 4109
I20241205 08:49:05 2997514 dinov2 helpers.py:102]   [510/634]  eta: 0:08:23    time: 3.978033  data: 0.001050  max mem: 4109
I20241205 08:49:45 2997514 dinov2 helpers.py:102]   [520/634]  eta: 0:07:42    time: 3.978682  data: 0.000838  max mem: 4109
I20241205 08:50:25 2997514 dinov2 helpers.py:102]   [530/634]  eta: 0:07:01    time: 3.979457  data: 0.000563  max mem: 4109
I20241205 08:51:05 2997514 dinov2 helpers.py:102]   [540/634]  eta: 0:06:21    time: 3.977791  data: 0.000513  max mem: 4109
I20241205 08:51:45 2997514 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.980619  data: 0.000642  max mem: 4109
I20241205 08:52:24 2997514 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.982417  data: 0.000640  max mem: 4109
I20241205 08:53:04 2997514 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.977910  data: 0.000556  max mem: 4109
I20241205 08:53:44 2997514 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.977151  data: 0.000709  max mem: 4109
I20241205 08:54:24 2997514 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.984294  data: 0.000943  max mem: 4109
I20241205 08:55:04 2997514 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.986933  data: 0.001501  max mem: 4109
I20241205 08:55:43 2997514 dinov2 helpers.py:102]   [610/634]  eta: 0:01:37    time: 3.939720  data: 0.001214  max mem: 4109
I20241205 08:56:21 2997514 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.859269  data: 0.000531  max mem: 4109
I20241205 08:56:59 2997514 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.829722  data: 0.000563  max mem: 4109
I20241205 08:57:18 2997514 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.156383  data: 0.000506  max mem: 4109
I20241205 08:57:18 2997514 dinov2 helpers.py:130]  Total time: 0:42:46 (4.047908 s / it)
I20241205 08:57:18 2997514 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:57:18 2997514 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:57:19 2997514 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:57:19 2997514 dinov2 loaders.py:157] sampler: distributed
I20241205 08:57:19 2997514 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:57:19 2997514 dinov2 loaders.py:229] # of batches: 78
I20241205 08:57:19 2997514 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:57:33 2997514 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:17:41    time: 13.610677  data: 9.791977  max mem: 4109
I20241205 08:58:14 2997514 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:32    time: 4.885869  data: 0.895291  max mem: 4109
I20241205 08:58:54 2997514 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:19    time: 4.012663  data: 0.005459  max mem: 4109
I20241205 08:59:34 2997514 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:27    time: 4.003310  data: 0.006036  max mem: 4109
I20241205 09:00:14 2997514 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:41    time: 3.998799  data: 0.006076  max mem: 4109
I20241205 09:00:51 2997514 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:55    time: 3.864875  data: 0.006634  max mem: 4109
I20241205 09:01:23 2997514 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 3.455291  data: 0.005795  max mem: 4109
I20241205 09:01:45 2997514 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:29    time: 2.704012  data: 0.002549  max mem: 4109
I20241205 09:01:56 2997514 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.042479  data: 0.001135  max mem: 4109
I20241205 09:01:56 2997514 dinov2 helpers.py:130] Test: Total time: 0:04:35 (3.535400 s / it)
I20241205 09:01:56 2997514 dinov2 utils.py:79] Averaged stats: 
I20241205 09:01:56 2997514 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 84.32
I20241205 09:01:56 2997514 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 84.73
I20241205 09:01:56 2997514 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.05
I20241205 09:01:56 2997514 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.08
submitit INFO (2024-12-05 09:01:56,754) - Job completed successfully
I20241205 09:01:56 2997514 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 09:01:56,755) - Exiting after successful completion
I20241205 09:01:56 2997514 submitit submission.py:61] Exiting after successful completion
