submitit INFO (2024-12-05 08:13:22,547) - Starting with JobEnvironment(job_id=2997507, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-05 08:13:22,547) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset/2997507_submitted.pkl
I20241205 08:13:31 2997511 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 08:13:31 2997511 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_pixelated_C/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAPixelatedTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 08:13:31 2997511 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 08:13:31 2997511 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 08:13:31 2997511 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 08:13:58 2997511 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 08:14:03 2997511 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 08:14:04 2997511 dinov2 loaders.py:94] using dataset: "CelebAPixelatedTrain"
Load image list
I20241205 08:14:11 2997511 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 08:14:11 2997511 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 08:14:13 2997511 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 08:14:13 2997511 dinov2 knn.py:260] Extracting features for train set...
I20241205 08:14:13 2997511 dinov2 loaders.py:157] sampler: distributed
I20241205 08:14:13 2997511 dinov2 loaders.py:216] using PyTorch data loader
W20241205 08:14:13 2997511 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 08:14:13 2997511 dinov2 loaders.py:229] # of batches: 634
I20241205 08:14:44 2997511 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 08:14:44 2997511 dinov2 helpers.py:102]   [  0/634]  eta: 5:19:29    time: 30.236073  data: 9.790576  max mem: 3463
I20241205 08:14:47 2997511 dinov2 helpers.py:102]   [ 10/634]  eta: 0:32:12    time: 3.097001  data: 0.893695  max mem: 4109
I20241205 08:14:59 2997511 dinov2 helpers.py:102]   [ 20/634]  eta: 0:22:19    time: 0.779694  data: 0.231537  max mem: 4109
I20241205 08:15:18 2997511 dinov2 helpers.py:102]   [ 30/634]  eta: 0:21:03    time: 1.540289  data: 0.229714  max mem: 4109
I20241205 08:15:54 2997511 dinov2 helpers.py:102]   [ 40/634]  eta: 0:24:26    time: 2.768410  data: 0.000533  max mem: 4109
I20241205 08:16:34 2997511 dinov2 helpers.py:102]   [ 50/634]  eta: 0:26:50    time: 3.786910  data: 0.000621  max mem: 4109
I20241205 08:17:13 2997511 dinov2 helpers.py:102]   [ 60/634]  eta: 0:28:15    time: 3.949716  data: 0.000619  max mem: 4109
I20241205 08:17:53 2997511 dinov2 helpers.py:102]   [ 70/634]  eta: 0:29:07    time: 3.965711  data: 0.000629  max mem: 4109
I20241205 08:18:33 2997511 dinov2 helpers.py:102]   [ 80/634]  eta: 0:29:36    time: 3.979387  data: 0.000957  max mem: 4109
I20241205 08:19:13 2997511 dinov2 helpers.py:102]   [ 90/634]  eta: 0:29:51    time: 3.989091  data: 0.001110  max mem: 4109
I20241205 08:19:53 2997511 dinov2 helpers.py:102]   [100/634]  eta: 0:29:55    time: 3.992482  data: 0.000778  max mem: 4109
I20241205 08:20:33 2997511 dinov2 helpers.py:102]   [110/634]  eta: 0:29:51    time: 3.992439  data: 0.000700  max mem: 4109
I20241205 08:21:13 2997511 dinov2 helpers.py:102]   [120/634]  eta: 0:29:41    time: 3.990687  data: 0.001029  max mem: 4109
I20241205 08:21:53 2997511 dinov2 helpers.py:102]   [130/634]  eta: 0:29:27    time: 3.988999  data: 0.001108  max mem: 4109
I20241205 08:22:32 2997511 dinov2 helpers.py:102]   [140/634]  eta: 0:29:08    time: 3.986171  data: 0.000832  max mem: 4109
I20241205 08:23:12 2997511 dinov2 helpers.py:102]   [150/634]  eta: 0:28:47    time: 3.983245  data: 0.002482  max mem: 4109
I20241205 08:23:52 2997511 dinov2 helpers.py:102]   [160/634]  eta: 0:28:24    time: 3.985176  data: 0.002506  max mem: 4109
I20241205 08:24:32 2997511 dinov2 helpers.py:102]   [170/634]  eta: 0:27:58    time: 3.982433  data: 0.000852  max mem: 4109
I20241205 08:25:12 2997511 dinov2 helpers.py:102]   [180/634]  eta: 0:27:31    time: 3.979950  data: 0.000814  max mem: 4109
I20241205 08:25:52 2997511 dinov2 helpers.py:102]   [190/634]  eta: 0:27:03    time: 3.984138  data: 0.000648  max mem: 4109
I20241205 08:26:31 2997511 dinov2 helpers.py:102]   [200/634]  eta: 0:26:33    time: 3.982869  data: 0.000829  max mem: 4109
I20241205 08:27:11 2997511 dinov2 helpers.py:102]   [210/634]  eta: 0:26:03    time: 3.980597  data: 0.001333  max mem: 4109
I20241205 08:27:51 2997511 dinov2 helpers.py:102]   [220/634]  eta: 0:25:31    time: 3.980571  data: 0.001844  max mem: 4109
I20241205 08:28:31 2997511 dinov2 helpers.py:102]   [230/634]  eta: 0:24:59    time: 3.980630  data: 0.001386  max mem: 4109
I20241205 08:29:11 2997511 dinov2 helpers.py:102]   [240/634]  eta: 0:24:27    time: 3.981630  data: 0.000905  max mem: 4109
I20241205 08:29:51 2997511 dinov2 helpers.py:102]   [250/634]  eta: 0:23:53    time: 3.981420  data: 0.001060  max mem: 4109
I20241205 08:30:30 2997511 dinov2 helpers.py:102]   [260/634]  eta: 0:23:19    time: 3.977693  data: 0.001190  max mem: 4109
I20241205 08:31:10 2997511 dinov2 helpers.py:102]   [270/634]  eta: 0:22:45    time: 3.978640  data: 0.001157  max mem: 4109
I20241205 08:31:50 2997511 dinov2 helpers.py:102]   [280/634]  eta: 0:22:11    time: 3.980809  data: 0.002061  max mem: 4109
I20241205 08:32:30 2997511 dinov2 helpers.py:102]   [290/634]  eta: 0:21:36    time: 3.979749  data: 0.002174  max mem: 4109
I20241205 08:33:09 2997511 dinov2 helpers.py:102]   [300/634]  eta: 0:21:00    time: 3.977542  data: 0.001101  max mem: 4109
I20241205 08:33:49 2997511 dinov2 helpers.py:102]   [310/634]  eta: 0:20:25    time: 3.976884  data: 0.001131  max mem: 4109
I20241205 08:34:29 2997511 dinov2 helpers.py:102]   [320/634]  eta: 0:19:49    time: 3.979787  data: 0.000864  max mem: 4109
I20241205 08:35:09 2997511 dinov2 helpers.py:102]   [330/634]  eta: 0:19:13    time: 3.979710  data: 0.000702  max mem: 4109
I20241205 08:35:49 2997511 dinov2 helpers.py:102]   [340/634]  eta: 0:18:36    time: 3.977019  data: 0.000802  max mem: 4109
I20241205 08:36:28 2997511 dinov2 helpers.py:102]   [350/634]  eta: 0:18:00    time: 3.976136  data: 0.000735  max mem: 4109
I20241205 08:37:08 2997511 dinov2 helpers.py:102]   [360/634]  eta: 0:17:23    time: 3.975999  data: 0.000877  max mem: 4109
I20241205 08:37:48 2997511 dinov2 helpers.py:102]   [370/634]  eta: 0:16:46    time: 3.977014  data: 0.001069  max mem: 4109
I20241205 08:38:28 2997511 dinov2 helpers.py:102]   [380/634]  eta: 0:16:09    time: 3.978844  data: 0.000988  max mem: 4109
I20241205 08:39:07 2997511 dinov2 helpers.py:102]   [390/634]  eta: 0:15:32    time: 3.978723  data: 0.000995  max mem: 4109
I20241205 08:39:47 2997511 dinov2 helpers.py:102]   [400/634]  eta: 0:14:55    time: 3.978755  data: 0.001177  max mem: 4109
I20241205 08:40:27 2997511 dinov2 helpers.py:102]   [410/634]  eta: 0:14:17    time: 3.977751  data: 0.001618  max mem: 4109
I20241205 08:41:07 2997511 dinov2 helpers.py:102]   [420/634]  eta: 0:13:40    time: 3.981406  data: 0.001267  max mem: 4109
I20241205 08:41:47 2997511 dinov2 helpers.py:102]   [430/634]  eta: 0:13:02    time: 3.981391  data: 0.000695  max mem: 4109
I20241205 08:42:26 2997511 dinov2 helpers.py:102]   [440/634]  eta: 0:12:24    time: 3.978752  data: 0.000816  max mem: 4109
I20241205 08:43:06 2997511 dinov2 helpers.py:102]   [450/634]  eta: 0:11:46    time: 3.978845  data: 0.000864  max mem: 4109
I20241205 08:43:46 2997511 dinov2 helpers.py:102]   [460/634]  eta: 0:11:09    time: 3.979718  data: 0.000967  max mem: 4109
I20241205 08:44:26 2997511 dinov2 helpers.py:102]   [470/634]  eta: 0:10:31    time: 3.980011  data: 0.001285  max mem: 4109
I20241205 08:45:06 2997511 dinov2 helpers.py:102]   [480/634]  eta: 0:09:53    time: 3.977021  data: 0.001210  max mem: 4109
I20241205 08:45:45 2997511 dinov2 helpers.py:102]   [490/634]  eta: 0:09:14    time: 3.976739  data: 0.001026  max mem: 4109
I20241205 08:46:25 2997511 dinov2 helpers.py:102]   [500/634]  eta: 0:08:36    time: 3.978755  data: 0.001046  max mem: 4109
I20241205 08:47:05 2997511 dinov2 helpers.py:102]   [510/634]  eta: 0:07:58    time: 3.978814  data: 0.001329  max mem: 4109
I20241205 08:47:45 2997511 dinov2 helpers.py:102]   [520/634]  eta: 0:07:20    time: 3.977919  data: 0.002232  max mem: 4109
I20241205 08:48:24 2997511 dinov2 helpers.py:102]   [530/634]  eta: 0:06:41    time: 3.977950  data: 0.002571  max mem: 4109
I20241205 08:49:04 2997511 dinov2 helpers.py:102]   [540/634]  eta: 0:06:03    time: 3.979702  data: 0.001796  max mem: 4109
I20241205 08:49:44 2997511 dinov2 helpers.py:102]   [550/634]  eta: 0:05:24    time: 3.978624  data: 0.001122  max mem: 4109
I20241205 08:50:24 2997511 dinov2 helpers.py:102]   [560/634]  eta: 0:04:46    time: 3.976897  data: 0.001184  max mem: 4109
I20241205 08:51:04 2997511 dinov2 helpers.py:102]   [570/634]  eta: 0:04:07    time: 3.981374  data: 0.001304  max mem: 4109
I20241205 08:51:44 2997511 dinov2 helpers.py:102]   [580/634]  eta: 0:03:29    time: 3.983256  data: 0.001435  max mem: 4109
I20241205 08:52:23 2997511 dinov2 helpers.py:102]   [590/634]  eta: 0:02:50    time: 3.983348  data: 0.001684  max mem: 4109
I20241205 08:53:03 2997511 dinov2 helpers.py:102]   [600/634]  eta: 0:02:11    time: 3.983701  data: 0.001353  max mem: 4109
I20241205 08:53:43 2997511 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.980718  data: 0.000915  max mem: 4109
I20241205 08:54:23 2997511 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.978501  data: 0.000872  max mem: 4109
I20241205 08:55:03 2997511 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.978814  data: 0.001242  max mem: 4109
I20241205 08:55:22 2997511 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.358864  data: 0.001142  max mem: 4109
I20241205 08:55:22 2997511 dinov2 helpers.py:130]  Total time: 0:41:09 (3.894583 s / it)
I20241205 08:55:22 2997511 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:55:22 2997511 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:55:23 2997511 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:55:23 2997511 dinov2 loaders.py:157] sampler: distributed
I20241205 08:55:23 2997511 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:55:23 2997511 dinov2 loaders.py:229] # of batches: 78
I20241205 08:55:23 2997511 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:55:33 2997511 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:20    time: 8.718191  data: 4.584846  max mem: 4109
I20241205 08:56:13 2997511 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:03    time: 4.469383  data: 0.419126  max mem: 4109
I20241205 08:56:53 2997511 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:04    time: 3.988772  data: 0.002655  max mem: 4109
I20241205 08:57:28 2997511 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:11    time: 3.720499  data: 0.003181  max mem: 4109
I20241205 08:58:06 2997511 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:30    time: 3.682515  data: 0.004107  max mem: 4109
I20241205 08:58:46 2997511 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:51    time: 3.933749  data: 0.007154  max mem: 4109
I20241205 08:59:26 2997511 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 4.010962  data: 0.006290  max mem: 4109
I20241205 09:00:07 2997511 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 4.010188  data: 0.003452  max mem: 4109
I20241205 09:00:33 2997511 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.916526  data: 0.002399  max mem: 4109
I20241205 09:00:33 2997511 dinov2 helpers.py:130] Test: Total time: 0:05:08 (3.957374 s / it)
I20241205 09:00:33 2997511 dinov2 utils.py:79] Averaged stats: 
I20241205 09:00:34 2997511 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 84.32
I20241205 09:00:34 2997511 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 84.73
I20241205 09:00:34 2997511 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.05
I20241205 09:00:34 2997511 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.08
submitit INFO (2024-12-05 09:00:35,138) - Job completed successfully
I20241205 09:00:35 2997511 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 09:00:35,166) - Exiting after successful completion
I20241205 09:00:35 2997511 submitit submission.py:61] Exiting after successful completion
