submitit INFO (2024-12-05 08:13:22,599) - Starting with JobEnvironment(job_id=2997507, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-05 08:13:22,599) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset/2997507_submitted.pkl
I20241205 08:13:32 2997515 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 08:13:32 2997515 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_pixelated_C/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAPixelatedTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 08:13:32 2997515 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 08:13:32 2997515 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 08:13:32 2997515 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 08:14:02 2997515 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 08:14:06 2997515 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 08:14:06 2997515 dinov2 loaders.py:94] using dataset: "CelebAPixelatedTrain"
Load image list
I20241205 08:14:13 2997515 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 08:14:13 2997515 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 08:14:17 2997515 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 08:14:17 2997515 dinov2 knn.py:260] Extracting features for train set...
I20241205 08:14:17 2997515 dinov2 loaders.py:157] sampler: distributed
I20241205 08:14:17 2997515 dinov2 loaders.py:216] using PyTorch data loader
W20241205 08:14:17 2997515 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 08:14:17 2997515 dinov2 loaders.py:229] # of batches: 634
I20241205 08:14:52 2997515 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 08:14:52 2997515 dinov2 helpers.py:102]   [  0/634]  eta: 6:09:10    time: 34.937199  data: 9.070024  max mem: 3463
I20241205 08:15:05 2997515 dinov2 helpers.py:102]   [ 10/634]  eta: 0:45:17    time: 4.354293  data: 0.829465  max mem: 4109
I20241205 08:15:30 2997515 dinov2 helpers.py:102]   [ 20/634]  eta: 0:35:10    time: 1.862650  data: 0.003713  max mem: 4109
I20241205 08:16:09 2997515 dinov2 helpers.py:102]   [ 30/634]  eta: 0:36:08    time: 3.169276  data: 0.002231  max mem: 4109
I20241205 08:16:48 2997515 dinov2 helpers.py:102]   [ 40/634]  eta: 0:36:24    time: 3.929671  data: 0.003303  max mem: 4109
I20241205 08:17:28 2997515 dinov2 helpers.py:102]   [ 50/634]  eta: 0:36:20    time: 3.957411  data: 0.003906  max mem: 4109
I20241205 08:18:08 2997515 dinov2 helpers.py:102]   [ 60/634]  eta: 0:36:06    time: 3.972825  data: 0.002367  max mem: 4109
I20241205 08:18:48 2997515 dinov2 helpers.py:102]   [ 70/634]  eta: 0:35:46    time: 3.986688  data: 0.000993  max mem: 4109
I20241205 08:19:28 2997515 dinov2 helpers.py:102]   [ 80/634]  eta: 0:35:20    time: 3.992552  data: 0.000873  max mem: 4109
I20241205 08:20:07 2997515 dinov2 helpers.py:102]   [ 90/634]  eta: 0:34:52    time: 3.992668  data: 0.000991  max mem: 4109
I20241205 08:20:47 2997515 dinov2 helpers.py:102]   [100/634]  eta: 0:34:21    time: 3.992778  data: 0.002712  max mem: 4109
I20241205 08:21:27 2997515 dinov2 helpers.py:102]   [110/634]  eta: 0:33:49    time: 3.992432  data: 0.002649  max mem: 4109
I20241205 08:22:07 2997515 dinov2 helpers.py:102]   [120/634]  eta: 0:33:15    time: 3.992255  data: 0.000931  max mem: 4109
I20241205 08:22:47 2997515 dinov2 helpers.py:102]   [130/634]  eta: 0:32:40    time: 3.990657  data: 0.000954  max mem: 4109
I20241205 08:23:27 2997515 dinov2 helpers.py:102]   [140/634]  eta: 0:32:05    time: 3.990583  data: 0.001648  max mem: 4109
I20241205 08:24:07 2997515 dinov2 helpers.py:102]   [150/634]  eta: 0:31:29    time: 3.989745  data: 0.002314  max mem: 4109
I20241205 08:24:47 2997515 dinov2 helpers.py:102]   [160/634]  eta: 0:30:52    time: 3.985972  data: 0.002165  max mem: 4109
I20241205 08:25:27 2997515 dinov2 helpers.py:102]   [170/634]  eta: 0:30:15    time: 3.986019  data: 0.001883  max mem: 4109
I20241205 08:26:06 2997515 dinov2 helpers.py:102]   [180/634]  eta: 0:29:38    time: 3.983178  data: 0.001353  max mem: 4109
I20241205 08:26:46 2997515 dinov2 helpers.py:102]   [190/634]  eta: 0:29:00    time: 3.984848  data: 0.000896  max mem: 4109
I20241205 08:27:26 2997515 dinov2 helpers.py:102]   [200/634]  eta: 0:28:22    time: 3.984220  data: 0.001298  max mem: 4109
I20241205 08:28:06 2997515 dinov2 helpers.py:102]   [210/634]  eta: 0:27:44    time: 3.981450  data: 0.001455  max mem: 4109
I20241205 08:28:46 2997515 dinov2 helpers.py:102]   [220/634]  eta: 0:27:06    time: 3.982378  data: 0.001240  max mem: 4109
I20241205 08:29:26 2997515 dinov2 helpers.py:102]   [230/634]  eta: 0:26:28    time: 3.981588  data: 0.001443  max mem: 4109
I20241205 08:30:05 2997515 dinov2 helpers.py:102]   [240/634]  eta: 0:25:49    time: 3.981469  data: 0.001378  max mem: 4109
I20241205 08:30:45 2997515 dinov2 helpers.py:102]   [250/634]  eta: 0:25:11    time: 3.980574  data: 0.001133  max mem: 4109
I20241205 08:31:25 2997515 dinov2 helpers.py:102]   [260/634]  eta: 0:24:32    time: 3.982045  data: 0.001565  max mem: 4109
I20241205 08:32:05 2997515 dinov2 helpers.py:102]   [270/634]  eta: 0:23:53    time: 3.978859  data: 0.002183  max mem: 4109
I20241205 08:32:45 2997515 dinov2 helpers.py:102]   [280/634]  eta: 0:23:14    time: 3.976345  data: 0.002102  max mem: 4109
I20241205 08:33:24 2997515 dinov2 helpers.py:102]   [290/634]  eta: 0:22:35    time: 3.975804  data: 0.001248  max mem: 4109
I20241205 08:34:04 2997515 dinov2 helpers.py:102]   [300/634]  eta: 0:21:56    time: 3.976985  data: 0.000971  max mem: 4109
I20241205 08:34:44 2997515 dinov2 helpers.py:102]   [310/634]  eta: 0:21:17    time: 3.979733  data: 0.001398  max mem: 4109
I20241205 08:35:24 2997515 dinov2 helpers.py:102]   [320/634]  eta: 0:20:38    time: 3.979643  data: 0.002286  max mem: 4109
I20241205 08:36:03 2997515 dinov2 helpers.py:102]   [330/634]  eta: 0:19:59    time: 3.977058  data: 0.002090  max mem: 4109
I20241205 08:36:43 2997515 dinov2 helpers.py:102]   [340/634]  eta: 0:19:20    time: 3.977030  data: 0.000956  max mem: 4109
I20241205 08:37:23 2997515 dinov2 helpers.py:102]   [350/634]  eta: 0:18:41    time: 3.978654  data: 0.001218  max mem: 4109
I20241205 08:38:03 2997515 dinov2 helpers.py:102]   [360/634]  eta: 0:18:01    time: 3.977203  data: 0.001312  max mem: 4109
I20241205 08:38:43 2997515 dinov2 helpers.py:102]   [370/634]  eta: 0:17:22    time: 3.976133  data: 0.001121  max mem: 4109
I20241205 08:39:22 2997515 dinov2 helpers.py:102]   [380/634]  eta: 0:16:43    time: 3.978563  data: 0.001148  max mem: 4109
I20241205 08:40:02 2997515 dinov2 helpers.py:102]   [390/634]  eta: 0:16:03    time: 3.978725  data: 0.001085  max mem: 4109
I20241205 08:40:42 2997515 dinov2 helpers.py:102]   [400/634]  eta: 0:15:24    time: 3.977790  data: 0.000878  max mem: 4109
I20241205 08:41:22 2997515 dinov2 helpers.py:102]   [410/634]  eta: 0:14:45    time: 3.982213  data: 0.001648  max mem: 4109
I20241205 08:42:02 2997515 dinov2 helpers.py:102]   [420/634]  eta: 0:14:05    time: 3.983195  data: 0.001831  max mem: 4109
I20241205 08:42:41 2997515 dinov2 helpers.py:102]   [430/634]  eta: 0:13:26    time: 3.977941  data: 0.000915  max mem: 4109
I20241205 08:43:21 2997515 dinov2 helpers.py:102]   [440/634]  eta: 0:12:47    time: 3.978021  data: 0.001016  max mem: 4109
I20241205 08:44:01 2997515 dinov2 helpers.py:102]   [450/634]  eta: 0:12:07    time: 3.979625  data: 0.002040  max mem: 4109
I20241205 08:44:41 2997515 dinov2 helpers.py:102]   [460/634]  eta: 0:11:28    time: 3.977552  data: 0.002221  max mem: 4109
I20241205 08:45:20 2997515 dinov2 helpers.py:102]   [470/634]  eta: 0:10:48    time: 3.977577  data: 0.001183  max mem: 4109
I20241205 08:46:00 2997515 dinov2 helpers.py:102]   [480/634]  eta: 0:10:09    time: 3.977709  data: 0.000926  max mem: 4109
I20241205 08:46:40 2997515 dinov2 helpers.py:102]   [490/634]  eta: 0:09:29    time: 3.978842  data: 0.001319  max mem: 4109
I20241205 08:47:20 2997515 dinov2 helpers.py:102]   [500/634]  eta: 0:08:50    time: 3.979681  data: 0.002149  max mem: 4109
I20241205 08:48:00 2997515 dinov2 helpers.py:102]   [510/634]  eta: 0:08:10    time: 3.978868  data: 0.001754  max mem: 4109
I20241205 08:48:40 2997515 dinov2 helpers.py:102]   [520/634]  eta: 0:07:31    time: 3.982535  data: 0.001101  max mem: 4109
I20241205 08:49:19 2997515 dinov2 helpers.py:102]   [530/634]  eta: 0:06:51    time: 3.983144  data: 0.001410  max mem: 4109
I20241205 08:49:59 2997515 dinov2 helpers.py:102]   [540/634]  eta: 0:06:12    time: 3.979526  data: 0.001186  max mem: 4109
I20241205 08:50:39 2997515 dinov2 helpers.py:102]   [550/634]  eta: 0:05:32    time: 3.983111  data: 0.000878  max mem: 4109
I20241205 08:51:19 2997515 dinov2 helpers.py:102]   [560/634]  eta: 0:04:53    time: 3.983201  data: 0.000837  max mem: 4109
I20241205 08:51:59 2997515 dinov2 helpers.py:102]   [570/634]  eta: 0:04:13    time: 3.980653  data: 0.000834  max mem: 4109
I20241205 08:52:38 2997515 dinov2 helpers.py:102]   [580/634]  eta: 0:03:33    time: 3.981640  data: 0.001598  max mem: 4109
I20241205 08:53:18 2997515 dinov2 helpers.py:102]   [590/634]  eta: 0:02:54    time: 3.983372  data: 0.001577  max mem: 4109
I20241205 08:53:58 2997515 dinov2 helpers.py:102]   [600/634]  eta: 0:02:14    time: 3.982385  data: 0.001180  max mem: 4109
I20241205 08:54:38 2997515 dinov2 helpers.py:102]   [610/634]  eta: 0:01:35    time: 3.979798  data: 0.002564  max mem: 4109
I20241205 08:55:18 2997515 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.980736  data: 0.002699  max mem: 4109
I20241205 08:55:57 2997515 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.941081  data: 0.002815  max mem: 4109
I20241205 08:56:15 2997515 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.280129  data: 0.002687  max mem: 4109
I20241205 08:56:16 2997515 dinov2 helpers.py:130]  Total time: 0:41:58 (3.972070 s / it)
I20241205 08:56:16 2997515 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:56:16 2997515 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:56:17 2997515 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:56:17 2997515 dinov2 loaders.py:157] sampler: distributed
I20241205 08:56:17 2997515 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:56:17 2997515 dinov2 loaders.py:229] # of batches: 78
I20241205 08:56:17 2997515 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:56:27 2997515 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:12:03    time: 9.278410  data: 5.112814  max mem: 4109
I20241205 08:57:07 2997515 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:05    time: 4.487682  data: 0.471875  max mem: 4109
I20241205 08:57:40 2997515 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:48    time: 3.674195  data: 0.007983  max mem: 4109
I20241205 08:58:20 2997515 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:10    time: 3.673477  data: 0.007705  max mem: 4109
I20241205 08:59:00 2997515 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:31    time: 4.014747  data: 0.008685  max mem: 4109
I20241205 08:59:40 2997515 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:51    time: 4.013487  data: 0.012995  max mem: 4109
I20241205 09:00:20 2997515 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 4.003869  data: 0.012591  max mem: 4109
I20241205 09:00:57 2997515 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 3.825629  data: 0.007123  max mem: 4109
I20241205 09:01:18 2997515 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.488487  data: 0.004112  max mem: 4109
I20241205 09:01:18 2997515 dinov2 helpers.py:130] Test: Total time: 0:05:00 (3.857767 s / it)
I20241205 09:01:18 2997515 dinov2 utils.py:79] Averaged stats: 
I20241205 09:01:19 2997515 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 84.32
I20241205 09:01:19 2997515 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 84.73
I20241205 09:01:19 2997515 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.05
I20241205 09:01:19 2997515 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.08
submitit INFO (2024-12-05 09:01:20,029) - Job completed successfully
I20241205 09:01:20 2997515 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 09:01:20,051) - Exiting after successful completion
I20241205 09:01:20 2997515 submitit submission.py:61] Exiting after successful completion
