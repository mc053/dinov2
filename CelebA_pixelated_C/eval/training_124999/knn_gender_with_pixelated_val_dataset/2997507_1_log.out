submitit INFO (2024-12-05 08:13:22,440) - Starting with JobEnvironment(job_id=2997507, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-05 08:13:22,440) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset/2997507_submitted.pkl
I20241205 08:13:30 2997509 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 08:13:30 2997509 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_pixelated_C/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAPixelatedTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 08:13:30 2997509 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 08:13:30 2997509 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_C/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 08:13:30 2997509 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 08:14:03 2997509 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 08:14:07 2997509 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_C/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 08:14:07 2997509 dinov2 loaders.py:94] using dataset: "CelebAPixelatedTrain"
Load image list
I20241205 08:14:14 2997509 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 08:14:14 2997509 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 08:14:19 2997509 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 08:14:19 2997509 dinov2 knn.py:260] Extracting features for train set...
I20241205 08:14:19 2997509 dinov2 loaders.py:157] sampler: distributed
I20241205 08:14:19 2997509 dinov2 loaders.py:216] using PyTorch data loader
W20241205 08:14:19 2997509 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 08:14:19 2997509 dinov2 loaders.py:229] # of batches: 634
I20241205 08:15:03 2997509 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 08:15:03 2997509 dinov2 helpers.py:102]   [  0/634]  eta: 7:39:57    time: 43.529217  data: 8.967744  max mem: 3463
I20241205 08:15:19 2997509 dinov2 helpers.py:102]   [ 10/634]  eta: 0:56:54    time: 5.472695  data: 0.818227  max mem: 4109
I20241205 08:15:57 2997509 dinov2 helpers.py:102]   [ 20/634]  eta: 0:47:25    time: 2.689280  data: 0.002544  max mem: 4109
I20241205 08:16:36 2997509 dinov2 helpers.py:102]   [ 30/634]  eta: 0:44:24    time: 3.826600  data: 0.001847  max mem: 4109
I20241205 08:17:16 2997509 dinov2 helpers.py:102]   [ 40/634]  eta: 0:42:34    time: 3.950957  data: 0.002134  max mem: 4109
I20241205 08:17:55 2997509 dinov2 helpers.py:102]   [ 50/634]  eta: 0:41:14    time: 3.967714  data: 0.001615  max mem: 4109
I20241205 08:18:35 2997509 dinov2 helpers.py:102]   [ 60/634]  eta: 0:40:08    time: 3.981989  data: 0.000825  max mem: 4109
I20241205 08:19:15 2997509 dinov2 helpers.py:102]   [ 70/634]  eta: 0:39:10    time: 3.990714  data: 0.000720  max mem: 4109
I20241205 08:19:55 2997509 dinov2 helpers.py:102]   [ 80/634]  eta: 0:38:16    time: 3.992604  data: 0.000806  max mem: 4109
I20241205 08:20:35 2997509 dinov2 helpers.py:102]   [ 90/634]  eta: 0:37:26    time: 3.992553  data: 0.000904  max mem: 4109
I20241205 08:21:15 2997509 dinov2 helpers.py:102]   [100/634]  eta: 0:36:37    time: 3.988911  data: 0.001607  max mem: 4109
I20241205 08:21:55 2997509 dinov2 helpers.py:102]   [110/634]  eta: 0:35:50    time: 3.989240  data: 0.001691  max mem: 4109
I20241205 08:22:35 2997509 dinov2 helpers.py:102]   [120/634]  eta: 0:35:04    time: 3.992498  data: 0.001554  max mem: 4109
I20241205 08:23:14 2997509 dinov2 helpers.py:102]   [130/634]  eta: 0:34:19    time: 3.983934  data: 0.001400  max mem: 4109
I20241205 08:23:54 2997509 dinov2 helpers.py:102]   [140/634]  eta: 0:33:34    time: 3.978837  data: 0.000589  max mem: 4109
I20241205 08:24:34 2997509 dinov2 helpers.py:102]   [150/634]  eta: 0:32:50    time: 3.979748  data: 0.000614  max mem: 4109
I20241205 08:25:14 2997509 dinov2 helpers.py:102]   [160/634]  eta: 0:32:07    time: 3.980563  data: 0.000804  max mem: 4109
I20241205 08:25:54 2997509 dinov2 helpers.py:102]   [170/634]  eta: 0:31:24    time: 3.981409  data: 0.000893  max mem: 4109
I20241205 08:26:33 2997509 dinov2 helpers.py:102]   [180/634]  eta: 0:30:41    time: 3.976915  data: 0.000890  max mem: 4109
I20241205 08:27:13 2997509 dinov2 helpers.py:102]   [190/634]  eta: 0:29:59    time: 3.976062  data: 0.001451  max mem: 4109
I20241205 08:27:53 2997509 dinov2 helpers.py:102]   [200/634]  eta: 0:29:17    time: 3.977798  data: 0.001744  max mem: 4109
I20241205 08:28:33 2997509 dinov2 helpers.py:102]   [210/634]  eta: 0:28:35    time: 3.977014  data: 0.001999  max mem: 4109
I20241205 08:29:13 2997509 dinov2 helpers.py:102]   [220/634]  eta: 0:27:53    time: 3.977128  data: 0.002850  max mem: 4109
I20241205 08:29:52 2997509 dinov2 helpers.py:102]   [230/634]  eta: 0:27:11    time: 3.978826  data: 0.002160  max mem: 4109
I20241205 08:30:32 2997509 dinov2 helpers.py:102]   [240/634]  eta: 0:26:30    time: 3.978570  data: 0.001843  max mem: 4109
I20241205 08:31:12 2997509 dinov2 helpers.py:102]   [250/634]  eta: 0:25:49    time: 3.976114  data: 0.001794  max mem: 4109
I20241205 08:31:52 2997509 dinov2 helpers.py:102]   [260/634]  eta: 0:25:08    time: 3.977212  data: 0.001371  max mem: 4109
I20241205 08:32:31 2997509 dinov2 helpers.py:102]   [270/634]  eta: 0:24:26    time: 3.979540  data: 0.001539  max mem: 4109
I20241205 08:33:11 2997509 dinov2 helpers.py:102]   [280/634]  eta: 0:23:46    time: 3.978455  data: 0.001116  max mem: 4109
I20241205 08:33:51 2997509 dinov2 helpers.py:102]   [290/634]  eta: 0:23:05    time: 3.976017  data: 0.000774  max mem: 4109
I20241205 08:34:31 2997509 dinov2 helpers.py:102]   [300/634]  eta: 0:22:24    time: 3.977088  data: 0.000853  max mem: 4109
I20241205 08:35:11 2997509 dinov2 helpers.py:102]   [310/634]  eta: 0:21:43    time: 3.979631  data: 0.000793  max mem: 4109
I20241205 08:35:50 2997509 dinov2 helpers.py:102]   [320/634]  eta: 0:21:02    time: 3.979762  data: 0.001688  max mem: 4109
I20241205 08:36:30 2997509 dinov2 helpers.py:102]   [330/634]  eta: 0:20:22    time: 3.977070  data: 0.001712  max mem: 4109
I20241205 08:37:10 2997509 dinov2 helpers.py:102]   [340/634]  eta: 0:19:41    time: 3.975955  data: 0.000686  max mem: 4109
I20241205 08:37:50 2997509 dinov2 helpers.py:102]   [350/634]  eta: 0:19:01    time: 3.977910  data: 0.000798  max mem: 4109
I20241205 08:38:29 2997509 dinov2 helpers.py:102]   [360/634]  eta: 0:18:20    time: 3.977123  data: 0.000965  max mem: 4109
I20241205 08:39:09 2997509 dinov2 helpers.py:102]   [370/634]  eta: 0:17:40    time: 3.976935  data: 0.000929  max mem: 4109
I20241205 08:39:49 2997509 dinov2 helpers.py:102]   [380/634]  eta: 0:16:59    time: 3.980515  data: 0.001273  max mem: 4109
I20241205 08:40:29 2997509 dinov2 helpers.py:102]   [390/634]  eta: 0:16:19    time: 3.980418  data: 0.001339  max mem: 4109
I20241205 08:41:09 2997509 dinov2 helpers.py:102]   [400/634]  eta: 0:15:39    time: 3.975962  data: 0.000839  max mem: 4109
I20241205 08:41:48 2997509 dinov2 helpers.py:102]   [410/634]  eta: 0:14:58    time: 3.978708  data: 0.000795  max mem: 4109
I20241205 08:42:28 2997509 dinov2 helpers.py:102]   [420/634]  eta: 0:14:18    time: 3.978757  data: 0.000766  max mem: 4109
I20241205 08:43:08 2997509 dinov2 helpers.py:102]   [430/634]  eta: 0:13:38    time: 3.974363  data: 0.000728  max mem: 4109
I20241205 08:43:48 2997509 dinov2 helpers.py:102]   [440/634]  eta: 0:12:57    time: 3.974298  data: 0.001207  max mem: 4109
I20241205 08:44:27 2997509 dinov2 helpers.py:102]   [450/634]  eta: 0:12:17    time: 3.976907  data: 0.001405  max mem: 4109
I20241205 08:45:07 2997509 dinov2 helpers.py:102]   [460/634]  eta: 0:11:37    time: 3.977617  data: 0.001541  max mem: 4109
I20241205 08:45:47 2997509 dinov2 helpers.py:102]   [470/634]  eta: 0:10:57    time: 3.979462  data: 0.001265  max mem: 4109
I20241205 08:46:27 2997509 dinov2 helpers.py:102]   [480/634]  eta: 0:10:17    time: 3.979654  data: 0.000542  max mem: 4109
I20241205 08:47:07 2997509 dinov2 helpers.py:102]   [490/634]  eta: 0:09:36    time: 3.976934  data: 0.000524  max mem: 4109
I20241205 08:47:46 2997509 dinov2 helpers.py:102]   [500/634]  eta: 0:08:56    time: 3.978801  data: 0.000670  max mem: 4109
I20241205 08:48:26 2997509 dinov2 helpers.py:102]   [510/634]  eta: 0:08:16    time: 3.979718  data: 0.001461  max mem: 4109
I20241205 08:49:06 2997509 dinov2 helpers.py:102]   [520/634]  eta: 0:07:36    time: 3.981349  data: 0.001600  max mem: 4109
I20241205 08:49:46 2997509 dinov2 helpers.py:102]   [530/634]  eta: 0:06:56    time: 3.981499  data: 0.000934  max mem: 4109
I20241205 08:50:26 2997509 dinov2 helpers.py:102]   [540/634]  eta: 0:06:16    time: 3.977736  data: 0.000724  max mem: 4109
I20241205 08:51:05 2997509 dinov2 helpers.py:102]   [550/634]  eta: 0:05:36    time: 3.977590  data: 0.000687  max mem: 4109
I20241205 08:51:45 2997509 dinov2 helpers.py:102]   [560/634]  eta: 0:04:56    time: 3.981557  data: 0.000894  max mem: 4109
I20241205 08:52:25 2997509 dinov2 helpers.py:102]   [570/634]  eta: 0:04:16    time: 3.984019  data: 0.000858  max mem: 4109
I20241205 08:53:05 2997509 dinov2 helpers.py:102]   [580/634]  eta: 0:03:36    time: 3.982171  data: 0.000618  max mem: 4109
I20241205 08:53:45 2997509 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.980753  data: 0.000993  max mem: 4109
I20241205 08:54:25 2997509 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.983466  data: 0.001187  max mem: 4109
I20241205 08:55:04 2997509 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.984298  data: 0.001066  max mem: 4109
I20241205 08:55:43 2997509 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.939703  data: 0.000927  max mem: 4109
I20241205 08:56:22 2997509 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.856480  data: 0.000857  max mem: 4109
I20241205 08:56:41 2997509 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.216060  data: 0.000787  max mem: 4109
I20241205 08:56:41 2997509 dinov2 helpers.py:130]  Total time: 0:42:21 (4.008936 s / it)
I20241205 08:56:41 2997509 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:56:41 2997509 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:56:42 2997509 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:56:42 2997509 dinov2 loaders.py:157] sampler: distributed
I20241205 08:56:42 2997509 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:56:42 2997509 dinov2 loaders.py:229] # of batches: 78
I20241205 08:56:42 2997509 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:56:51 2997509 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:17    time: 8.688890  data: 4.620339  max mem: 4109
I20241205 08:57:27 2997509 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:32    time: 4.002597  data: 0.423265  max mem: 4109
I20241205 08:58:05 2997509 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:47    time: 3.677163  data: 0.004274  max mem: 4109
I20241205 08:58:45 2997509 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:09    time: 3.910063  data: 0.005179  max mem: 4109
I20241205 08:59:25 2997509 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:30    time: 4.006258  data: 0.006483  max mem: 4109
I20241205 09:00:05 2997509 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:51    time: 4.015010  data: 0.004867  max mem: 4109
I20241205 09:00:43 2997509 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 3.915749  data: 0.004352  max mem: 4109
I20241205 09:01:17 2997509 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:30    time: 3.607965  data: 0.007006  max mem: 4109
I20241205 09:01:34 2997509 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.048029  data: 0.004635  max mem: 4109
I20241205 09:01:34 2997509 dinov2 helpers.py:130] Test: Total time: 0:04:51 (3.735230 s / it)
I20241205 09:01:34 2997509 dinov2 utils.py:79] Averaged stats: 
I20241205 09:01:35 2997509 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 84.32
I20241205 09:01:35 2997509 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 84.73
I20241205 09:01:35 2997509 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.05
I20241205 09:01:35 2997509 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.08
submitit INFO (2024-12-05 09:01:35,707) - Job completed successfully
I20241205 09:01:35 2997509 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 09:01:35,724) - Exiting after successful completion
I20241205 09:01:35 2997509 submitit submission.py:61] Exiting after successful completion
