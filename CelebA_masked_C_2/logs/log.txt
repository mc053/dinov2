I20241211 10:16:04 3820504 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241211 10:16:04 3820504 dinov2 config.py:60] config_file: dinov2/configs/train/celeba_masked_c.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_masked_C_2']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_masked_C_2
I20241211 10:16:04 3820504 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241211 10:16:04 3820504 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAMaskedTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_masked_C_2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241211 10:16:05 3820504 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241211 10:16:10 3820504 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241211 10:16:15 3820504 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241211 10:16:16 3820504 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241211 10:16:16 3820504 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/dinov2_env/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20241211 10:16:16 3820504 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241211 10:16:16 3820504 dinov2 param_groups.py:54] chunked fsdp
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241211 10:16:16 3820504 dinov2 param_groups.py:64] else code branch
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241211 10:16:16 3820504 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241211 10:16:16 3820504 dinov2 train.py:102] Schedulers ready.
I20241211 10:16:16 3820504 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241211 10:16:16 3820504 dinov2 augmentations.py:34] ###################################
I20241211 10:16:16 3820504 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241211 10:16:16 3820504 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241211 10:16:16 3820504 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241211 10:16:16 3820504 dinov2 augmentations.py:38] local_crops_number: 8
I20241211 10:16:16 3820504 dinov2 augmentations.py:39] global_crops_size: 224
I20241211 10:16:16 3820504 dinov2 augmentations.py:40] local_crops_size: 96
I20241211 10:16:16 3820504 dinov2 augmentations.py:41] ###################################
I20241211 10:16:16 3820504 dinov2 loaders.py:94] using dataset: "CelebAMaskedTrain"
I20241211 10:16:19 3820504 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241211 10:16:19 3820504 dinov2 loaders.py:132] sampler: sharded infinite
I20241211 10:16:19 3820504 dinov2 loaders.py:216] using PyTorch data loader
W20241211 10:16:19 3820504 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241211 10:16:19 3820504 dinov2 loaders.py:231] infinite data loader
I20241211 10:16:19 3820504 dinov2 train.py:217] Starting training from iteration 0
W20241211 10:16:40 3820504 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/dinov2_env/lib/python3.9/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage_data_ptr = tensors[0].storage().data_ptr()

W20241211 10:16:40 3820504 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/dinov2_env/lib/python3.9/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  if x.storage().data_ptr() != storage_data_ptr:

I20241211 10:16:42 3820504 dinov2 helpers.py:102] Training  [     0/125000]  eta: 33 days, 6:26:29  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 13.7393 (13.7393)  dino_local_crops_loss: 9.1752 (9.1752)  dino_global_crops_loss: 1.1469 (1.1469)  koleo_loss: 0.6416 (0.6416)  ibot_loss: 2.7757 (2.7757)  time: 22.995115  data: 4.053607  max mem: 8182
I20241211 10:17:04 3820504 dinov2 helpers.py:102] Training  [    10/125000]  eta: 5 days, 21:20:40  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.1415 (14.1126)  dino_local_crops_loss: 9.5212 (9.4937)  dino_global_crops_loss: 1.1902 (1.1867)  koleo_loss: 0.6392 (0.6411)  ibot_loss: 2.7924 (2.7911)  time: 4.071052  data: 0.368708  max mem: 10690
I20241211 10:17:27 3820504 dinov2 helpers.py:102] Training  [    20/125000]  eta: 4 days, 16:24:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3823 (14.3120)  dino_local_crops_loss: 9.7293 (9.6654)  dino_global_crops_loss: 1.2162 (1.2082)  koleo_loss: 0.6377 (0.6390)  ibot_loss: 2.8011 (2.7995)  time: 2.249960  data: 0.000190  max mem: 10692
I20241211 10:17:51 3820504 dinov2 helpers.py:102] Training  [    30/125000]  eta: 4 days, 6:18:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.6134 (14.4233)  dino_local_crops_loss: 9.9259 (9.7634)  dino_global_crops_loss: 1.2407 (1.2204)  koleo_loss: 0.6328 (0.6354)  ibot_loss: 2.8119 (2.8040)  time: 2.329210  data: 0.000189  max mem: 10692
I20241211 10:18:15 3820504 dinov2 helpers.py:102] Training  [    40/125000]  eta: 4 days, 1:58:15  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.6740 (14.4875)  dino_local_crops_loss: 9.9939 (9.8232)  dino_global_crops_loss: 1.2492 (1.2279)  koleo_loss: 0.6182 (0.6296)  ibot_loss: 2.8145 (2.8068)  time: 2.386340  data: 0.000216  max mem: 10692
I20241211 10:18:40 3820504 dinov2 helpers.py:102] Training  [    50/125000]  eta: 3 days, 23:27:46  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.6855 (14.5259)  dino_local_crops_loss: 10.0163 (9.8618)  dino_global_crops_loss: 1.2520 (1.2327)  koleo_loss: 0.6016 (0.6225)  ibot_loss: 2.8162 (2.8089)  time: 2.445315  data: 0.000205  max mem: 10692
I20241211 10:19:05 3820504 dinov2 helpers.py:102] Training  [    60/125000]  eta: 3 days, 22:13:59  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.6743 (14.5465)  dino_local_crops_loss: 10.0207 (9.8879)  dino_global_crops_loss: 1.2526 (1.2360)  koleo_loss: 0.5835 (0.6126)  ibot_loss: 2.8163 (2.8100)  time: 2.495353  data: 0.000196  max mem: 10692
I20241211 10:19:30 3820504 dinov2 helpers.py:102] Training  [    70/125000]  eta: 3 days, 21:04:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.6370 (14.5567)  dino_local_crops_loss: 10.0185 (9.9059)  dino_global_crops_loss: 1.2523 (1.2382)  koleo_loss: 0.5488 (0.6018)  ibot_loss: 2.8155 (2.8108)  time: 2.507793  data: 0.000223  max mem: 10692
I20241211 10:19:54 3820504 dinov2 helpers.py:102] Training  [    80/125000]  eta: 3 days, 20:04:05  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.5991 (14.5596)  dino_local_crops_loss: 10.0112 (9.9183)  dino_global_crops_loss: 1.2514 (1.2398)  koleo_loss: 0.5205 (0.5901)  ibot_loss: 2.8156 (2.8115)  time: 2.464297  data: 0.000209  max mem: 10692
I20241211 10:20:19 3820504 dinov2 helpers.py:102] Training  [    90/125000]  eta: 3 days, 19:22:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.5589 (14.5568)  dino_local_crops_loss: 9.9999 (9.9266)  dino_global_crops_loss: 1.2500 (1.2408)  koleo_loss: 0.4907 (0.5774)  ibot_loss: 2.8155 (2.8120)  time: 2.460277  data: 0.000184  max mem: 10694
I20241211 10:20:44 3820504 dinov2 helpers.py:102] Training  [   100/125000]  eta: 3 days, 18:51:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.4954 (14.5486)  dino_local_crops_loss: 9.9868 (9.9319)  dino_global_crops_loss: 1.2484 (1.2415)  koleo_loss: 0.4460 (0.5628)  ibot_loss: 2.8153 (2.8123)  time: 2.479564  data: 0.000194  max mem: 10694
I20241211 10:21:09 3820504 dinov2 helpers.py:102] Training  [   110/125000]  eta: 3 days, 18:23:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.4658 (14.5396)  dino_local_crops_loss: 9.9736 (9.9352)  dino_global_crops_loss: 1.2467 (1.2419)  koleo_loss: 0.4272 (0.5498)  ibot_loss: 2.8153 (2.8126)  time: 2.479580  data: 0.000185  max mem: 10694
I20241211 10:21:34 3820504 dinov2 helpers.py:102] Training  [   120/125000]  eta: 3 days, 18:08:10  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.4155 (14.5285)  dino_local_crops_loss: 9.9621 (9.9371)  dino_global_crops_loss: 1.2453 (1.2421)  koleo_loss: 0.3926 (0.5366)  ibot_loss: 2.8152 (2.8127)  time: 2.495161  data: 0.000181  max mem: 10694
I20241211 10:21:59 3820504 dinov2 helpers.py:102] Training  [   130/125000]  eta: 3 days, 17:54:53  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3926 (14.5177)  dino_local_crops_loss: 9.9525 (9.9380)  dino_global_crops_loss: 1.2441 (1.2422)  koleo_loss: 0.3821 (0.5246)  ibot_loss: 2.8143 (2.8128)  time: 2.517947  data: 0.000192  max mem: 10694
I20241211 10:22:24 3820504 dinov2 helpers.py:102] Training  [   140/125000]  eta: 3 days, 17:43:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3831 (14.5076)  dino_local_crops_loss: 9.9450 (9.9383)  dino_global_crops_loss: 1.2431 (1.2423)  koleo_loss: 0.3784 (0.5142)  ibot_loss: 2.8134 (2.8129)  time: 2.519155  data: 0.000205  max mem: 10694
I20241211 10:22:49 3820504 dinov2 helpers.py:102] Training  [   150/125000]  eta: 3 days, 17:31:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3592 (14.4968)  dino_local_crops_loss: 9.9384 (9.9381)  dino_global_crops_loss: 1.2423 (1.2423)  koleo_loss: 0.3628 (0.5036)  ibot_loss: 2.8122 (2.8128)  time: 2.509889  data: 0.000200  max mem: 10694
I20241211 10:23:14 3820504 dinov2 helpers.py:102] Training  [   160/125000]  eta: 3 days, 17:19:58  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3279 (14.4858)  dino_local_crops_loss: 9.9326 (9.9376)  dino_global_crops_loss: 1.2416 (1.2422)  koleo_loss: 0.3447 (0.4933)  ibot_loss: 2.8114 (2.8127)  time: 2.497892  data: 0.000179  max mem: 10694
I20241211 10:23:39 3820504 dinov2 helpers.py:102] Training  [   170/125000]  eta: 3 days, 17:10:26  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.3011 (14.4743)  dino_local_crops_loss: 9.9267 (9.9368)  dino_global_crops_loss: 1.2408 (1.2421)  koleo_loss: 0.3235 (0.4829)  ibot_loss: 2.8105 (2.8125)  time: 2.499009  data: 0.000167  max mem: 10694
I20241211 10:24:04 3820504 dinov2 helpers.py:102] Training  [   180/125000]  eta: 3 days, 17:03:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.2776 (14.4626)  dino_local_crops_loss: 9.9210 (9.9358)  dino_global_crops_loss: 1.2401 (1.2420)  koleo_loss: 0.3059 (0.4725)  ibot_loss: 2.8086 (2.8123)  time: 2.509480  data: 0.000192  max mem: 10694
I20241211 10:24:30 3820504 dinov2 helpers.py:102] Training  [   190/125000]  eta: 3 days, 16:57:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.2408 (14.4505)  dino_local_crops_loss: 9.9148 (9.9345)  dino_global_crops_loss: 1.2393 (1.2418)  koleo_loss: 0.2817 (0.4622)  ibot_loss: 2.8076 (2.8120)  time: 2.518127  data: 0.000190  max mem: 10694
I20241211 10:24:55 3820504 dinov2 helpers.py:102] Training  [   200/125000]  eta: 3 days, 16:50:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.2193 (14.4379)  dino_local_crops_loss: 9.9073 (9.9330)  dino_global_crops_loss: 1.2384 (1.2416)  koleo_loss: 0.2644 (0.4516)  ibot_loss: 2.8068 (2.8117)  time: 2.510096  data: 0.000175  max mem: 10694
I20241211 10:25:20 3820504 dinov2 helpers.py:102] Training  [   210/125000]  eta: 3 days, 16:45:21  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.1783 (14.4252)  dino_local_crops_loss: 9.8998 (9.9313)  dino_global_crops_loss: 1.2375 (1.2414)  koleo_loss: 0.2373 (0.4412)  ibot_loss: 2.8051 (2.8114)  time: 2.506775  data: 0.000180  max mem: 10694
I20241211 10:25:45 3820504 dinov2 helpers.py:102] Training  [   220/125000]  eta: 3 days, 16:41:07  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.1500 (14.4119)  dino_local_crops_loss: 9.8924 (9.9293)  dino_global_crops_loss: 1.2366 (1.2412)  koleo_loss: 0.2197 (0.4306)  ibot_loss: 2.8023 (2.8109)  time: 2.515960  data: 0.000179  max mem: 10694
I20241211 10:26:10 3820504 dinov2 helpers.py:102] Training  [   230/125000]  eta: 3 days, 16:35:30  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.1131 (14.3983)  dino_local_crops_loss: 9.8833 (9.9272)  dino_global_crops_loss: 1.2356 (1.2409)  koleo_loss: 0.1968 (0.4198)  ibot_loss: 2.8004 (2.8104)  time: 2.510508  data: 0.000179  max mem: 10694
I20241211 10:26:35 3820504 dinov2 helpers.py:102] Training  [   240/125000]  eta: 3 days, 16:30:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.0817 (14.3842)  dino_local_crops_loss: 9.8761 (9.9249)  dino_global_crops_loss: 1.2346 (1.2406)  koleo_loss: 0.1693 (0.4089)  ibot_loss: 2.8000 (2.8099)  time: 2.502754  data: 0.000192  max mem: 10694
I20241211 10:27:00 3820504 dinov2 helpers.py:102] Training  [   250/125000]  eta: 3 days, 16:26:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 14.0366 (14.3697)  dino_local_crops_loss: 9.8677 (9.9225)  dino_global_crops_loss: 1.2335 (1.2403)  koleo_loss: 0.1418 (0.3977)  ibot_loss: 2.7949 (2.8092)  time: 2.504322  data: 0.000190  max mem: 10694
I20241211 10:27:25 3820504 dinov2 helpers.py:102] Training  [   260/125000]  eta: 3 days, 16:24:38  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 12.0000 (12.0000)  total_loss: 13.9946 (14.3550)  dino_local_crops_loss: 9.8612 (9.9200)  dino_global_crops_loss: 1.2330 (1.2400)  koleo_loss: 0.1121 (0.3863)  ibot_loss: 2.7934 (2.8086)  time: 2.521769  data: 0.000176  max mem: 10694
