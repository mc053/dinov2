submitit INFO (2024-12-03 06:31:07,592) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-03 06:31:07,592) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:16 1958667 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:16 1958667 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:16 1958667 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:16 1958667 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:16 1958667 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:53 1958667 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:58 1958667 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:58 1958667 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:32:13 1958667 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:32:13 1958667 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:15 1958667 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:15 1958667 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:15 1958667 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:15 1958667 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:15 1958667 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:15 1958667 dinov2 loaders.py:223] # of batches: 634
I20241203 06:33:12 1958667 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:33:12 1958667 dinov2 helpers.py:102]   [  0/634]  eta: 9:56:58    time: 56.496399  data: 14.680174  max mem: 3463
I20241203 06:33:43 1958667 dinov2 helpers.py:102]   [ 10/634]  eta: 1:22:39    time: 7.947360  data: 1.339178  max mem: 4109
I20241203 06:34:22 1958667 dinov2 helpers.py:102]   [ 20/634]  eta: 1:01:40    time: 3.503309  data: 0.002939  max mem: 4109
I20241203 06:35:01 1958667 dinov2 helpers.py:102]   [ 30/634]  eta: 0:53:55    time: 3.932341  data: 0.001103  max mem: 4109
I20241203 06:35:41 1958667 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:39    time: 3.953065  data: 0.001049  max mem: 4109
I20241203 06:36:20 1958667 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:47    time: 3.956086  data: 0.001933  max mem: 4109
I20241203 06:37:00 1958667 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:40    time: 3.960673  data: 0.001905  max mem: 4109
I20241203 06:37:40 1958667 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:57    time: 3.965348  data: 0.000729  max mem: 4109
I20241203 06:38:20 1958667 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:31    time: 3.971951  data: 0.001592  max mem: 4109
I20241203 06:38:59 1958667 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:15    time: 3.976173  data: 0.001438  max mem: 4109
I20241203 06:39:39 1958667 dinov2 helpers.py:102]   [100/634]  eta: 0:39:06    time: 3.974380  data: 0.000732  max mem: 4109
I20241203 06:40:19 1958667 dinov2 helpers.py:102]   [110/634]  eta: 0:38:02    time: 3.974408  data: 0.000817  max mem: 4109
I20241203 06:40:59 1958667 dinov2 helpers.py:102]   [120/634]  eta: 0:37:02    time: 3.974335  data: 0.000739  max mem: 4109
I20241203 06:41:38 1958667 dinov2 helpers.py:102]   [130/634]  eta: 0:36:06    time: 3.974307  data: 0.001293  max mem: 4109
I20241203 06:42:18 1958667 dinov2 helpers.py:102]   [140/634]  eta: 0:35:11    time: 3.974154  data: 0.001374  max mem: 4109
I20241203 06:42:58 1958667 dinov2 helpers.py:102]   [150/634]  eta: 0:34:19    time: 3.973727  data: 0.001488  max mem: 4109
I20241203 06:43:37 1958667 dinov2 helpers.py:102]   [160/634]  eta: 0:33:28    time: 3.973233  data: 0.001689  max mem: 4109
I20241203 06:44:17 1958667 dinov2 helpers.py:102]   [170/634]  eta: 0:32:38    time: 3.973374  data: 0.001907  max mem: 4109
I20241203 06:44:57 1958667 dinov2 helpers.py:102]   [180/634]  eta: 0:31:50    time: 3.973764  data: 0.001787  max mem: 4109
I20241203 06:45:37 1958667 dinov2 helpers.py:102]   [190/634]  eta: 0:31:02    time: 3.974014  data: 0.001126  max mem: 4109
I20241203 06:46:16 1958667 dinov2 helpers.py:102]   [200/634]  eta: 0:30:16    time: 3.973796  data: 0.001227  max mem: 4109
I20241203 06:46:56 1958667 dinov2 helpers.py:102]   [210/634]  eta: 0:29:30    time: 3.973668  data: 0.002465  max mem: 4109
I20241203 06:47:36 1958667 dinov2 helpers.py:102]   [220/634]  eta: 0:28:44    time: 3.973900  data: 0.002890  max mem: 4109
I20241203 06:48:16 1958667 dinov2 helpers.py:102]   [230/634]  eta: 0:27:59    time: 3.974025  data: 0.001575  max mem: 4109
I20241203 06:48:55 1958667 dinov2 helpers.py:102]   [240/634]  eta: 0:27:15    time: 3.974230  data: 0.001171  max mem: 4109
I20241203 06:49:35 1958667 dinov2 helpers.py:102]   [250/634]  eta: 0:26:30    time: 3.974365  data: 0.001302  max mem: 4109
I20241203 06:50:15 1958667 dinov2 helpers.py:102]   [260/634]  eta: 0:25:47    time: 3.974046  data: 0.001997  max mem: 4109
I20241203 06:50:55 1958667 dinov2 helpers.py:102]   [270/634]  eta: 0:25:03    time: 3.973272  data: 0.002320  max mem: 4109
I20241203 06:51:34 1958667 dinov2 helpers.py:102]   [280/634]  eta: 0:24:20    time: 3.971539  data: 0.001709  max mem: 4109
I20241203 06:52:14 1958667 dinov2 helpers.py:102]   [290/634]  eta: 0:23:36    time: 3.967035  data: 0.001076  max mem: 4109
I20241203 06:52:54 1958667 dinov2 helpers.py:102]   [300/634]  eta: 0:22:54    time: 3.962258  data: 0.000855  max mem: 4109
I20241203 06:53:33 1958667 dinov2 helpers.py:102]   [310/634]  eta: 0:22:11    time: 3.961646  data: 0.001213  max mem: 4109
I20241203 06:54:13 1958667 dinov2 helpers.py:102]   [320/634]  eta: 0:21:28    time: 3.958153  data: 0.001540  max mem: 4109
I20241203 06:54:52 1958667 dinov2 helpers.py:102]   [330/634]  eta: 0:20:46    time: 3.960044  data: 0.003394  max mem: 4109
I20241203 06:55:32 1958667 dinov2 helpers.py:102]   [340/634]  eta: 0:20:04    time: 3.963048  data: 0.003080  max mem: 4109
I20241203 06:56:12 1958667 dinov2 helpers.py:102]   [350/634]  eta: 0:19:22    time: 3.963495  data: 0.001511  max mem: 4109
I20241203 06:56:51 1958667 dinov2 helpers.py:102]   [360/634]  eta: 0:18:40    time: 3.970433  data: 0.001552  max mem: 4109
I20241203 06:57:31 1958667 dinov2 helpers.py:102]   [370/634]  eta: 0:17:58    time: 3.973445  data: 0.000893  max mem: 4109
I20241203 06:58:11 1958667 dinov2 helpers.py:102]   [380/634]  eta: 0:17:17    time: 3.973535  data: 0.001018  max mem: 4109
I20241203 06:58:51 1958667 dinov2 helpers.py:102]   [390/634]  eta: 0:16:35    time: 3.973768  data: 0.001233  max mem: 4109
I20241203 06:59:30 1958667 dinov2 helpers.py:102]   [400/634]  eta: 0:15:54    time: 3.974596  data: 0.000847  max mem: 4109
I20241203 07:00:10 1958667 dinov2 helpers.py:102]   [410/634]  eta: 0:15:12    time: 3.974692  data: 0.000908  max mem: 4109
I20241203 07:00:50 1958667 dinov2 helpers.py:102]   [420/634]  eta: 0:14:31    time: 3.974194  data: 0.000978  max mem: 4109
I20241203 07:01:30 1958667 dinov2 helpers.py:102]   [430/634]  eta: 0:13:50    time: 3.974072  data: 0.000805  max mem: 4109
I20241203 07:02:09 1958667 dinov2 helpers.py:102]   [440/634]  eta: 0:13:09    time: 3.975783  data: 0.000893  max mem: 4109
I20241203 07:02:49 1958667 dinov2 helpers.py:102]   [450/634]  eta: 0:12:28    time: 3.975933  data: 0.001181  max mem: 4109
I20241203 07:03:29 1958667 dinov2 helpers.py:102]   [460/634]  eta: 0:11:47    time: 3.974251  data: 0.001029  max mem: 4109
I20241203 07:04:09 1958667 dinov2 helpers.py:102]   [470/634]  eta: 0:11:06    time: 3.976044  data: 0.001128  max mem: 4109
I20241203 07:04:48 1958667 dinov2 helpers.py:102]   [480/634]  eta: 0:10:25    time: 3.975894  data: 0.001326  max mem: 4109
I20241203 07:05:28 1958667 dinov2 helpers.py:102]   [490/634]  eta: 0:09:44    time: 3.974220  data: 0.001733  max mem: 4109
I20241203 07:06:08 1958667 dinov2 helpers.py:102]   [500/634]  eta: 0:09:03    time: 3.974249  data: 0.002018  max mem: 4109
I20241203 07:06:48 1958667 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.974059  data: 0.001270  max mem: 4109
I20241203 07:07:27 1958667 dinov2 helpers.py:102]   [520/634]  eta: 0:07:42    time: 3.974280  data: 0.001489  max mem: 4109
I20241203 07:08:07 1958667 dinov2 helpers.py:102]   [530/634]  eta: 0:07:01    time: 3.974170  data: 0.001730  max mem: 4109
I20241203 07:08:47 1958667 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.973984  data: 0.001010  max mem: 4109
I20241203 07:09:27 1958667 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.975125  data: 0.001130  max mem: 4109
I20241203 07:10:06 1958667 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.976140  data: 0.002514  max mem: 4109
I20241203 07:10:46 1958667 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.975175  data: 0.003414  max mem: 4109
I20241203 07:11:26 1958667 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.974184  data: 0.002283  max mem: 4109
I20241203 07:12:06 1958667 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.975901  data: 0.001146  max mem: 4109
I20241203 07:12:45 1958667 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.974283  data: 0.001986  max mem: 4109
I20241203 07:13:20 1958667 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.739160  data: 0.002054  max mem: 4109
I20241203 07:13:49 1958667 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.194727  data: 0.000880  max mem: 4109
I20241203 07:14:13 1958667 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.607782  data: 0.000509  max mem: 4109
I20241203 07:14:22 1958667 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.572569  data: 0.000449  max mem: 4109
I20241203 07:14:23 1958667 dinov2 helpers.py:130]  Total time: 0:42:07 (3.986218 s / it)
I20241203 07:14:23 1958667 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:14:23 1958667 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:14:23 1958667 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:14:23 1958667 dinov2 loaders.py:151] sampler: distributed
I20241203 07:14:23 1958667 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:14:23 1958667 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:31 1958667 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:45,479) - Submitted job triggered an exception
E20241203 07:14:45 1958667 submitit submission.py:68] Submitted job triggered an exception
