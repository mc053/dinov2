submitit INFO (2024-12-04 09:27:28,381) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-04 09:27:28,381) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:37 2542582 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:37 2542582 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:37 2542582 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:37 2542582 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:37 2542582 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:13 2542582 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:18 2542582 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:18 2542582 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:29 2542582 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:29 2542582 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:34 2542582 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:34 2542582 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:34 2542582 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:34 2542582 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:34 2542582 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:34 2542582 dinov2 loaders.py:223] # of batches: 634
I20241204 09:29:22 2542582 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:29:22 2542582 dinov2 helpers.py:102]   [  0/634]  eta: 8:28:55    time: 48.163261  data: 11.113194  max mem: 3463
I20241204 09:29:51 2542582 dinov2 helpers.py:102]   [ 10/634]  eta: 1:13:13    time: 7.040928  data: 1.013548  max mem: 4109
I20241204 09:30:31 2542582 dinov2 helpers.py:102]   [ 20/634]  eta: 0:56:56    time: 3.434427  data: 0.002736  max mem: 4109
I20241204 09:31:10 2542582 dinov2 helpers.py:102]   [ 30/634]  eta: 0:50:46    time: 3.946702  data: 0.001377  max mem: 4109
I20241204 09:31:50 2542582 dinov2 helpers.py:102]   [ 40/634]  eta: 0:47:20    time: 3.961096  data: 0.000774  max mem: 4109
I20241204 09:32:30 2542582 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:00    time: 3.972029  data: 0.001443  max mem: 4109
I20241204 09:33:09 2542582 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:12    time: 3.974362  data: 0.001509  max mem: 4109
I20241204 09:33:49 2542582 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:45    time: 3.976611  data: 0.001007  max mem: 4109
I20241204 09:34:29 2542582 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:28    time: 3.977610  data: 0.001161  max mem: 4109
I20241204 09:35:09 2542582 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:20    time: 3.976577  data: 0.001059  max mem: 4109
I20241204 09:35:48 2542582 dinov2 helpers.py:102]   [100/634]  eta: 0:38:17    time: 3.975834  data: 0.001127  max mem: 4109
I20241204 09:36:28 2542582 dinov2 helpers.py:102]   [110/634]  eta: 0:37:19    time: 3.974128  data: 0.001369  max mem: 4109
I20241204 09:37:08 2542582 dinov2 helpers.py:102]   [120/634]  eta: 0:36:24    time: 3.975924  data: 0.002050  max mem: 4109
I20241204 09:37:48 2542582 dinov2 helpers.py:102]   [130/634]  eta: 0:35:31    time: 3.977749  data: 0.002833  max mem: 4109
I20241204 09:38:28 2542582 dinov2 helpers.py:102]   [140/634]  eta: 0:34:40    time: 3.979539  data: 0.002039  max mem: 4109
I20241204 09:39:07 2542582 dinov2 helpers.py:102]   [150/634]  eta: 0:33:50    time: 3.979553  data: 0.001927  max mem: 4109
I20241204 09:39:47 2542582 dinov2 helpers.py:102]   [160/634]  eta: 0:33:02    time: 3.978676  data: 0.001907  max mem: 4109
I20241204 09:40:27 2542582 dinov2 helpers.py:102]   [170/634]  eta: 0:32:14    time: 3.977771  data: 0.001019  max mem: 4109
I20241204 09:41:07 2542582 dinov2 helpers.py:102]   [180/634]  eta: 0:31:28    time: 3.979604  data: 0.000942  max mem: 4109
I20241204 09:41:47 2542582 dinov2 helpers.py:102]   [190/634]  eta: 0:30:42    time: 3.984986  data: 0.000870  max mem: 4109
I20241204 09:42:26 2542582 dinov2 helpers.py:102]   [200/634]  eta: 0:29:57    time: 3.983215  data: 0.002160  max mem: 4109
I20241204 09:43:06 2542582 dinov2 helpers.py:102]   [210/634]  eta: 0:29:13    time: 3.980605  data: 0.002127  max mem: 4109
I20241204 09:43:46 2542582 dinov2 helpers.py:102]   [220/634]  eta: 0:28:28    time: 3.982285  data: 0.000834  max mem: 4109
I20241204 09:44:26 2542582 dinov2 helpers.py:102]   [230/634]  eta: 0:27:45    time: 3.984233  data: 0.000916  max mem: 4109
I20241204 09:45:06 2542582 dinov2 helpers.py:102]   [240/634]  eta: 0:27:01    time: 3.986049  data: 0.000740  max mem: 4109
I20241204 09:45:46 2542582 dinov2 helpers.py:102]   [250/634]  eta: 0:26:18    time: 3.984060  data: 0.000586  max mem: 4109
I20241204 09:46:25 2542582 dinov2 helpers.py:102]   [260/634]  eta: 0:25:35    time: 3.981545  data: 0.000750  max mem: 4109
I20241204 09:47:05 2542582 dinov2 helpers.py:102]   [270/634]  eta: 0:24:52    time: 3.977930  data: 0.000880  max mem: 4109
I20241204 09:47:45 2542582 dinov2 helpers.py:102]   [280/634]  eta: 0:24:10    time: 3.978797  data: 0.000886  max mem: 4109
I20241204 09:48:25 2542582 dinov2 helpers.py:102]   [290/634]  eta: 0:23:27    time: 3.981421  data: 0.000661  max mem: 4109
I20241204 09:49:05 2542582 dinov2 helpers.py:102]   [300/634]  eta: 0:22:45    time: 3.981395  data: 0.001696  max mem: 4109
I20241204 09:49:44 2542582 dinov2 helpers.py:102]   [310/634]  eta: 0:22:03    time: 3.983260  data: 0.002059  max mem: 4109
I20241204 09:50:24 2542582 dinov2 helpers.py:102]   [320/634]  eta: 0:21:21    time: 3.980521  data: 0.000965  max mem: 4109
I20241204 09:51:04 2542582 dinov2 helpers.py:102]   [330/634]  eta: 0:20:40    time: 3.976894  data: 0.001886  max mem: 4109
I20241204 09:51:44 2542582 dinov2 helpers.py:102]   [340/634]  eta: 0:19:58    time: 3.977725  data: 0.001954  max mem: 4109
I20241204 09:52:24 2542582 dinov2 helpers.py:102]   [350/634]  eta: 0:19:16    time: 3.980250  data: 0.000838  max mem: 4109
I20241204 09:53:03 2542582 dinov2 helpers.py:102]   [360/634]  eta: 0:18:35    time: 3.979396  data: 0.000786  max mem: 4109
I20241204 09:53:43 2542582 dinov2 helpers.py:102]   [370/634]  eta: 0:17:54    time: 3.980461  data: 0.001060  max mem: 4109
I20241204 09:54:23 2542582 dinov2 helpers.py:102]   [380/634]  eta: 0:17:12    time: 3.981246  data: 0.001040  max mem: 4109
I20241204 09:55:03 2542582 dinov2 helpers.py:102]   [390/634]  eta: 0:16:31    time: 3.982255  data: 0.000931  max mem: 4109
I20241204 09:55:43 2542582 dinov2 helpers.py:102]   [400/634]  eta: 0:15:50    time: 3.987008  data: 0.001340  max mem: 4109
I20241204 09:56:23 2542582 dinov2 helpers.py:102]   [410/634]  eta: 0:15:09    time: 3.987907  data: 0.001032  max mem: 4109
I20241204 09:57:02 2542582 dinov2 helpers.py:102]   [420/634]  eta: 0:14:28    time: 3.986071  data: 0.000655  max mem: 4109
I20241204 09:57:42 2542582 dinov2 helpers.py:102]   [430/634]  eta: 0:13:47    time: 3.987319  data: 0.000955  max mem: 4109
I20241204 09:58:22 2542582 dinov2 helpers.py:102]   [440/634]  eta: 0:13:06    time: 3.987386  data: 0.001131  max mem: 4109
I20241204 09:59:02 2542582 dinov2 helpers.py:102]   [450/634]  eta: 0:12:25    time: 3.983439  data: 0.000928  max mem: 4109
I20241204 09:59:42 2542582 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.984182  data: 0.000776  max mem: 4109
I20241204 10:00:22 2542582 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.985137  data: 0.000896  max mem: 4109
I20241204 10:01:02 2542582 dinov2 helpers.py:102]   [480/634]  eta: 0:10:23    time: 3.979863  data: 0.000869  max mem: 4109
I20241204 10:01:41 2542582 dinov2 helpers.py:102]   [490/634]  eta: 0:09:42    time: 3.978747  data: 0.000820  max mem: 4109
I20241204 10:02:21 2542582 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.979428  data: 0.000905  max mem: 4109
I20241204 10:03:01 2542582 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.980413  data: 0.001113  max mem: 4109
I20241204 10:03:41 2542582 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.983169  data: 0.001469  max mem: 4109
I20241204 10:04:21 2542582 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.982381  data: 0.001357  max mem: 4109
I20241204 10:05:00 2542582 dinov2 helpers.py:102]   [540/634]  eta: 0:06:19    time: 3.981379  data: 0.001213  max mem: 4109
I20241204 10:05:40 2542582 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.977496  data: 0.001195  max mem: 4109
I20241204 10:06:20 2542582 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.973829  data: 0.001010  max mem: 4109
I20241204 10:07:00 2542582 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.973588  data: 0.001587  max mem: 4109
I20241204 10:07:39 2542582 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.973483  data: 0.001576  max mem: 4109
I20241204 10:08:19 2542582 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.973576  data: 0.001074  max mem: 4109
I20241204 10:08:59 2542582 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.973595  data: 0.001018  max mem: 4109
I20241204 10:09:38 2542582 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.929563  data: 0.000724  max mem: 4109
I20241204 10:10:16 2542582 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.877922  data: 0.000776  max mem: 4109
I20241204 10:10:55 2542582 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.853534  data: 0.001226  max mem: 4109
I20241204 10:11:14 2542582 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.222635  data: 0.001178  max mem: 4109
I20241204 10:11:14 2542582 dinov2 helpers.py:130]  Total time: 0:42:40 (4.038558 s / it)
I20241204 10:11:14 2542582 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:11:14 2542582 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:11:15 2542582 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:11:15 2542582 dinov2 loaders.py:151] sampler: distributed
I20241204 10:11:15 2542582 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:11:15 2542582 dinov2 loaders.py:223] # of batches: 78
I20241204 10:11:15 2542582 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:11:25 2542582 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:15    time: 8.664684  data: 5.725370  max mem: 4109
I20241204 10:11:59 2542582 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:25    time: 3.906785  data: 0.528509  max mem: 4109
I20241204 10:12:39 2542582 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:48    time: 3.711481  data: 0.007718  max mem: 4109
I20241204 10:13:19 2542582 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:10    time: 3.992009  data: 0.006783  max mem: 4109
I20241204 10:13:59 2542582 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:30    time: 3.993056  data: 0.009824  max mem: 4109
I20241204 10:14:38 2542582 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:50    time: 3.954475  data: 0.009177  max mem: 4109
I20241204 10:15:10 2542582 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:09    time: 3.579206  data: 0.005806  max mem: 4109
I20241204 10:15:37 2542582 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:29    time: 2.937565  data: 0.004593  max mem: 4109
I20241204 10:15:53 2542582 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.578204  data: 0.002259  max mem: 4109
I20241204 10:15:53 2542582 dinov2 helpers.py:130] Test: Total time: 0:04:36 (3.549876 s / it)
I20241204 10:15:53 2542582 dinov2 utils.py:79] Averaged stats: 
I20241204 10:15:54 2542582 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.15
I20241204 10:15:54 2542582 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.03
I20241204 10:15:54 2542582 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:15:54 2542582 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.92
submitit INFO (2024-12-04 10:15:54,379) - Job completed successfully
I20241204 10:15:54 2542582 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:15:54,381) - Exiting after successful completion
I20241204 10:15:54 2542582 submitit submission.py:61] Exiting after successful completion
