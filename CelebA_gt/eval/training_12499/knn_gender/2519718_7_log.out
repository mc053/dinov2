submitit INFO (2024-12-04 08:36:13,489) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-04 08:36:13,489) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:21 2519726 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:21 2519726 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:21 2519726 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:21 2519726 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:21 2519726 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:56 2519726 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:36:59 2519726 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:36:59 2519726 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:10 2519726 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:10 2519726 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:15 2519726 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:15 2519726 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:15 2519726 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:15 2519726 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:15 2519726 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:15 2519726 dinov2 loaders.py:223] # of batches: 634
I20241204 08:38:09 2519726 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:38:09 2519726 dinov2 helpers.py:102]   [  0/634]  eta: 9:26:24    time: 53.602863  data: 13.750059  max mem: 3463
I20241204 08:38:37 2519726 dinov2 helpers.py:102]   [ 10/634]  eta: 1:17:20    time: 7.436213  data: 1.251015  max mem: 4109
I20241204 08:39:16 2519726 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:03    time: 3.379139  data: 0.001657  max mem: 4109
I20241204 08:39:56 2519726 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:11    time: 3.947126  data: 0.001614  max mem: 4109
I20241204 08:40:36 2519726 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:23    time: 3.959875  data: 0.001090  max mem: 4109
I20241204 08:41:15 2519726 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:48    time: 3.965452  data: 0.001653  max mem: 4109
I20241204 08:41:55 2519726 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:52    time: 3.966589  data: 0.001635  max mem: 4109
I20241204 08:42:35 2519726 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:16    time: 3.965506  data: 0.001254  max mem: 4109
I20241204 08:43:14 2519726 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:55    time: 3.965466  data: 0.001300  max mem: 4109
I20241204 08:43:54 2519726 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:43    time: 3.967221  data: 0.001175  max mem: 4109
I20241204 08:44:34 2519726 dinov2 helpers.py:102]   [100/634]  eta: 0:38:37    time: 3.969345  data: 0.001022  max mem: 4109
I20241204 08:45:13 2519726 dinov2 helpers.py:102]   [110/634]  eta: 0:37:36    time: 3.964783  data: 0.000924  max mem: 4109
I20241204 08:45:53 2519726 dinov2 helpers.py:102]   [120/634]  eta: 0:36:38    time: 3.962367  data: 0.000907  max mem: 4109
I20241204 08:46:33 2519726 dinov2 helpers.py:102]   [130/634]  eta: 0:35:44    time: 3.965141  data: 0.000720  max mem: 4109
I20241204 08:47:12 2519726 dinov2 helpers.py:102]   [140/634]  eta: 0:34:51    time: 3.961030  data: 0.000701  max mem: 4109
I20241204 08:47:52 2519726 dinov2 helpers.py:102]   [150/634]  eta: 0:34:00    time: 3.961709  data: 0.001018  max mem: 4109
I20241204 08:48:32 2519726 dinov2 helpers.py:102]   [160/634]  eta: 0:33:10    time: 3.966115  data: 0.001131  max mem: 4109
I20241204 08:49:11 2519726 dinov2 helpers.py:102]   [170/634]  eta: 0:32:22    time: 3.964602  data: 0.000797  max mem: 4109
I20241204 08:49:51 2519726 dinov2 helpers.py:102]   [180/634]  eta: 0:31:35    time: 3.965181  data: 0.001076  max mem: 4109
I20241204 08:50:30 2519726 dinov2 helpers.py:102]   [190/634]  eta: 0:30:48    time: 3.964485  data: 0.001595  max mem: 4109
I20241204 08:51:10 2519726 dinov2 helpers.py:102]   [200/634]  eta: 0:30:02    time: 3.965611  data: 0.001116  max mem: 4109
I20241204 08:51:50 2519726 dinov2 helpers.py:102]   [210/634]  eta: 0:29:17    time: 3.968862  data: 0.000941  max mem: 4109
I20241204 08:52:29 2519726 dinov2 helpers.py:102]   [220/634]  eta: 0:28:32    time: 3.967169  data: 0.001822  max mem: 4109
I20241204 08:53:09 2519726 dinov2 helpers.py:102]   [230/634]  eta: 0:27:48    time: 3.964474  data: 0.001627  max mem: 4109
I20241204 08:53:49 2519726 dinov2 helpers.py:102]   [240/634]  eta: 0:27:04    time: 3.965225  data: 0.001009  max mem: 4109
I20241204 08:54:28 2519726 dinov2 helpers.py:102]   [250/634]  eta: 0:26:20    time: 3.967916  data: 0.001813  max mem: 4109
I20241204 08:55:08 2519726 dinov2 helpers.py:102]   [260/634]  eta: 0:25:37    time: 3.967929  data: 0.001753  max mem: 4109
I20241204 08:55:48 2519726 dinov2 helpers.py:102]   [270/634]  eta: 0:24:54    time: 3.964343  data: 0.001081  max mem: 4109
I20241204 08:56:27 2519726 dinov2 helpers.py:102]   [280/634]  eta: 0:24:11    time: 3.964312  data: 0.000894  max mem: 4109
I20241204 08:57:07 2519726 dinov2 helpers.py:102]   [290/634]  eta: 0:23:28    time: 3.964378  data: 0.000623  max mem: 4109
I20241204 08:57:47 2519726 dinov2 helpers.py:102]   [300/634]  eta: 0:22:46    time: 3.959780  data: 0.000747  max mem: 4109
I20241204 08:58:26 2519726 dinov2 helpers.py:102]   [310/634]  eta: 0:22:04    time: 3.961450  data: 0.000858  max mem: 4109
I20241204 08:59:06 2519726 dinov2 helpers.py:102]   [320/634]  eta: 0:21:22    time: 3.965105  data: 0.000840  max mem: 4109
I20241204 08:59:46 2519726 dinov2 helpers.py:102]   [330/634]  eta: 0:20:40    time: 3.964083  data: 0.000769  max mem: 4109
I20241204 09:00:25 2519726 dinov2 helpers.py:102]   [340/634]  eta: 0:19:58    time: 3.960374  data: 0.000962  max mem: 4109
I20241204 09:01:05 2519726 dinov2 helpers.py:102]   [350/634]  eta: 0:19:16    time: 3.958521  data: 0.001066  max mem: 4109
I20241204 09:01:44 2519726 dinov2 helpers.py:102]   [360/634]  eta: 0:18:34    time: 3.955783  data: 0.001841  max mem: 4109
I20241204 09:02:24 2519726 dinov2 helpers.py:102]   [370/634]  eta: 0:17:53    time: 3.955639  data: 0.002449  max mem: 4109
I20241204 09:03:03 2519726 dinov2 helpers.py:102]   [380/634]  eta: 0:17:12    time: 3.957289  data: 0.001604  max mem: 4109
I20241204 09:03:43 2519726 dinov2 helpers.py:102]   [390/634]  eta: 0:16:30    time: 3.956231  data: 0.001215  max mem: 4109
I20241204 09:04:23 2519726 dinov2 helpers.py:102]   [400/634]  eta: 0:15:49    time: 3.957895  data: 0.001279  max mem: 4109
I20241204 09:05:02 2519726 dinov2 helpers.py:102]   [410/634]  eta: 0:15:08    time: 3.957188  data: 0.001629  max mem: 4109
I20241204 09:05:42 2519726 dinov2 helpers.py:102]   [420/634]  eta: 0:14:27    time: 3.957219  data: 0.001568  max mem: 4109
I20241204 09:06:21 2519726 dinov2 helpers.py:102]   [430/634]  eta: 0:13:46    time: 3.958935  data: 0.000997  max mem: 4109
I20241204 09:07:01 2519726 dinov2 helpers.py:102]   [440/634]  eta: 0:13:05    time: 3.956263  data: 0.000783  max mem: 4109
I20241204 09:07:40 2519726 dinov2 helpers.py:102]   [450/634]  eta: 0:12:24    time: 3.954293  data: 0.001102  max mem: 4109
I20241204 09:08:20 2519726 dinov2 helpers.py:102]   [460/634]  eta: 0:11:43    time: 3.954238  data: 0.001276  max mem: 4109
I20241204 09:08:59 2519726 dinov2 helpers.py:102]   [470/634]  eta: 0:11:03    time: 3.953685  data: 0.001209  max mem: 4109
I20241204 09:09:39 2519726 dinov2 helpers.py:102]   [480/634]  eta: 0:10:22    time: 3.954472  data: 0.000992  max mem: 4109
I20241204 09:10:19 2519726 dinov2 helpers.py:102]   [490/634]  eta: 0:09:41    time: 3.957009  data: 0.001174  max mem: 4109
I20241204 09:10:58 2519726 dinov2 helpers.py:102]   [500/634]  eta: 0:09:01    time: 3.955428  data: 0.002192  max mem: 4109
I20241204 09:11:38 2519726 dinov2 helpers.py:102]   [510/634]  eta: 0:08:20    time: 3.954481  data: 0.001756  max mem: 4109
I20241204 09:12:17 2519726 dinov2 helpers.py:102]   [520/634]  eta: 0:07:39    time: 3.956357  data: 0.002196  max mem: 4109
I20241204 09:12:57 2519726 dinov2 helpers.py:102]   [530/634]  eta: 0:06:59    time: 3.955018  data: 0.002372  max mem: 4109
I20241204 09:13:36 2519726 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.953169  data: 0.000978  max mem: 4109
I20241204 09:14:16 2519726 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.954835  data: 0.001305  max mem: 4109
I20241204 09:14:55 2519726 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.954653  data: 0.001438  max mem: 4109
I20241204 09:15:35 2519726 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.954530  data: 0.002114  max mem: 4109
I20241204 09:16:15 2519726 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.954854  data: 0.001996  max mem: 4109
I20241204 09:16:54 2519726 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.954852  data: 0.001077  max mem: 4109
I20241204 09:17:34 2519726 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.956393  data: 0.001173  max mem: 4109
I20241204 09:18:13 2519726 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.954865  data: 0.001696  max mem: 4109
I20241204 09:18:42 2519726 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.440023  data: 0.001419  max mem: 4109
I20241204 09:19:07 2519726 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.699271  data: 0.000630  max mem: 4109
I20241204 09:19:19 2519726 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.816904  data: 0.000605  max mem: 4109
I20241204 09:19:19 2519726 dinov2 helpers.py:130]  Total time: 0:42:04 (3.981278 s / it)
I20241204 09:19:19 2519726 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:19:19 2519726 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:19:20 2519726 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:19:20 2519726 dinov2 loaders.py:151] sampler: distributed
I20241204 09:19:20 2519726 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:19:20 2519726 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:19:20,514) - Submitted job triggered an exception
E20241204 09:19:20 2519726 submitit submission.py:68] Submitted job triggered an exception
