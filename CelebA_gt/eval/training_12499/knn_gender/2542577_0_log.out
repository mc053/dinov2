submitit INFO (2024-12-04 09:27:28,365) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-04 09:27:28,365) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:36 2542578 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:36 2542578 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:36 2542578 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:36 2542578 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:36 2542578 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:12 2542578 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:17 2542578 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:17 2542578 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:28 2542578 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:28 2542578 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:32 2542578 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:32 2542578 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:32 2542578 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:32 2542578 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:32 2542578 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:32 2542578 dinov2 loaders.py:223] # of batches: 634
I20241204 09:29:26 2542578 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:29:26 2542578 dinov2 helpers.py:102]   [  0/634]  eta: 9:31:58    time: 54.129700  data: 16.126030  max mem: 3463
I20241204 09:29:58 2542578 dinov2 helpers.py:102]   [ 10/634]  eta: 1:21:10    time: 7.804709  data: 1.467992  max mem: 4109
I20241204 09:30:38 2542578 dinov2 helpers.py:102]   [ 20/634]  eta: 1:01:03    time: 3.559235  data: 0.002358  max mem: 4109
I20241204 09:31:17 2542578 dinov2 helpers.py:102]   [ 30/634]  eta: 0:53:32    time: 3.952178  data: 0.002630  max mem: 4109
I20241204 09:31:57 2542578 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:24    time: 3.965561  data: 0.002836  max mem: 4109
I20241204 09:32:37 2542578 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:38    time: 3.973274  data: 0.001868  max mem: 4109
I20241204 09:33:16 2542578 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:33    time: 3.975695  data: 0.000699  max mem: 4109
I20241204 09:33:56 2542578 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:53    time: 3.979377  data: 0.000925  max mem: 4109
I20241204 09:34:36 2542578 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:27    time: 3.979066  data: 0.001279  max mem: 4109
I20241204 09:35:16 2542578 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:11    time: 3.975611  data: 0.000944  max mem: 4109
I20241204 09:35:55 2542578 dinov2 helpers.py:102]   [100/634]  eta: 0:39:03    time: 3.975856  data: 0.000601  max mem: 4109
I20241204 09:36:35 2542578 dinov2 helpers.py:102]   [110/634]  eta: 0:38:00    time: 3.976833  data: 0.000797  max mem: 4109
I20241204 09:37:15 2542578 dinov2 helpers.py:102]   [120/634]  eta: 0:37:00    time: 3.976061  data: 0.001176  max mem: 4109
I20241204 09:37:55 2542578 dinov2 helpers.py:102]   [130/634]  eta: 0:36:04    time: 3.975855  data: 0.001123  max mem: 4109
I20241204 09:38:35 2542578 dinov2 helpers.py:102]   [140/634]  eta: 0:35:10    time: 3.978134  data: 0.000893  max mem: 4109
I20241204 09:39:14 2542578 dinov2 helpers.py:102]   [150/634]  eta: 0:34:18    time: 3.979712  data: 0.000741  max mem: 4109
I20241204 09:39:54 2542578 dinov2 helpers.py:102]   [160/634]  eta: 0:33:27    time: 3.979919  data: 0.001061  max mem: 4109
I20241204 09:40:34 2542578 dinov2 helpers.py:102]   [170/634]  eta: 0:32:38    time: 3.980466  data: 0.002253  max mem: 4109
I20241204 09:41:14 2542578 dinov2 helpers.py:102]   [180/634]  eta: 0:31:50    time: 3.978736  data: 0.001791  max mem: 4109
I20241204 09:41:54 2542578 dinov2 helpers.py:102]   [190/634]  eta: 0:31:02    time: 3.980482  data: 0.000850  max mem: 4109
I20241204 09:42:33 2542578 dinov2 helpers.py:102]   [200/634]  eta: 0:30:16    time: 3.985078  data: 0.000998  max mem: 4109
I20241204 09:43:13 2542578 dinov2 helpers.py:102]   [210/634]  eta: 0:29:30    time: 3.988622  data: 0.000939  max mem: 4109
I20241204 09:43:53 2542578 dinov2 helpers.py:102]   [220/634]  eta: 0:28:45    time: 3.984946  data: 0.000956  max mem: 4109
I20241204 09:44:33 2542578 dinov2 helpers.py:102]   [230/634]  eta: 0:28:00    time: 3.979672  data: 0.000853  max mem: 4109
I20241204 09:45:13 2542578 dinov2 helpers.py:102]   [240/634]  eta: 0:27:15    time: 3.982483  data: 0.002876  max mem: 4109
I20241204 09:45:53 2542578 dinov2 helpers.py:102]   [250/634]  eta: 0:26:31    time: 3.985034  data: 0.002787  max mem: 4109
I20241204 09:46:32 2542578 dinov2 helpers.py:102]   [260/634]  eta: 0:25:47    time: 3.984108  data: 0.000618  max mem: 4109
I20241204 09:47:12 2542578 dinov2 helpers.py:102]   [270/634]  eta: 0:25:04    time: 3.985192  data: 0.000872  max mem: 4109
I20241204 09:47:52 2542578 dinov2 helpers.py:102]   [280/634]  eta: 0:24:21    time: 3.982435  data: 0.001631  max mem: 4109
I20241204 09:48:32 2542578 dinov2 helpers.py:102]   [290/634]  eta: 0:23:38    time: 3.981358  data: 0.001447  max mem: 4109
I20241204 09:49:12 2542578 dinov2 helpers.py:102]   [300/634]  eta: 0:22:55    time: 3.985830  data: 0.000811  max mem: 4109
I20241204 09:49:52 2542578 dinov2 helpers.py:102]   [310/634]  eta: 0:22:12    time: 3.986856  data: 0.000874  max mem: 4109
I20241204 09:50:32 2542578 dinov2 helpers.py:102]   [320/634]  eta: 0:21:30    time: 3.985942  data: 0.002007  max mem: 4109
I20241204 09:51:11 2542578 dinov2 helpers.py:102]   [330/634]  eta: 0:20:48    time: 3.983972  data: 0.002456  max mem: 4109
I20241204 09:51:51 2542578 dinov2 helpers.py:102]   [340/634]  eta: 0:20:06    time: 3.978613  data: 0.001114  max mem: 4109
I20241204 09:52:31 2542578 dinov2 helpers.py:102]   [350/634]  eta: 0:19:24    time: 3.977698  data: 0.000574  max mem: 4109
I20241204 09:53:11 2542578 dinov2 helpers.py:102]   [360/634]  eta: 0:18:42    time: 3.982202  data: 0.000689  max mem: 4109
I20241204 09:53:51 2542578 dinov2 helpers.py:102]   [370/634]  eta: 0:18:00    time: 3.984003  data: 0.001387  max mem: 4109
I20241204 09:54:30 2542578 dinov2 helpers.py:102]   [380/634]  eta: 0:17:18    time: 3.984831  data: 0.001501  max mem: 4109
I20241204 09:55:10 2542578 dinov2 helpers.py:102]   [390/634]  eta: 0:16:37    time: 3.984116  data: 0.000888  max mem: 4109
I20241204 09:55:50 2542578 dinov2 helpers.py:102]   [400/634]  eta: 0:15:55    time: 3.987883  data: 0.000917  max mem: 4109
I20241204 09:56:30 2542578 dinov2 helpers.py:102]   [410/634]  eta: 0:15:14    time: 3.991505  data: 0.001098  max mem: 4109
I20241204 09:57:10 2542578 dinov2 helpers.py:102]   [420/634]  eta: 0:14:33    time: 3.990704  data: 0.001062  max mem: 4109
I20241204 09:57:50 2542578 dinov2 helpers.py:102]   [430/634]  eta: 0:13:51    time: 3.990871  data: 0.000735  max mem: 4109
I20241204 09:58:30 2542578 dinov2 helpers.py:102]   [440/634]  eta: 0:13:10    time: 3.987327  data: 0.000539  max mem: 4109
I20241204 09:59:10 2542578 dinov2 helpers.py:102]   [450/634]  eta: 0:12:29    time: 3.986194  data: 0.000697  max mem: 4109
I20241204 09:59:50 2542578 dinov2 helpers.py:102]   [460/634]  eta: 0:11:48    time: 3.986850  data: 0.000844  max mem: 4109
I20241204 10:00:29 2542578 dinov2 helpers.py:102]   [470/634]  eta: 0:11:07    time: 3.981523  data: 0.000919  max mem: 4109
I20241204 10:01:09 2542578 dinov2 helpers.py:102]   [480/634]  eta: 0:10:26    time: 3.980647  data: 0.000794  max mem: 4109
I20241204 10:01:49 2542578 dinov2 helpers.py:102]   [490/634]  eta: 0:09:45    time: 3.982276  data: 0.000624  max mem: 4109
I20241204 10:02:29 2542578 dinov2 helpers.py:102]   [500/634]  eta: 0:09:04    time: 3.983941  data: 0.000778  max mem: 4109
I20241204 10:03:09 2542578 dinov2 helpers.py:102]   [510/634]  eta: 0:08:23    time: 3.987582  data: 0.000788  max mem: 4109
I20241204 10:03:48 2542578 dinov2 helpers.py:102]   [520/634]  eta: 0:07:43    time: 3.982361  data: 0.000781  max mem: 4109
I20241204 10:04:28 2542578 dinov2 helpers.py:102]   [530/634]  eta: 0:07:02    time: 3.979014  data: 0.000890  max mem: 4109
I20241204 10:05:08 2542578 dinov2 helpers.py:102]   [540/634]  eta: 0:06:21    time: 3.979527  data: 0.001803  max mem: 4109
I20241204 10:05:48 2542578 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.975525  data: 0.001673  max mem: 4109
I20241204 10:06:28 2542578 dinov2 helpers.py:102]   [560/634]  eta: 0:05:00    time: 3.973709  data: 0.000612  max mem: 4109
I20241204 10:07:07 2542578 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.973526  data: 0.001066  max mem: 4109
I20241204 10:07:47 2542578 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.973462  data: 0.001249  max mem: 4109
I20241204 10:08:27 2542578 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.973678  data: 0.000775  max mem: 4109
I20241204 10:09:07 2542578 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.975432  data: 0.000584  max mem: 4109
I20241204 10:09:45 2542578 dinov2 helpers.py:102]   [610/634]  eta: 0:01:37    time: 3.929298  data: 0.000802  max mem: 4109
I20241204 10:10:24 2542578 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.851429  data: 0.001598  max mem: 4109
I20241204 10:11:02 2542578 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.852997  data: 0.001478  max mem: 4109
I20241204 10:11:21 2542578 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.209507  data: 0.001401  max mem: 4109
I20241204 10:11:21 2542578 dinov2 helpers.py:130]  Total time: 0:42:49 (4.052109 s / it)
I20241204 10:11:21 2542578 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:11:21 2542578 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:11:22 2542578 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:11:22 2542578 dinov2 loaders.py:151] sampler: distributed
I20241204 10:11:22 2542578 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:11:22 2542578 dinov2 loaders.py:223] # of batches: 78
I20241204 10:11:22 2542578 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:11:34 2542578 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:15:27    time: 11.893804  data: 8.937809  max mem: 4109
I20241204 10:12:14 2542578 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:16    time: 4.660074  data: 0.818208  max mem: 4109
I20241204 10:12:54 2542578 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:12    time: 3.972298  data: 0.005306  max mem: 4109
I20241204 10:13:34 2542578 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:23    time: 4.005633  data: 0.004895  max mem: 4109
I20241204 10:14:14 2542578 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:38    time: 4.001054  data: 0.007719  max mem: 4109
I20241204 10:14:51 2542578 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:54    time: 3.860029  data: 0.007041  max mem: 4109
I20241204 10:15:21 2542578 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 3.368375  data: 0.003935  max mem: 4109
I20241204 10:15:46 2542578 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:29    time: 2.766128  data: 0.004635  max mem: 4109
I20241204 10:16:00 2542578 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.336637  data: 0.002991  max mem: 4109
I20241204 10:16:00 2542578 dinov2 helpers.py:130] Test: Total time: 0:04:37 (3.552709 s / it)
I20241204 10:16:00 2542578 dinov2 utils.py:79] Averaged stats: 
I20241204 10:16:00 2542578 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.15
I20241204 10:16:00 2542578 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.03
I20241204 10:16:00 2542578 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:16:00 2542578 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.92
submitit INFO (2024-12-04 10:16:00,637) - Job completed successfully
I20241204 10:16:00 2542578 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:16:00,638) - Exiting after successful completion
I20241204 10:16:00 2542578 submitit submission.py:61] Exiting after successful completion
