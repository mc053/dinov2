submitit INFO (2024-12-04 08:36:13,482) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-04 08:36:13,482) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:21 2519725 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:21 2519725 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:21 2519725 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:21 2519725 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:21 2519725 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:51 2519725 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:36:55 2519725 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:36:56 2519725 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:02 2519725 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:02 2519725 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:05 2519725 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:05 2519725 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:05 2519725 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:05 2519725 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:05 2519725 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:05 2519725 dinov2 loaders.py:223] # of batches: 634
I20241204 08:37:42 2519725 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:37:42 2519725 dinov2 helpers.py:102]   [  0/634]  eta: 6:31:06    time: 37.013569  data: 9.244555  max mem: 3463
I20241204 08:37:54 2519725 dinov2 helpers.py:102]   [ 10/634]  eta: 0:46:26    time: 4.465837  data: 0.842761  max mem: 4109
I20241204 08:38:10 2519725 dinov2 helpers.py:102]   [ 20/634]  eta: 0:31:23    time: 1.369591  data: 0.001505  max mem: 4109
I20241204 08:38:46 2519725 dinov2 helpers.py:102]   [ 30/634]  eta: 0:32:34    time: 2.560078  data: 0.000811  max mem: 4109
I20241204 08:39:25 2519725 dinov2 helpers.py:102]   [ 40/634]  eta: 0:33:44    time: 3.767488  data: 0.001032  max mem: 4109
I20241204 08:40:05 2519725 dinov2 helpers.py:102]   [ 50/634]  eta: 0:34:13    time: 3.948851  data: 0.001117  max mem: 4109
I20241204 08:40:44 2519725 dinov2 helpers.py:102]   [ 60/634]  eta: 0:34:20    time: 3.960504  data: 0.001779  max mem: 4109
I20241204 08:41:24 2519725 dinov2 helpers.py:102]   [ 70/634]  eta: 0:34:14    time: 3.967333  data: 0.001522  max mem: 4109
I20241204 08:42:04 2519725 dinov2 helpers.py:102]   [ 80/634]  eta: 0:34:00    time: 3.967430  data: 0.001067  max mem: 4109
I20241204 08:42:43 2519725 dinov2 helpers.py:102]   [ 90/634]  eta: 0:33:40    time: 3.967407  data: 0.001799  max mem: 4109
I20241204 08:43:23 2519725 dinov2 helpers.py:102]   [100/634]  eta: 0:33:16    time: 3.969092  data: 0.002071  max mem: 4109
I20241204 08:44:03 2519725 dinov2 helpers.py:102]   [110/634]  eta: 0:32:50    time: 3.969837  data: 0.001381  max mem: 4109
I20241204 08:44:42 2519725 dinov2 helpers.py:102]   [120/634]  eta: 0:32:21    time: 3.970171  data: 0.000796  max mem: 4109
I20241204 08:45:22 2519725 dinov2 helpers.py:102]   [130/634]  eta: 0:31:51    time: 3.968432  data: 0.001855  max mem: 4109
I20241204 08:46:02 2519725 dinov2 helpers.py:102]   [140/634]  eta: 0:31:19    time: 3.966170  data: 0.002104  max mem: 4109
I20241204 08:46:41 2519725 dinov2 helpers.py:102]   [150/634]  eta: 0:30:46    time: 3.965109  data: 0.000978  max mem: 4109
I20241204 08:47:21 2519725 dinov2 helpers.py:102]   [160/634]  eta: 0:30:12    time: 3.963480  data: 0.000778  max mem: 4109
I20241204 08:48:01 2519725 dinov2 helpers.py:102]   [170/634]  eta: 0:29:38    time: 3.965266  data: 0.000905  max mem: 4109
I20241204 08:48:40 2519725 dinov2 helpers.py:102]   [180/634]  eta: 0:29:03    time: 3.965321  data: 0.001243  max mem: 4109
I20241204 08:49:20 2519725 dinov2 helpers.py:102]   [190/634]  eta: 0:28:27    time: 3.964511  data: 0.001400  max mem: 4109
I20241204 08:50:00 2519725 dinov2 helpers.py:102]   [200/634]  eta: 0:27:51    time: 3.964396  data: 0.001070  max mem: 4109
I20241204 08:50:39 2519725 dinov2 helpers.py:102]   [210/634]  eta: 0:27:15    time: 3.964695  data: 0.000959  max mem: 4109
I20241204 08:51:19 2519725 dinov2 helpers.py:102]   [220/634]  eta: 0:26:39    time: 3.966494  data: 0.000939  max mem: 4109
I20241204 08:51:58 2519725 dinov2 helpers.py:102]   [230/634]  eta: 0:26:02    time: 3.963356  data: 0.001242  max mem: 4109
I20241204 08:52:38 2519725 dinov2 helpers.py:102]   [240/634]  eta: 0:25:25    time: 3.965232  data: 0.001354  max mem: 4109
I20241204 08:53:18 2519725 dinov2 helpers.py:102]   [250/634]  eta: 0:24:47    time: 3.967230  data: 0.001086  max mem: 4109
I20241204 08:53:57 2519725 dinov2 helpers.py:102]   [260/634]  eta: 0:24:10    time: 3.964381  data: 0.001283  max mem: 4109
I20241204 08:54:37 2519725 dinov2 helpers.py:102]   [270/634]  eta: 0:23:32    time: 3.963421  data: 0.001069  max mem: 4109
I20241204 08:55:17 2519725 dinov2 helpers.py:102]   [280/634]  eta: 0:22:55    time: 3.964356  data: 0.000777  max mem: 4109
I20241204 08:55:56 2519725 dinov2 helpers.py:102]   [290/634]  eta: 0:22:17    time: 3.964327  data: 0.000827  max mem: 4109
I20241204 08:56:36 2519725 dinov2 helpers.py:102]   [300/634]  eta: 0:21:39    time: 3.965265  data: 0.003448  max mem: 4109
I20241204 08:57:16 2519725 dinov2 helpers.py:102]   [310/634]  eta: 0:21:01    time: 3.966450  data: 0.003623  max mem: 4109
I20241204 08:57:55 2519725 dinov2 helpers.py:102]   [320/634]  eta: 0:20:22    time: 3.966007  data: 0.001700  max mem: 4109
I20241204 08:58:35 2519725 dinov2 helpers.py:102]   [330/634]  eta: 0:19:44    time: 3.965611  data: 0.001608  max mem: 4109
I20241204 08:59:15 2519725 dinov2 helpers.py:102]   [340/634]  eta: 0:19:06    time: 3.966059  data: 0.001873  max mem: 4109
I20241204 08:59:54 2519725 dinov2 helpers.py:102]   [350/634]  eta: 0:18:27    time: 3.963275  data: 0.001937  max mem: 4109
I20241204 09:00:34 2519725 dinov2 helpers.py:102]   [360/634]  eta: 0:17:49    time: 3.960323  data: 0.001080  max mem: 4109
I20241204 09:01:14 2519725 dinov2 helpers.py:102]   [370/634]  eta: 0:17:10    time: 3.964759  data: 0.000858  max mem: 4109
I20241204 09:01:53 2519725 dinov2 helpers.py:102]   [380/634]  eta: 0:16:31    time: 3.963786  data: 0.000724  max mem: 4109
I20241204 09:02:33 2519725 dinov2 helpers.py:102]   [390/634]  eta: 0:15:53    time: 3.956434  data: 0.000692  max mem: 4109
I20241204 09:03:12 2519725 dinov2 helpers.py:102]   [400/634]  eta: 0:15:14    time: 3.954497  data: 0.000760  max mem: 4109
I20241204 09:03:52 2519725 dinov2 helpers.py:102]   [410/634]  eta: 0:14:35    time: 3.956248  data: 0.000703  max mem: 4109
I20241204 09:04:31 2519725 dinov2 helpers.py:102]   [420/634]  eta: 0:13:56    time: 3.958931  data: 0.001711  max mem: 4109
I20241204 09:05:11 2519725 dinov2 helpers.py:102]   [430/634]  eta: 0:13:17    time: 3.961713  data: 0.001792  max mem: 4109
I20241204 09:05:51 2519725 dinov2 helpers.py:102]   [440/634]  eta: 0:12:39    time: 3.959077  data: 0.000833  max mem: 4109
I20241204 09:06:30 2519725 dinov2 helpers.py:102]   [450/634]  eta: 0:12:00    time: 3.956254  data: 0.001095  max mem: 4109
I20241204 09:07:10 2519725 dinov2 helpers.py:102]   [460/634]  eta: 0:11:21    time: 3.956205  data: 0.001112  max mem: 4109
I20241204 09:07:49 2519725 dinov2 helpers.py:102]   [470/634]  eta: 0:10:42    time: 3.954273  data: 0.001487  max mem: 4109
I20241204 09:08:29 2519725 dinov2 helpers.py:102]   [480/634]  eta: 0:10:03    time: 3.954244  data: 0.002180  max mem: 4109
I20241204 09:09:08 2519725 dinov2 helpers.py:102]   [490/634]  eta: 0:09:24    time: 3.954231  data: 0.001676  max mem: 4109
I20241204 09:09:48 2519725 dinov2 helpers.py:102]   [500/634]  eta: 0:08:44    time: 3.954394  data: 0.001024  max mem: 4109
I20241204 09:10:28 2519725 dinov2 helpers.py:102]   [510/634]  eta: 0:08:05    time: 3.957407  data: 0.001358  max mem: 4109
I20241204 09:11:07 2519725 dinov2 helpers.py:102]   [520/634]  eta: 0:07:26    time: 3.955467  data: 0.002507  max mem: 4109
I20241204 09:11:47 2519725 dinov2 helpers.py:102]   [530/634]  eta: 0:06:47    time: 3.954482  data: 0.002176  max mem: 4109
I20241204 09:12:26 2519725 dinov2 helpers.py:102]   [540/634]  eta: 0:06:08    time: 3.957386  data: 0.000931  max mem: 4109
I20241204 09:13:06 2519725 dinov2 helpers.py:102]   [550/634]  eta: 0:05:29    time: 3.959532  data: 0.000912  max mem: 4109
I20241204 09:13:45 2519725 dinov2 helpers.py:102]   [560/634]  eta: 0:04:50    time: 3.957770  data: 0.001828  max mem: 4109
I20241204 09:14:25 2519725 dinov2 helpers.py:102]   [570/634]  eta: 0:04:11    time: 3.958410  data: 0.001784  max mem: 4109
I20241204 09:15:04 2519725 dinov2 helpers.py:102]   [580/634]  eta: 0:03:31    time: 3.957222  data: 0.000982  max mem: 4109
I20241204 09:15:44 2519725 dinov2 helpers.py:102]   [590/634]  eta: 0:02:52    time: 3.955455  data: 0.001080  max mem: 4109
I20241204 09:16:24 2519725 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.962905  data: 0.000969  max mem: 4109
I20241204 09:17:03 2519725 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.960161  data: 0.000781  max mem: 4109
I20241204 09:17:43 2519725 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.955546  data: 0.000859  max mem: 4109
I20241204 09:18:21 2519725 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.880791  data: 0.001288  max mem: 4109
I20241204 09:18:35 2519725 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.013235  data: 0.001091  max mem: 4109
I20241204 09:18:36 2519725 dinov2 helpers.py:130]  Total time: 0:41:30 (3.928285 s / it)
I20241204 09:18:36 2519725 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:18:36 2519725 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:18:36 2519725 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:18:36 2519725 dinov2 loaders.py:151] sampler: distributed
I20241204 09:18:36 2519725 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:18:36 2519725 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:18:36,888) - Submitted job triggered an exception
E20241204 09:18:36 2519725 submitit submission.py:68] Submitted job triggered an exception
