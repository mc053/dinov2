submitit INFO (2024-12-04 08:36:13,490) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=2(8), node=0(1), global_rank=2(8))
submitit INFO (2024-12-04 08:36:13,490) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:22 2519721 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:22 2519721 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:22 2519721 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:22 2519721 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:22 2519721 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:57 2519721 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:37:01 2519721 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:37:02 2519721 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:16 2519721 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:16 2519721 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:23 2519721 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:23 2519721 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:23 2519721 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:23 2519721 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:23 2519721 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:23 2519721 dinov2 loaders.py:223] # of batches: 634
I20241204 08:38:18 2519721 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:38:18 2519721 dinov2 helpers.py:102]   [  0/634]  eta: 9:38:03    time: 54.705601  data: 14.777169  max mem: 3463
I20241204 08:38:50 2519721 dinov2 helpers.py:102]   [ 10/634]  eta: 1:22:13    time: 7.905955  data: 1.348764  max mem: 4109
I20241204 08:39:29 2519721 dinov2 helpers.py:102]   [ 20/634]  eta: 1:01:36    time: 3.586238  data: 0.003738  max mem: 4109
I20241204 08:40:09 2519721 dinov2 helpers.py:102]   [ 30/634]  eta: 0:53:54    time: 3.952384  data: 0.001332  max mem: 4109
I20241204 08:40:49 2519721 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:39    time: 3.961327  data: 0.001150  max mem: 4109
I20241204 08:41:28 2519721 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:49    time: 3.965554  data: 0.001188  max mem: 4109
I20241204 08:42:08 2519721 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:41    time: 3.966548  data: 0.001011  max mem: 4109
I20241204 08:42:47 2519721 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:58    time: 3.964710  data: 0.000950  max mem: 4109
I20241204 08:43:27 2519721 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:31    time: 3.963697  data: 0.002098  max mem: 4109
I20241204 08:44:07 2519721 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:14    time: 3.967223  data: 0.002025  max mem: 4109
I20241204 08:44:47 2519721 dinov2 helpers.py:102]   [100/634]  eta: 0:39:05    time: 3.968404  data: 0.001070  max mem: 4109
I20241204 08:45:26 2519721 dinov2 helpers.py:102]   [110/634]  eta: 0:38:01    time: 3.965543  data: 0.001801  max mem: 4109
I20241204 08:46:06 2519721 dinov2 helpers.py:102]   [120/634]  eta: 0:37:01    time: 3.964411  data: 0.002250  max mem: 4109
I20241204 08:46:45 2519721 dinov2 helpers.py:102]   [130/634]  eta: 0:36:04    time: 3.961524  data: 0.002078  max mem: 4109
I20241204 08:47:25 2519721 dinov2 helpers.py:102]   [140/634]  eta: 0:35:09    time: 3.959819  data: 0.001915  max mem: 4109
I20241204 08:48:05 2519721 dinov2 helpers.py:102]   [150/634]  eta: 0:34:17    time: 3.963523  data: 0.001976  max mem: 4109
I20241204 08:48:44 2519721 dinov2 helpers.py:102]   [160/634]  eta: 0:33:26    time: 3.966180  data: 0.001909  max mem: 4109
I20241204 08:49:24 2519721 dinov2 helpers.py:102]   [170/634]  eta: 0:32:36    time: 3.965371  data: 0.001205  max mem: 4109
I20241204 08:50:04 2519721 dinov2 helpers.py:102]   [180/634]  eta: 0:31:48    time: 3.963537  data: 0.001425  max mem: 4109
I20241204 08:50:43 2519721 dinov2 helpers.py:102]   [190/634]  eta: 0:31:00    time: 3.963833  data: 0.001682  max mem: 4109
I20241204 08:51:23 2519721 dinov2 helpers.py:102]   [200/634]  eta: 0:30:13    time: 3.963784  data: 0.002349  max mem: 4109
I20241204 08:52:02 2519721 dinov2 helpers.py:102]   [210/634]  eta: 0:29:27    time: 3.957870  data: 0.002862  max mem: 4109
I20241204 08:52:42 2519721 dinov2 helpers.py:102]   [220/634]  eta: 0:28:41    time: 3.958939  data: 0.001533  max mem: 4109
I20241204 08:53:22 2519721 dinov2 helpers.py:102]   [230/634]  eta: 0:27:56    time: 3.964558  data: 0.001033  max mem: 4109
I20241204 08:54:01 2519721 dinov2 helpers.py:102]   [240/634]  eta: 0:27:12    time: 3.961661  data: 0.000990  max mem: 4109
I20241204 08:54:41 2519721 dinov2 helpers.py:102]   [250/634]  eta: 0:26:28    time: 3.960713  data: 0.000727  max mem: 4109
I20241204 08:55:21 2519721 dinov2 helpers.py:102]   [260/634]  eta: 0:25:44    time: 3.964351  data: 0.000966  max mem: 4109
I20241204 08:56:00 2519721 dinov2 helpers.py:102]   [270/634]  eta: 0:25:00    time: 3.966153  data: 0.001303  max mem: 4109
I20241204 08:56:40 2519721 dinov2 helpers.py:102]   [280/634]  eta: 0:24:17    time: 3.963430  data: 0.001168  max mem: 4109
I20241204 08:57:19 2519721 dinov2 helpers.py:102]   [290/634]  eta: 0:23:34    time: 3.961700  data: 0.001402  max mem: 4109
I20241204 08:57:59 2519721 dinov2 helpers.py:102]   [300/634]  eta: 0:22:51    time: 3.960649  data: 0.001430  max mem: 4109
I20241204 08:58:39 2519721 dinov2 helpers.py:102]   [310/634]  eta: 0:22:09    time: 3.963178  data: 0.001285  max mem: 4109
I20241204 08:59:18 2519721 dinov2 helpers.py:102]   [320/634]  eta: 0:21:26    time: 3.966010  data: 0.001209  max mem: 4109
I20241204 08:59:58 2519721 dinov2 helpers.py:102]   [330/634]  eta: 0:20:44    time: 3.961540  data: 0.001300  max mem: 4109
I20241204 09:00:38 2519721 dinov2 helpers.py:102]   [340/634]  eta: 0:20:02    time: 3.957657  data: 0.001353  max mem: 4109
I20241204 09:01:17 2519721 dinov2 helpers.py:102]   [350/634]  eta: 0:19:20    time: 3.957467  data: 0.000822  max mem: 4109
I20241204 09:01:57 2519721 dinov2 helpers.py:102]   [360/634]  eta: 0:18:38    time: 3.957468  data: 0.000798  max mem: 4109
I20241204 09:02:36 2519721 dinov2 helpers.py:102]   [370/634]  eta: 0:17:56    time: 3.954659  data: 0.000888  max mem: 4109
I20241204 09:03:16 2519721 dinov2 helpers.py:102]   [380/634]  eta: 0:17:15    time: 3.954480  data: 0.000785  max mem: 4109
I20241204 09:03:55 2519721 dinov2 helpers.py:102]   [390/634]  eta: 0:16:33    time: 3.959864  data: 0.000793  max mem: 4109
I20241204 09:04:35 2519721 dinov2 helpers.py:102]   [400/634]  eta: 0:15:52    time: 3.961658  data: 0.001472  max mem: 4109
I20241204 09:05:15 2519721 dinov2 helpers.py:102]   [410/634]  eta: 0:15:11    time: 3.957246  data: 0.001430  max mem: 4109
I20241204 09:05:54 2519721 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.954569  data: 0.000808  max mem: 4109
I20241204 09:06:34 2519721 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.954391  data: 0.000868  max mem: 4109
I20241204 09:07:13 2519721 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.955291  data: 0.001173  max mem: 4109
I20241204 09:07:53 2519721 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.954284  data: 0.001590  max mem: 4109
I20241204 09:08:32 2519721 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.952480  data: 0.001436  max mem: 4109
I20241204 09:09:12 2519721 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.954278  data: 0.000948  max mem: 4109
I20241204 09:09:51 2519721 dinov2 helpers.py:102]   [480/634]  eta: 0:10:23    time: 3.956138  data: 0.000680  max mem: 4109
I20241204 09:10:31 2519721 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.956600  data: 0.000785  max mem: 4109
I20241204 09:11:11 2519721 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.955533  data: 0.000979  max mem: 4109
I20241204 09:11:50 2519721 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.955231  data: 0.000925  max mem: 4109
I20241204 09:12:30 2519721 dinov2 helpers.py:102]   [520/634]  eta: 0:07:40    time: 3.954703  data: 0.001039  max mem: 4109
I20241204 09:13:09 2519721 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.955950  data: 0.000899  max mem: 4109
I20241204 09:13:49 2519721 dinov2 helpers.py:102]   [540/634]  eta: 0:06:19    time: 3.957640  data: 0.001534  max mem: 4109
I20241204 09:14:28 2519721 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.954757  data: 0.001810  max mem: 4109
I20241204 09:15:08 2519721 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.954591  data: 0.001205  max mem: 4109
I20241204 09:15:47 2519721 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.954618  data: 0.001201  max mem: 4109
I20241204 09:16:27 2519721 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.953030  data: 0.001449  max mem: 4109
I20241204 09:17:06 2519721 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.955650  data: 0.001519  max mem: 4109
I20241204 09:17:46 2519721 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.956465  data: 0.001265  max mem: 4109
I20241204 09:18:23 2519721 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.839916  data: 0.001338  max mem: 4109
I20241204 09:18:50 2519721 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.207225  data: 0.001060  max mem: 4109
I20241204 09:19:15 2519721 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.580551  data: 0.000667  max mem: 4109
I20241204 09:19:25 2519721 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.642664  data: 0.000596  max mem: 4109
I20241204 09:19:25 2519721 dinov2 helpers.py:130]  Total time: 0:42:02 (3.978483 s / it)
I20241204 09:19:25 2519721 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:19:25 2519721 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:19:25 2519721 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:19:25 2519721 dinov2 loaders.py:151] sampler: distributed
I20241204 09:19:25 2519721 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:19:25 2519721 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:19:25,735) - Submitted job triggered an exception
E20241204 09:19:25 2519721 submitit submission.py:68] Submitted job triggered an exception
