submitit INFO (2024-12-04 09:27:28,359) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-04 09:27:28,359) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:36 2542581 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:36 2542581 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:36 2542581 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:36 2542581 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:36 2542581 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:06 2542581 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:10 2542581 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:10 2542581 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:16 2542581 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:16 2542581 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:19 2542581 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:19 2542581 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:19 2542581 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:19 2542581 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:19 2542581 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:19 2542581 dinov2 loaders.py:223] # of batches: 634
I20241204 09:28:53 2542581 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:28:53 2542581 dinov2 helpers.py:102]   [  0/634]  eta: 5:56:31    time: 33.741253  data: 12.525873  max mem: 3463
I20241204 09:29:01 2542581 dinov2 helpers.py:102]   [ 10/634]  eta: 0:39:56    time: 3.839883  data: 1.143331  max mem: 4109
I20241204 09:29:16 2542581 dinov2 helpers.py:102]   [ 20/634]  eta: 0:27:52    time: 1.173089  data: 0.003153  max mem: 4109
I20241204 09:29:44 2542581 dinov2 helpers.py:102]   [ 30/634]  eta: 0:27:32    time: 2.127627  data: 0.000965  max mem: 4109
I20241204 09:30:23 2542581 dinov2 helpers.py:102]   [ 40/634]  eta: 0:29:58    time: 3.347573  data: 0.001507  max mem: 4109
I20241204 09:31:03 2542581 dinov2 helpers.py:102]   [ 50/634]  eta: 0:31:14    time: 3.944906  data: 0.001782  max mem: 4109
I20241204 09:31:43 2542581 dinov2 helpers.py:102]   [ 60/634]  eta: 0:31:53    time: 3.958396  data: 0.001022  max mem: 4109
I20241204 09:32:22 2542581 dinov2 helpers.py:102]   [ 70/634]  eta: 0:32:10    time: 3.968413  data: 0.000970  max mem: 4109
I20241204 09:33:02 2542581 dinov2 helpers.py:102]   [ 80/634]  eta: 0:32:14    time: 3.974436  data: 0.001012  max mem: 4109
I20241204 09:33:42 2542581 dinov2 helpers.py:102]   [ 90/634]  eta: 0:32:08    time: 3.975685  data: 0.000984  max mem: 4109
I20241204 09:34:22 2542581 dinov2 helpers.py:102]   [100/634]  eta: 0:31:55    time: 3.978486  data: 0.001578  max mem: 4109
I20241204 09:35:01 2542581 dinov2 helpers.py:102]   [110/634]  eta: 0:31:38    time: 3.979213  data: 0.001513  max mem: 4109
I20241204 09:35:41 2542581 dinov2 helpers.py:102]   [120/634]  eta: 0:31:17    time: 3.975744  data: 0.000850  max mem: 4109
I20241204 09:36:21 2542581 dinov2 helpers.py:102]   [130/634]  eta: 0:30:53    time: 3.974116  data: 0.000784  max mem: 4109
I20241204 09:37:01 2542581 dinov2 helpers.py:102]   [140/634]  eta: 0:30:26    time: 3.974211  data: 0.000723  max mem: 4109
I20241204 09:37:40 2542581 dinov2 helpers.py:102]   [150/634]  eta: 0:29:58    time: 3.978561  data: 0.000796  max mem: 4109
I20241204 09:38:20 2542581 dinov2 helpers.py:102]   [160/634]  eta: 0:29:29    time: 3.978760  data: 0.000754  max mem: 4109
I20241204 09:39:00 2542581 dinov2 helpers.py:102]   [170/634]  eta: 0:28:58    time: 3.974284  data: 0.000630  max mem: 4109
I20241204 09:39:40 2542581 dinov2 helpers.py:102]   [180/634]  eta: 0:28:26    time: 3.977727  data: 0.001537  max mem: 4109
I20241204 09:40:19 2542581 dinov2 helpers.py:102]   [190/634]  eta: 0:27:54    time: 3.979551  data: 0.001507  max mem: 4109
I20241204 09:40:59 2542581 dinov2 helpers.py:102]   [200/634]  eta: 0:27:21    time: 3.978580  data: 0.000638  max mem: 4109
I20241204 09:41:39 2542581 dinov2 helpers.py:102]   [210/634]  eta: 0:26:47    time: 3.979567  data: 0.000620  max mem: 4109
I20241204 09:42:19 2542581 dinov2 helpers.py:102]   [220/634]  eta: 0:26:12    time: 3.979651  data: 0.001181  max mem: 4109
I20241204 09:42:59 2542581 dinov2 helpers.py:102]   [230/634]  eta: 0:25:38    time: 3.980602  data: 0.001444  max mem: 4109
I20241204 09:43:39 2542581 dinov2 helpers.py:102]   [240/634]  eta: 0:25:02    time: 3.982337  data: 0.000889  max mem: 4109
I20241204 09:44:18 2542581 dinov2 helpers.py:102]   [250/634]  eta: 0:24:27    time: 3.984130  data: 0.000646  max mem: 4109
I20241204 09:44:58 2542581 dinov2 helpers.py:102]   [260/634]  eta: 0:23:51    time: 3.985043  data: 0.001022  max mem: 4109
I20241204 09:45:38 2542581 dinov2 helpers.py:102]   [270/634]  eta: 0:23:15    time: 3.984140  data: 0.002229  max mem: 4109
I20241204 09:46:18 2542581 dinov2 helpers.py:102]   [280/634]  eta: 0:22:38    time: 3.979689  data: 0.001936  max mem: 4109
I20241204 09:46:58 2542581 dinov2 helpers.py:102]   [290/634]  eta: 0:22:02    time: 3.981650  data: 0.000828  max mem: 4109
I20241204 09:47:38 2542581 dinov2 helpers.py:102]   [300/634]  eta: 0:21:25    time: 3.986971  data: 0.001614  max mem: 4109
I20241204 09:48:17 2542581 dinov2 helpers.py:102]   [310/634]  eta: 0:20:48    time: 3.988549  data: 0.001715  max mem: 4109
I20241204 09:48:57 2542581 dinov2 helpers.py:102]   [320/634]  eta: 0:20:11    time: 3.986867  data: 0.001107  max mem: 4109
I20241204 09:49:37 2542581 dinov2 helpers.py:102]   [330/634]  eta: 0:19:33    time: 3.985910  data: 0.001477  max mem: 4109
I20241204 09:50:17 2542581 dinov2 helpers.py:102]   [340/634]  eta: 0:18:56    time: 3.984183  data: 0.001152  max mem: 4109
I20241204 09:50:57 2542581 dinov2 helpers.py:102]   [350/634]  eta: 0:18:18    time: 3.979518  data: 0.000771  max mem: 4109
I20241204 09:51:37 2542581 dinov2 helpers.py:102]   [360/634]  eta: 0:17:40    time: 3.976664  data: 0.001315  max mem: 4109
I20241204 09:52:16 2542581 dinov2 helpers.py:102]   [370/634]  eta: 0:17:02    time: 3.975961  data: 0.001259  max mem: 4109
I20241204 09:52:56 2542581 dinov2 helpers.py:102]   [380/634]  eta: 0:16:24    time: 3.979496  data: 0.001055  max mem: 4109
I20241204 09:53:36 2542581 dinov2 helpers.py:102]   [390/634]  eta: 0:15:46    time: 3.981335  data: 0.001252  max mem: 4109
I20241204 09:54:16 2542581 dinov2 helpers.py:102]   [400/634]  eta: 0:15:08    time: 3.978543  data: 0.000879  max mem: 4109
I20241204 09:54:55 2542581 dinov2 helpers.py:102]   [410/634]  eta: 0:14:29    time: 3.978622  data: 0.000779  max mem: 4109
I20241204 09:55:35 2542581 dinov2 helpers.py:102]   [420/634]  eta: 0:13:51    time: 3.983335  data: 0.001222  max mem: 4109
I20241204 09:56:15 2542581 dinov2 helpers.py:102]   [430/634]  eta: 0:13:13    time: 3.984268  data: 0.001922  max mem: 4109
I20241204 09:56:55 2542581 dinov2 helpers.py:102]   [440/634]  eta: 0:12:34    time: 3.986128  data: 0.001680  max mem: 4109
I20241204 09:57:35 2542581 dinov2 helpers.py:102]   [450/634]  eta: 0:11:56    time: 3.988977  data: 0.000961  max mem: 4109
I20241204 09:58:15 2542581 dinov2 helpers.py:102]   [460/634]  eta: 0:11:17    time: 3.989180  data: 0.000927  max mem: 4109
I20241204 09:58:55 2542581 dinov2 helpers.py:102]   [470/634]  eta: 0:10:39    time: 3.988092  data: 0.001247  max mem: 4109
I20241204 09:59:35 2542581 dinov2 helpers.py:102]   [480/634]  eta: 0:10:00    time: 3.986872  data: 0.001164  max mem: 4109
I20241204 10:00:14 2542581 dinov2 helpers.py:102]   [490/634]  eta: 0:09:21    time: 3.982432  data: 0.001055  max mem: 4109
I20241204 10:00:54 2542581 dinov2 helpers.py:102]   [500/634]  eta: 0:08:42    time: 3.981597  data: 0.001191  max mem: 4109
I20241204 10:01:34 2542581 dinov2 helpers.py:102]   [510/634]  eta: 0:08:04    time: 3.982345  data: 0.001360  max mem: 4109
I20241204 10:02:14 2542581 dinov2 helpers.py:102]   [520/634]  eta: 0:07:25    time: 3.981282  data: 0.001352  max mem: 4109
I20241204 10:02:54 2542581 dinov2 helpers.py:102]   [530/634]  eta: 0:06:46    time: 3.978548  data: 0.000883  max mem: 4109
I20241204 10:03:33 2542581 dinov2 helpers.py:102]   [540/634]  eta: 0:06:07    time: 3.977842  data: 0.001008  max mem: 4109
I20241204 10:04:13 2542581 dinov2 helpers.py:102]   [550/634]  eta: 0:05:28    time: 3.979757  data: 0.001322  max mem: 4109
I20241204 10:04:53 2542581 dinov2 helpers.py:102]   [560/634]  eta: 0:04:49    time: 3.976069  data: 0.001282  max mem: 4109
I20241204 10:05:33 2542581 dinov2 helpers.py:102]   [570/634]  eta: 0:04:10    time: 3.975845  data: 0.001033  max mem: 4109
I20241204 10:06:12 2542581 dinov2 helpers.py:102]   [580/634]  eta: 0:03:31    time: 3.976557  data: 0.000836  max mem: 4109
I20241204 10:06:52 2542581 dinov2 helpers.py:102]   [590/634]  eta: 0:02:52    time: 3.975290  data: 0.000603  max mem: 4109
I20241204 10:07:32 2542581 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.974278  data: 0.000812  max mem: 4109
I20241204 10:08:12 2542581 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.973603  data: 0.000841  max mem: 4109
I20241204 10:08:51 2542581 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.973595  data: 0.000758  max mem: 4109
I20241204 10:09:30 2542581 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.930918  data: 0.001031  max mem: 4109
I20241204 10:09:50 2542581 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.305177  data: 0.000896  max mem: 4109
I20241204 10:09:50 2542581 dinov2 helpers.py:130]  Total time: 0:41:30 (3.928845 s / it)
I20241204 10:09:50 2542581 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:09:50 2542581 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:09:51 2542581 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:09:51 2542581 dinov2 loaders.py:151] sampler: distributed
I20241204 10:09:51 2542581 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:09:51 2542581 dinov2 loaders.py:223] # of batches: 78
I20241204 10:09:51 2542581 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:10:00 2542581 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:25    time: 8.786588  data: 4.654113  max mem: 4109
I20241204 10:10:41 2542581 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:02    time: 4.446047  data: 0.427533  max mem: 4109
I20241204 10:11:20 2542581 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:04    time: 3.991773  data: 0.004514  max mem: 4109
I20241204 10:11:53 2542581 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:08    time: 3.634740  data: 0.002741  max mem: 4109
I20241204 10:12:33 2542581 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:29    time: 3.653572  data: 0.005921  max mem: 4109
I20241204 10:13:13 2542581 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:50    time: 3.997782  data: 0.007051  max mem: 4109
I20241204 10:13:53 2542581 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 3.994688  data: 0.005209  max mem: 4109
I20241204 10:14:33 2542581 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 3.993025  data: 0.006663  max mem: 4109
I20241204 10:14:56 2542581 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.757114  data: 0.004128  max mem: 4109
I20241204 10:14:56 2542581 dinov2 helpers.py:130] Test: Total time: 0:05:04 (3.905824 s / it)
I20241204 10:14:56 2542581 dinov2 utils.py:79] Averaged stats: 
I20241204 10:14:58 2542581 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.15
I20241204 10:14:58 2542581 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.03
I20241204 10:14:58 2542581 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:14:58 2542581 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.92
submitit INFO (2024-12-04 10:14:58,475) - Job completed successfully
I20241204 10:14:58 2542581 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:14:58,482) - Exiting after successful completion
I20241204 10:14:58 2542581 submitit submission.py:61] Exiting after successful completion
