submitit INFO (2024-12-03 06:31:07,543) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-03 06:31:07,543) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:16 1958661 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:16 1958661 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:16 1958661 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:16 1958661 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:16 1958661 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:53 1958661 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:59 1958661 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:59 1958661 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:32:17 1958661 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:32:17 1958661 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:20 1958661 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:20 1958661 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:20 1958661 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:20 1958661 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:20 1958661 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:20 1958661 dinov2 loaders.py:223] # of batches: 634
I20241203 06:33:16 1958661 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:33:16 1958661 dinov2 helpers.py:102]   [  0/634]  eta: 9:50:08    time: 55.849747  data: 14.059924  max mem: 3463
I20241203 06:33:48 1958661 dinov2 helpers.py:102]   [ 10/634]  eta: 1:23:09    time: 7.995448  data: 1.279128  max mem: 4109
I20241203 06:34:27 1958661 dinov2 helpers.py:102]   [ 20/634]  eta: 1:01:59    time: 3.568587  data: 0.001135  max mem: 4109
I20241203 06:35:07 1958661 dinov2 helpers.py:102]   [ 30/634]  eta: 0:54:09    time: 3.941436  data: 0.001906  max mem: 4109
I20241203 06:35:47 1958661 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:49    time: 3.954617  data: 0.002320  max mem: 4109
I20241203 06:36:26 1958661 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:55    time: 3.956089  data: 0.001867  max mem: 4109
I20241203 06:37:06 1958661 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:46    time: 3.961684  data: 0.001191  max mem: 4109
I20241203 06:37:45 1958661 dinov2 helpers.py:102]   [ 70/634]  eta: 0:43:03    time: 3.969142  data: 0.000729  max mem: 4109
I20241203 06:38:25 1958661 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:36    time: 3.974013  data: 0.000664  max mem: 4109
I20241203 06:39:05 1958661 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:19    time: 3.974525  data: 0.000697  max mem: 4109
I20241203 06:39:45 1958661 dinov2 helpers.py:102]   [100/634]  eta: 0:39:10    time: 3.977139  data: 0.000972  max mem: 4109
I20241203 06:40:25 1958661 dinov2 helpers.py:102]   [110/634]  eta: 0:38:06    time: 3.976941  data: 0.000974  max mem: 4109
I20241203 06:41:04 1958661 dinov2 helpers.py:102]   [120/634]  eta: 0:37:06    time: 3.974206  data: 0.001004  max mem: 4109
I20241203 06:41:44 1958661 dinov2 helpers.py:102]   [130/634]  eta: 0:36:09    time: 3.974236  data: 0.001548  max mem: 4109
I20241203 06:42:24 1958661 dinov2 helpers.py:102]   [140/634]  eta: 0:35:14    time: 3.974047  data: 0.001478  max mem: 4109
I20241203 06:43:03 1958661 dinov2 helpers.py:102]   [150/634]  eta: 0:34:21    time: 3.973561  data: 0.001449  max mem: 4109
I20241203 06:43:43 1958661 dinov2 helpers.py:102]   [160/634]  eta: 0:33:30    time: 3.973286  data: 0.002253  max mem: 4109
I20241203 06:44:23 1958661 dinov2 helpers.py:102]   [170/634]  eta: 0:32:41    time: 3.973745  data: 0.002618  max mem: 4109
I20241203 06:45:03 1958661 dinov2 helpers.py:102]   [180/634]  eta: 0:31:52    time: 3.973960  data: 0.001824  max mem: 4109
I20241203 06:45:42 1958661 dinov2 helpers.py:102]   [190/634]  eta: 0:31:04    time: 3.973604  data: 0.001119  max mem: 4109
I20241203 06:46:22 1958661 dinov2 helpers.py:102]   [200/634]  eta: 0:30:17    time: 3.973712  data: 0.001074  max mem: 4109
I20241203 06:47:02 1958661 dinov2 helpers.py:102]   [210/634]  eta: 0:29:31    time: 3.973934  data: 0.000841  max mem: 4109
I20241203 06:47:42 1958661 dinov2 helpers.py:102]   [220/634]  eta: 0:28:46    time: 3.973871  data: 0.001094  max mem: 4109
I20241203 06:48:21 1958661 dinov2 helpers.py:102]   [230/634]  eta: 0:28:01    time: 3.973987  data: 0.001144  max mem: 4109
I20241203 06:49:01 1958661 dinov2 helpers.py:102]   [240/634]  eta: 0:27:16    time: 3.974234  data: 0.001098  max mem: 4109
I20241203 06:49:41 1958661 dinov2 helpers.py:102]   [250/634]  eta: 0:26:32    time: 3.973244  data: 0.001582  max mem: 4109
I20241203 06:50:21 1958661 dinov2 helpers.py:102]   [260/634]  eta: 0:25:48    time: 3.972825  data: 0.001681  max mem: 4109
I20241203 06:51:00 1958661 dinov2 helpers.py:102]   [270/634]  eta: 0:25:04    time: 3.973441  data: 0.001677  max mem: 4109
I20241203 06:51:40 1958661 dinov2 helpers.py:102]   [280/634]  eta: 0:24:21    time: 3.970367  data: 0.001294  max mem: 4109
I20241203 06:52:20 1958661 dinov2 helpers.py:102]   [290/634]  eta: 0:23:37    time: 3.965932  data: 0.001215  max mem: 4109
I20241203 06:52:59 1958661 dinov2 helpers.py:102]   [300/634]  eta: 0:22:54    time: 3.959063  data: 0.001112  max mem: 4109
I20241203 06:53:39 1958661 dinov2 helpers.py:102]   [310/634]  eta: 0:22:12    time: 3.957255  data: 0.000954  max mem: 4109
I20241203 06:54:18 1958661 dinov2 helpers.py:102]   [320/634]  eta: 0:21:29    time: 3.961715  data: 0.002304  max mem: 4109
I20241203 06:54:58 1958661 dinov2 helpers.py:102]   [330/634]  eta: 0:20:47    time: 3.962757  data: 0.002346  max mem: 4109
I20241203 06:55:38 1958661 dinov2 helpers.py:102]   [340/634]  eta: 0:20:04    time: 3.965034  data: 0.002922  max mem: 4109
I20241203 06:56:17 1958661 dinov2 helpers.py:102]   [350/634]  eta: 0:19:22    time: 3.969978  data: 0.002676  max mem: 4109
I20241203 06:56:57 1958661 dinov2 helpers.py:102]   [360/634]  eta: 0:18:41    time: 3.973096  data: 0.001043  max mem: 4109
I20241203 06:57:37 1958661 dinov2 helpers.py:102]   [370/634]  eta: 0:17:59    time: 3.973424  data: 0.001484  max mem: 4109
I20241203 06:58:17 1958661 dinov2 helpers.py:102]   [380/634]  eta: 0:17:17    time: 3.973532  data: 0.001212  max mem: 4109
I20241203 06:58:56 1958661 dinov2 helpers.py:102]   [390/634]  eta: 0:16:36    time: 3.973662  data: 0.000630  max mem: 4109
I20241203 06:59:36 1958661 dinov2 helpers.py:102]   [400/634]  eta: 0:15:54    time: 3.974733  data: 0.000695  max mem: 4109
I20241203 07:00:16 1958661 dinov2 helpers.py:102]   [410/634]  eta: 0:15:13    time: 3.974839  data: 0.001820  max mem: 4109
I20241203 07:00:56 1958661 dinov2 helpers.py:102]   [420/634]  eta: 0:14:31    time: 3.974135  data: 0.001792  max mem: 4109
I20241203 07:01:35 1958661 dinov2 helpers.py:102]   [430/634]  eta: 0:13:50    time: 3.974105  data: 0.000884  max mem: 4109
I20241203 07:02:15 1958661 dinov2 helpers.py:102]   [440/634]  eta: 0:13:09    time: 3.976664  data: 0.001327  max mem: 4109
I20241203 07:02:55 1958661 dinov2 helpers.py:102]   [450/634]  eta: 0:12:28    time: 3.975895  data: 0.001244  max mem: 4109
I20241203 07:03:35 1958661 dinov2 helpers.py:102]   [460/634]  eta: 0:11:47    time: 3.973413  data: 0.000770  max mem: 4109
I20241203 07:04:14 1958661 dinov2 helpers.py:102]   [470/634]  eta: 0:11:06    time: 3.974235  data: 0.000725  max mem: 4109
I20241203 07:04:54 1958661 dinov2 helpers.py:102]   [480/634]  eta: 0:10:25    time: 3.975890  data: 0.001148  max mem: 4109
I20241203 07:05:34 1958661 dinov2 helpers.py:102]   [490/634]  eta: 0:09:44    time: 3.975951  data: 0.001172  max mem: 4109
I20241203 07:06:14 1958661 dinov2 helpers.py:102]   [500/634]  eta: 0:09:03    time: 3.974209  data: 0.000769  max mem: 4109
I20241203 07:06:53 1958661 dinov2 helpers.py:102]   [510/634]  eta: 0:08:23    time: 3.974195  data: 0.001497  max mem: 4109
I20241203 07:07:33 1958661 dinov2 helpers.py:102]   [520/634]  eta: 0:07:42    time: 3.974151  data: 0.001451  max mem: 4109
I20241203 07:08:13 1958661 dinov2 helpers.py:102]   [530/634]  eta: 0:07:01    time: 3.974337  data: 0.001459  max mem: 4109
I20241203 07:08:53 1958661 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.974234  data: 0.001430  max mem: 4109
I20241203 07:09:32 1958661 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.973980  data: 0.000912  max mem: 4109
I20241203 07:10:12 1958661 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.976337  data: 0.001025  max mem: 4109
I20241203 07:10:52 1958661 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.976161  data: 0.001319  max mem: 4109
I20241203 07:11:32 1958661 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.975709  data: 0.001451  max mem: 4109
I20241203 07:12:11 1958661 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.976042  data: 0.000811  max mem: 4109
I20241203 07:12:51 1958661 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.944325  data: 0.000664  max mem: 4109
I20241203 07:13:26 1958661 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.706336  data: 0.000654  max mem: 4109
I20241203 07:13:53 1958661 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.118767  data: 0.000480  max mem: 4109
I20241203 07:14:16 1958661 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.500948  data: 0.000692  max mem: 4109
I20241203 07:14:24 1958661 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.435420  data: 0.000653  max mem: 4109
I20241203 07:14:24 1958661 dinov2 helpers.py:130]  Total time: 0:42:03 (3.980890 s / it)
I20241203 07:14:24 1958661 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:14:24 1958661 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:14:24 1958661 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:14:24 1958661 dinov2 loaders.py:151] sampler: distributed
I20241203 07:14:24 1958661 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:14:24 1958661 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:30 1958661 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:43,503) - Submitted job triggered an exception
E20241203 07:14:43 1958661 submitit submission.py:68] Submitted job triggered an exception
