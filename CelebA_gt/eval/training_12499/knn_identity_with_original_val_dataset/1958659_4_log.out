submitit INFO (2024-12-03 06:31:07,568) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-03 06:31:07,568) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:15 1958664 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:15 1958664 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:15 1958664 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:15 1958664 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:15 1958664 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:44 1958664 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:49 1958664 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:50 1958664 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:31:58 1958664 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:31:58 1958664 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:31:59 1958664 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:31:59 1958664 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:31:59 1958664 dinov2 loaders.py:151] sampler: distributed
I20241203 06:31:59 1958664 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:31:59 1958664 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:31:59 1958664 dinov2 loaders.py:223] # of batches: 634
I20241203 06:32:35 1958664 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:32:35 1958664 dinov2 helpers.py:102]   [  0/634]  eta: 6:21:54    time: 36.142754  data: 13.571133  max mem: 3463
I20241203 06:32:47 1958664 dinov2 helpers.py:102]   [ 10/634]  eta: 0:45:36    time: 4.385383  data: 1.237259  max mem: 4109
I20241203 06:33:06 1958664 dinov2 helpers.py:102]   [ 20/634]  eta: 0:32:27    time: 1.523094  data: 0.002360  max mem: 4109
I20241203 06:33:40 1958664 dinov2 helpers.py:102]   [ 30/634]  eta: 0:32:39    time: 2.615300  data: 0.001048  max mem: 4109
I20241203 06:34:19 1958664 dinov2 helpers.py:102]   [ 40/634]  eta: 0:33:44    time: 3.656556  data: 0.000982  max mem: 4109
I20241203 06:34:58 1958664 dinov2 helpers.py:102]   [ 50/634]  eta: 0:34:12    time: 3.933010  data: 0.001160  max mem: 4109
I20241203 06:35:38 1958664 dinov2 helpers.py:102]   [ 60/634]  eta: 0:34:18    time: 3.950446  data: 0.001221  max mem: 4109
I20241203 06:36:18 1958664 dinov2 helpers.py:102]   [ 70/634]  eta: 0:34:11    time: 3.954352  data: 0.000840  max mem: 4109
I20241203 06:36:57 1958664 dinov2 helpers.py:102]   [ 80/634]  eta: 0:33:57    time: 3.958858  data: 0.001304  max mem: 4109
I20241203 06:37:37 1958664 dinov2 helpers.py:102]   [ 90/634]  eta: 0:33:37    time: 3.963612  data: 0.001343  max mem: 4109
I20241203 06:38:17 1958664 dinov2 helpers.py:102]   [100/634]  eta: 0:33:14    time: 3.969304  data: 0.001025  max mem: 4109
I20241203 06:38:56 1958664 dinov2 helpers.py:102]   [110/634]  eta: 0:32:48    time: 3.974280  data: 0.001054  max mem: 4109
I20241203 06:39:36 1958664 dinov2 helpers.py:102]   [120/634]  eta: 0:32:20    time: 3.974368  data: 0.000803  max mem: 4109
I20241203 06:40:16 1958664 dinov2 helpers.py:102]   [130/634]  eta: 0:31:50    time: 3.974365  data: 0.000489  max mem: 4109
I20241203 06:40:56 1958664 dinov2 helpers.py:102]   [140/634]  eta: 0:31:18    time: 3.974288  data: 0.001176  max mem: 4109
I20241203 06:41:35 1958664 dinov2 helpers.py:102]   [150/634]  eta: 0:30:46    time: 3.974365  data: 0.001376  max mem: 4109
I20241203 06:42:15 1958664 dinov2 helpers.py:102]   [160/634]  eta: 0:30:12    time: 3.974193  data: 0.000804  max mem: 4109
I20241203 06:42:55 1958664 dinov2 helpers.py:102]   [170/634]  eta: 0:29:38    time: 3.973652  data: 0.001454  max mem: 4109
I20241203 06:43:34 1958664 dinov2 helpers.py:102]   [180/634]  eta: 0:29:03    time: 3.973223  data: 0.001254  max mem: 4109
I20241203 06:44:14 1958664 dinov2 helpers.py:102]   [190/634]  eta: 0:28:28    time: 3.973423  data: 0.000857  max mem: 4109
I20241203 06:44:54 1958664 dinov2 helpers.py:102]   [200/634]  eta: 0:27:52    time: 3.973796  data: 0.001220  max mem: 4109
I20241203 06:45:34 1958664 dinov2 helpers.py:102]   [210/634]  eta: 0:27:16    time: 3.973864  data: 0.001108  max mem: 4109
I20241203 06:46:13 1958664 dinov2 helpers.py:102]   [220/634]  eta: 0:26:40    time: 3.973721  data: 0.000921  max mem: 4109
I20241203 06:46:53 1958664 dinov2 helpers.py:102]   [230/634]  eta: 0:26:03    time: 3.973867  data: 0.001488  max mem: 4109
I20241203 06:47:33 1958664 dinov2 helpers.py:102]   [240/634]  eta: 0:25:26    time: 3.973913  data: 0.001634  max mem: 4109
I20241203 06:48:13 1958664 dinov2 helpers.py:102]   [250/634]  eta: 0:24:49    time: 3.973989  data: 0.001180  max mem: 4109
I20241203 06:48:52 1958664 dinov2 helpers.py:102]   [260/634]  eta: 0:24:11    time: 3.974260  data: 0.001167  max mem: 4109
I20241203 06:49:32 1958664 dinov2 helpers.py:102]   [270/634]  eta: 0:23:34    time: 3.974190  data: 0.001317  max mem: 4109
I20241203 06:50:12 1958664 dinov2 helpers.py:102]   [280/634]  eta: 0:22:56    time: 3.975725  data: 0.001298  max mem: 4109
I20241203 06:50:52 1958664 dinov2 helpers.py:102]   [290/634]  eta: 0:22:18    time: 3.975258  data: 0.000925  max mem: 4109
I20241203 06:51:31 1958664 dinov2 helpers.py:102]   [300/634]  eta: 0:21:40    time: 3.972141  data: 0.000938  max mem: 4109
I20241203 06:52:11 1958664 dinov2 helpers.py:102]   [310/634]  eta: 0:21:02    time: 3.967923  data: 0.000897  max mem: 4109
I20241203 06:52:51 1958664 dinov2 helpers.py:102]   [320/634]  eta: 0:20:24    time: 3.961819  data: 0.001160  max mem: 4109
I20241203 06:53:30 1958664 dinov2 helpers.py:102]   [330/634]  eta: 0:19:45    time: 3.959998  data: 0.001359  max mem: 4109
I20241203 06:54:10 1958664 dinov2 helpers.py:102]   [340/634]  eta: 0:19:07    time: 3.961768  data: 0.000980  max mem: 4109
I20241203 06:54:49 1958664 dinov2 helpers.py:102]   [350/634]  eta: 0:18:28    time: 3.963474  data: 0.001242  max mem: 4109
I20241203 06:55:29 1958664 dinov2 helpers.py:102]   [360/634]  eta: 0:17:50    time: 3.962176  data: 0.001379  max mem: 4109
I20241203 06:56:09 1958664 dinov2 helpers.py:102]   [370/634]  eta: 0:17:11    time: 3.965376  data: 0.001249  max mem: 4109
I20241203 06:56:49 1958664 dinov2 helpers.py:102]   [380/634]  eta: 0:16:32    time: 3.972064  data: 0.001339  max mem: 4109
I20241203 06:57:28 1958664 dinov2 helpers.py:102]   [390/634]  eta: 0:15:54    time: 3.973447  data: 0.001645  max mem: 4109
I20241203 06:58:08 1958664 dinov2 helpers.py:102]   [400/634]  eta: 0:15:15    time: 3.973626  data: 0.001442  max mem: 4109
I20241203 06:58:48 1958664 dinov2 helpers.py:102]   [410/634]  eta: 0:14:36    time: 3.973746  data: 0.000948  max mem: 4109
I20241203 06:59:27 1958664 dinov2 helpers.py:102]   [420/634]  eta: 0:13:57    time: 3.973715  data: 0.000808  max mem: 4109
I20241203 07:00:07 1958664 dinov2 helpers.py:102]   [430/634]  eta: 0:13:18    time: 3.973811  data: 0.000800  max mem: 4109
I20241203 07:00:47 1958664 dinov2 helpers.py:102]   [440/634]  eta: 0:12:40    time: 3.975955  data: 0.001829  max mem: 4109
I20241203 07:01:27 1958664 dinov2 helpers.py:102]   [450/634]  eta: 0:12:01    time: 3.975895  data: 0.001687  max mem: 4109
I20241203 07:02:06 1958664 dinov2 helpers.py:102]   [460/634]  eta: 0:11:22    time: 3.974023  data: 0.000646  max mem: 4109
I20241203 07:02:46 1958664 dinov2 helpers.py:102]   [470/634]  eta: 0:10:43    time: 3.974053  data: 0.000594  max mem: 4109
I20241203 07:03:26 1958664 dinov2 helpers.py:102]   [480/634]  eta: 0:10:04    time: 3.974157  data: 0.001152  max mem: 4109
I20241203 07:04:06 1958664 dinov2 helpers.py:102]   [490/634]  eta: 0:09:24    time: 3.975992  data: 0.001078  max mem: 4109
I20241203 07:04:45 1958664 dinov2 helpers.py:102]   [500/634]  eta: 0:08:45    time: 3.975994  data: 0.000635  max mem: 4109
I20241203 07:05:25 1958664 dinov2 helpers.py:102]   [510/634]  eta: 0:08:06    time: 3.974164  data: 0.000831  max mem: 4109
I20241203 07:06:05 1958664 dinov2 helpers.py:102]   [520/634]  eta: 0:07:27    time: 3.974106  data: 0.000819  max mem: 4109
I20241203 07:06:45 1958664 dinov2 helpers.py:102]   [530/634]  eta: 0:06:48    time: 3.974114  data: 0.001053  max mem: 4109
I20241203 07:07:24 1958664 dinov2 helpers.py:102]   [540/634]  eta: 0:06:09    time: 3.974241  data: 0.001014  max mem: 4109
I20241203 07:08:04 1958664 dinov2 helpers.py:102]   [550/634]  eta: 0:05:30    time: 3.974183  data: 0.000841  max mem: 4109
I20241203 07:08:44 1958664 dinov2 helpers.py:102]   [560/634]  eta: 0:04:50    time: 3.974108  data: 0.000848  max mem: 4109
I20241203 07:09:24 1958664 dinov2 helpers.py:102]   [570/634]  eta: 0:04:11    time: 3.976056  data: 0.001219  max mem: 4109
I20241203 07:10:03 1958664 dinov2 helpers.py:102]   [580/634]  eta: 0:03:32    time: 3.976067  data: 0.001228  max mem: 4109
I20241203 07:10:43 1958664 dinov2 helpers.py:102]   [590/634]  eta: 0:02:53    time: 3.974320  data: 0.000758  max mem: 4109
I20241203 07:11:23 1958664 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.976028  data: 0.001516  max mem: 4109
I20241203 07:12:03 1958664 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.975854  data: 0.001566  max mem: 4109
I20241203 07:12:43 1958664 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.975843  data: 0.000794  max mem: 4109
I20241203 07:13:18 1958664 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.758713  data: 0.000873  max mem: 4109
I20241203 07:13:34 1958664 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 3.983829  data: 0.000796  max mem: 4109
I20241203 07:13:35 1958664 dinov2 helpers.py:130]  Total time: 0:41:35 (3.935895 s / it)
I20241203 07:13:35 1958664 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:13:35 1958664 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:13:35 1958664 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:13:35 1958664 dinov2 loaders.py:151] sampler: distributed
I20241203 07:13:35 1958664 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:13:35 1958664 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:29 1958664 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:42,036) - Submitted job triggered an exception
E20241203 07:14:42 1958664 submitit submission.py:68] Submitted job triggered an exception
