submitit INFO (2024-12-03 06:31:07,491) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-03 06:31:07,491) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:14 1958660 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:14 1958660 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:14 1958660 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:14 1958660 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:14 1958660 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:51 1958660 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:55 1958660 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:56 1958660 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:32:12 1958660 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:32:12 1958660 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:14 1958660 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:14 1958660 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:14 1958660 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:14 1958660 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:14 1958660 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:14 1958660 dinov2 loaders.py:223] # of batches: 634
I20241203 06:33:09 1958660 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:33:09 1958660 dinov2 helpers.py:102]   [  0/634]  eta: 9:32:09    time: 54.147045  data: 10.238249  max mem: 3463
I20241203 06:33:38 1958660 dinov2 helpers.py:102]   [ 10/634]  eta: 1:18:51    time: 7.583232  data: 0.932150  max mem: 4109
I20241203 06:34:17 1958660 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:44    time: 3.422048  data: 0.001503  max mem: 4109
I20241203 06:34:56 1958660 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:37    time: 3.931205  data: 0.001468  max mem: 4109
I20241203 06:35:36 1958660 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:40    time: 3.950532  data: 0.002532  max mem: 4109
I20241203 06:36:16 1958660 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:01    time: 3.955339  data: 0.002397  max mem: 4109
I20241203 06:36:55 1958660 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:02    time: 3.959719  data: 0.001160  max mem: 4109
I20241203 06:37:35 1958660 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:25    time: 3.966183  data: 0.001082  max mem: 4109
I20241203 06:38:15 1958660 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:03    time: 3.972799  data: 0.001180  max mem: 4109
I20241203 06:38:54 1958660 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:51    time: 3.976051  data: 0.001373  max mem: 4109
I20241203 06:39:34 1958660 dinov2 helpers.py:102]   [100/634]  eta: 0:38:44    time: 3.974175  data: 0.001268  max mem: 4109
I20241203 06:40:14 1958660 dinov2 helpers.py:102]   [110/634]  eta: 0:37:43    time: 3.974248  data: 0.001052  max mem: 4109
I20241203 06:40:54 1958660 dinov2 helpers.py:102]   [120/634]  eta: 0:36:45    time: 3.974355  data: 0.002011  max mem: 4109
I20241203 06:41:33 1958660 dinov2 helpers.py:102]   [130/634]  eta: 0:35:50    time: 3.974328  data: 0.002055  max mem: 4109
I20241203 06:42:13 1958660 dinov2 helpers.py:102]   [140/634]  eta: 0:34:57    time: 3.974108  data: 0.001344  max mem: 4109
I20241203 06:42:53 1958660 dinov2 helpers.py:102]   [150/634]  eta: 0:34:06    time: 3.973728  data: 0.001298  max mem: 4109
I20241203 06:43:33 1958660 dinov2 helpers.py:102]   [160/634]  eta: 0:33:16    time: 3.973329  data: 0.001107  max mem: 4109
I20241203 06:44:12 1958660 dinov2 helpers.py:102]   [170/634]  eta: 0:32:27    time: 3.973390  data: 0.001320  max mem: 4109
I20241203 06:44:52 1958660 dinov2 helpers.py:102]   [180/634]  eta: 0:31:40    time: 3.973798  data: 0.001127  max mem: 4109
I20241203 06:45:32 1958660 dinov2 helpers.py:102]   [190/634]  eta: 0:30:53    time: 3.973809  data: 0.001704  max mem: 4109
I20241203 06:46:12 1958660 dinov2 helpers.py:102]   [200/634]  eta: 0:30:07    time: 3.973732  data: 0.001794  max mem: 4109
I20241203 06:46:51 1958660 dinov2 helpers.py:102]   [210/634]  eta: 0:29:22    time: 3.974309  data: 0.001731  max mem: 4109
I20241203 06:47:31 1958660 dinov2 helpers.py:102]   [220/634]  eta: 0:28:37    time: 3.973863  data: 0.001660  max mem: 4109
I20241203 06:48:11 1958660 dinov2 helpers.py:102]   [230/634]  eta: 0:27:52    time: 3.973621  data: 0.001943  max mem: 4109
I20241203 06:48:51 1958660 dinov2 helpers.py:102]   [240/634]  eta: 0:27:08    time: 3.974338  data: 0.001908  max mem: 4109
I20241203 06:49:30 1958660 dinov2 helpers.py:102]   [250/634]  eta: 0:26:24    time: 3.974160  data: 0.001003  max mem: 4109
I20241203 06:50:10 1958660 dinov2 helpers.py:102]   [260/634]  eta: 0:25:41    time: 3.973838  data: 0.001371  max mem: 4109
I20241203 06:50:50 1958660 dinov2 helpers.py:102]   [270/634]  eta: 0:24:58    time: 3.973649  data: 0.001584  max mem: 4109
I20241203 06:51:29 1958660 dinov2 helpers.py:102]   [280/634]  eta: 0:24:15    time: 3.972314  data: 0.001138  max mem: 4109
I20241203 06:52:09 1958660 dinov2 helpers.py:102]   [290/634]  eta: 0:23:32    time: 3.965190  data: 0.000623  max mem: 4109
I20241203 06:52:49 1958660 dinov2 helpers.py:102]   [300/634]  eta: 0:22:49    time: 3.960834  data: 0.000711  max mem: 4109
I20241203 06:53:28 1958660 dinov2 helpers.py:102]   [310/634]  eta: 0:22:07    time: 3.964451  data: 0.000840  max mem: 4109
I20241203 06:54:08 1958660 dinov2 helpers.py:102]   [320/634]  eta: 0:21:24    time: 3.963576  data: 0.001058  max mem: 4109
I20241203 06:54:47 1958660 dinov2 helpers.py:102]   [330/634]  eta: 0:20:42    time: 3.958845  data: 0.001485  max mem: 4109
I20241203 06:55:27 1958660 dinov2 helpers.py:102]   [340/634]  eta: 0:20:00    time: 3.960315  data: 0.002050  max mem: 4109
I20241203 06:56:07 1958660 dinov2 helpers.py:102]   [350/634]  eta: 0:19:18    time: 3.965310  data: 0.002041  max mem: 4109
I20241203 06:56:47 1958660 dinov2 helpers.py:102]   [360/634]  eta: 0:18:37    time: 3.968387  data: 0.001421  max mem: 4109
I20241203 06:57:26 1958660 dinov2 helpers.py:102]   [370/634]  eta: 0:17:55    time: 3.971648  data: 0.000886  max mem: 4109
I20241203 06:58:06 1958660 dinov2 helpers.py:102]   [380/634]  eta: 0:17:14    time: 3.973600  data: 0.000744  max mem: 4109
I20241203 06:58:46 1958660 dinov2 helpers.py:102]   [390/634]  eta: 0:16:33    time: 3.973661  data: 0.000837  max mem: 4109
I20241203 06:59:25 1958660 dinov2 helpers.py:102]   [400/634]  eta: 0:15:51    time: 3.973785  data: 0.002201  max mem: 4109
I20241203 07:00:05 1958660 dinov2 helpers.py:102]   [410/634]  eta: 0:15:10    time: 3.974802  data: 0.002159  max mem: 4109
I20241203 07:00:45 1958660 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.974981  data: 0.000810  max mem: 4109
I20241203 07:01:25 1958660 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.974122  data: 0.001090  max mem: 4109
I20241203 07:02:04 1958660 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.974098  data: 0.000949  max mem: 4109
I20241203 07:02:44 1958660 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.974115  data: 0.000637  max mem: 4109
I20241203 07:03:24 1958660 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.975948  data: 0.000801  max mem: 4109
I20241203 07:04:04 1958660 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.975953  data: 0.000965  max mem: 4109
I20241203 07:04:43 1958660 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.974180  data: 0.000922  max mem: 4109
I20241203 07:05:23 1958660 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.974180  data: 0.000952  max mem: 4109
I20241203 07:06:03 1958660 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.974169  data: 0.000825  max mem: 4109
I20241203 07:06:43 1958660 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.974162  data: 0.000592  max mem: 4109
I20241203 07:07:22 1958660 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.975951  data: 0.000983  max mem: 4109
I20241203 07:08:02 1958660 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.975872  data: 0.001090  max mem: 4109
I20241203 07:08:42 1958660 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.974070  data: 0.000819  max mem: 4109
I20241203 07:09:22 1958660 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.974236  data: 0.001108  max mem: 4109
I20241203 07:10:01 1958660 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.974295  data: 0.002069  max mem: 4109
I20241203 07:10:41 1958660 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.976079  data: 0.001990  max mem: 4109
I20241203 07:11:21 1958660 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.976110  data: 0.000996  max mem: 4109
I20241203 07:12:01 1958660 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.974094  data: 0.001887  max mem: 4109
I20241203 07:12:40 1958660 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.975719  data: 0.002305  max mem: 4109
I20241203 07:13:16 1958660 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.770309  data: 0.001113  max mem: 4109
I20241203 07:13:46 1958660 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.280062  data: 0.000526  max mem: 4109
I20241203 07:14:10 1958660 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.695554  data: 0.002026  max mem: 4109
I20241203 07:14:20 1958660 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.674946  data: 0.001986  max mem: 4109
I20241203 07:14:20 1958660 dinov2 helpers.py:130]  Total time: 0:42:05 (3.984085 s / it)
I20241203 07:14:20 1958660 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:14:20 1958660 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:14:21 1958660 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:14:21 1958660 dinov2 loaders.py:151] sampler: distributed
I20241203 07:14:21 1958660 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:14:21 1958660 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:30 1958660 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:44,575) - Submitted job triggered an exception
E20241203 07:14:44 1958660 submitit submission.py:68] Submitted job triggered an exception
