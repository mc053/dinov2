submitit INFO (2024-12-03 06:31:07,611) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-03 06:31:07,611) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:15 1958666 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:15 1958666 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:15 1958666 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:15 1958666 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:15 1958666 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:50 1958666 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:52 1958666 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:53 1958666 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:32:03 1958666 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:32:03 1958666 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:05 1958666 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:05 1958666 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:05 1958666 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:05 1958666 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:05 1958666 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:05 1958666 dinov2 loaders.py:223] # of batches: 634
I20241203 06:32:52 1958666 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:32:52 1958666 dinov2 helpers.py:102]   [  0/634]  eta: 8:16:07    time: 46.951450  data: 8.521588  max mem: 3463
I20241203 06:33:09 1958666 dinov2 helpers.py:102]   [ 10/634]  eta: 0:59:52    time: 5.756894  data: 0.777274  max mem: 4109
I20241203 06:33:45 1958666 dinov2 helpers.py:102]   [ 20/634]  eta: 0:48:34    time: 2.636004  data: 0.003217  max mem: 4109
I20241203 06:34:24 1958666 dinov2 helpers.py:102]   [ 30/634]  eta: 0:45:05    time: 3.777909  data: 0.002744  max mem: 4109
I20241203 06:35:04 1958666 dinov2 helpers.py:102]   [ 40/634]  eta: 0:43:03    time: 3.934109  data: 0.001492  max mem: 4109
I20241203 06:35:43 1958666 dinov2 helpers.py:102]   [ 50/634]  eta: 0:41:35    time: 3.950324  data: 0.001183  max mem: 4109
I20241203 06:36:23 1958666 dinov2 helpers.py:102]   [ 60/634]  eta: 0:40:22    time: 3.954363  data: 0.001533  max mem: 4109
I20241203 06:37:03 1958666 dinov2 helpers.py:102]   [ 70/634]  eta: 0:39:19    time: 3.958931  data: 0.001961  max mem: 4109
I20241203 06:37:42 1958666 dinov2 helpers.py:102]   [ 80/634]  eta: 0:38:23    time: 3.969031  data: 0.001824  max mem: 4109
I20241203 06:38:22 1958666 dinov2 helpers.py:102]   [ 90/634]  eta: 0:37:31    time: 3.974812  data: 0.001213  max mem: 4109
I20241203 06:39:02 1958666 dinov2 helpers.py:102]   [100/634]  eta: 0:36:41    time: 3.976171  data: 0.001877  max mem: 4109
I20241203 06:39:42 1958666 dinov2 helpers.py:102]   [110/634]  eta: 0:35:53    time: 3.976169  data: 0.001904  max mem: 4109
I20241203 06:40:21 1958666 dinov2 helpers.py:102]   [120/634]  eta: 0:35:06    time: 3.974384  data: 0.000723  max mem: 4109
I20241203 06:41:01 1958666 dinov2 helpers.py:102]   [130/634]  eta: 0:34:20    time: 3.974455  data: 0.000540  max mem: 4109
I20241203 06:41:41 1958666 dinov2 helpers.py:102]   [140/634]  eta: 0:33:35    time: 3.974286  data: 0.000613  max mem: 4109
I20241203 06:42:21 1958666 dinov2 helpers.py:102]   [150/634]  eta: 0:32:51    time: 3.974030  data: 0.000911  max mem: 4109
I20241203 06:43:00 1958666 dinov2 helpers.py:102]   [160/634]  eta: 0:32:07    time: 3.973639  data: 0.000937  max mem: 4109
I20241203 06:43:40 1958666 dinov2 helpers.py:102]   [170/634]  eta: 0:31:24    time: 3.973196  data: 0.001048  max mem: 4109
I20241203 06:44:20 1958666 dinov2 helpers.py:102]   [180/634]  eta: 0:30:41    time: 3.973548  data: 0.001497  max mem: 4109
I20241203 06:44:59 1958666 dinov2 helpers.py:102]   [190/634]  eta: 0:29:59    time: 3.974142  data: 0.002345  max mem: 4109
I20241203 06:45:39 1958666 dinov2 helpers.py:102]   [200/634]  eta: 0:29:17    time: 3.973748  data: 0.002007  max mem: 4109
I20241203 06:46:19 1958666 dinov2 helpers.py:102]   [210/634]  eta: 0:28:35    time: 3.973736  data: 0.000826  max mem: 4109
I20241203 06:46:59 1958666 dinov2 helpers.py:102]   [220/634]  eta: 0:27:53    time: 3.973895  data: 0.000663  max mem: 4109
I20241203 06:47:38 1958666 dinov2 helpers.py:102]   [230/634]  eta: 0:27:11    time: 3.973934  data: 0.000714  max mem: 4109
I20241203 06:48:18 1958666 dinov2 helpers.py:102]   [240/634]  eta: 0:26:30    time: 3.973988  data: 0.000937  max mem: 4109
I20241203 06:48:58 1958666 dinov2 helpers.py:102]   [250/634]  eta: 0:25:48    time: 3.973988  data: 0.000830  max mem: 4109
I20241203 06:49:38 1958666 dinov2 helpers.py:102]   [260/634]  eta: 0:25:07    time: 3.974158  data: 0.000635  max mem: 4109
I20241203 06:50:17 1958666 dinov2 helpers.py:102]   [270/634]  eta: 0:24:26    time: 3.973729  data: 0.000732  max mem: 4109
I20241203 06:50:57 1958666 dinov2 helpers.py:102]   [280/634]  eta: 0:23:45    time: 3.973468  data: 0.001026  max mem: 4109
I20241203 06:51:37 1958666 dinov2 helpers.py:102]   [290/634]  eta: 0:23:04    time: 3.970410  data: 0.001497  max mem: 4109
I20241203 06:52:16 1958666 dinov2 helpers.py:102]   [300/634]  eta: 0:22:23    time: 3.962391  data: 0.001205  max mem: 4109
I20241203 06:52:56 1958666 dinov2 helpers.py:102]   [310/634]  eta: 0:21:42    time: 3.957265  data: 0.001141  max mem: 4109
I20241203 06:53:36 1958666 dinov2 helpers.py:102]   [320/634]  eta: 0:21:01    time: 3.958241  data: 0.001552  max mem: 4109
I20241203 06:54:15 1958666 dinov2 helpers.py:102]   [330/634]  eta: 0:20:21    time: 3.956328  data: 0.001119  max mem: 4109
I20241203 06:54:55 1958666 dinov2 helpers.py:102]   [340/634]  eta: 0:19:40    time: 3.957314  data: 0.001549  max mem: 4109
I20241203 06:55:34 1958666 dinov2 helpers.py:102]   [350/634]  eta: 0:18:59    time: 3.963149  data: 0.001546  max mem: 4109
I20241203 06:56:14 1958666 dinov2 helpers.py:102]   [360/634]  eta: 0:18:19    time: 3.965303  data: 0.001304  max mem: 4109
I20241203 06:56:54 1958666 dinov2 helpers.py:102]   [370/634]  eta: 0:17:39    time: 3.969423  data: 0.001344  max mem: 4109
I20241203 06:57:33 1958666 dinov2 helpers.py:102]   [380/634]  eta: 0:16:58    time: 3.973493  data: 0.002224  max mem: 4109
I20241203 06:58:13 1958666 dinov2 helpers.py:102]   [390/634]  eta: 0:16:18    time: 3.973677  data: 0.002397  max mem: 4109
I20241203 06:58:53 1958666 dinov2 helpers.py:102]   [400/634]  eta: 0:15:38    time: 3.973576  data: 0.001178  max mem: 4109
I20241203 06:59:33 1958666 dinov2 helpers.py:102]   [410/634]  eta: 0:14:57    time: 3.973762  data: 0.001691  max mem: 4109
I20241203 07:00:12 1958666 dinov2 helpers.py:102]   [420/634]  eta: 0:14:17    time: 3.974174  data: 0.001578  max mem: 4109
I20241203 07:00:52 1958666 dinov2 helpers.py:102]   [430/634]  eta: 0:13:37    time: 3.975992  data: 0.000831  max mem: 4109
I20241203 07:01:32 1958666 dinov2 helpers.py:102]   [440/634]  eta: 0:12:57    time: 3.975667  data: 0.000788  max mem: 4109
I20241203 07:02:12 1958666 dinov2 helpers.py:102]   [450/634]  eta: 0:12:16    time: 3.973938  data: 0.000910  max mem: 4109
I20241203 07:02:51 1958666 dinov2 helpers.py:102]   [460/634]  eta: 0:11:36    time: 3.974199  data: 0.001280  max mem: 4109
I20241203 07:03:31 1958666 dinov2 helpers.py:102]   [470/634]  eta: 0:10:56    time: 3.976074  data: 0.001243  max mem: 4109
I20241203 07:04:11 1958666 dinov2 helpers.py:102]   [480/634]  eta: 0:10:16    time: 3.975983  data: 0.000972  max mem: 4109
I20241203 07:04:51 1958666 dinov2 helpers.py:102]   [490/634]  eta: 0:09:36    time: 3.974143  data: 0.000860  max mem: 4109
I20241203 07:05:30 1958666 dinov2 helpers.py:102]   [500/634]  eta: 0:08:56    time: 3.974188  data: 0.000873  max mem: 4109
I20241203 07:06:10 1958666 dinov2 helpers.py:102]   [510/634]  eta: 0:08:16    time: 3.974115  data: 0.001084  max mem: 4109
I20241203 07:06:50 1958666 dinov2 helpers.py:102]   [520/634]  eta: 0:07:36    time: 3.974093  data: 0.001094  max mem: 4109
I20241203 07:07:30 1958666 dinov2 helpers.py:102]   [530/634]  eta: 0:06:56    time: 3.974182  data: 0.001226  max mem: 4109
I20241203 07:08:09 1958666 dinov2 helpers.py:102]   [540/634]  eta: 0:06:15    time: 3.976003  data: 0.001353  max mem: 4109
I20241203 07:08:49 1958666 dinov2 helpers.py:102]   [550/634]  eta: 0:05:35    time: 3.975887  data: 0.001159  max mem: 4109
I20241203 07:09:29 1958666 dinov2 helpers.py:102]   [560/634]  eta: 0:04:55    time: 3.974157  data: 0.001024  max mem: 4109
I20241203 07:10:09 1958666 dinov2 helpers.py:102]   [570/634]  eta: 0:04:15    time: 3.974414  data: 0.001140  max mem: 4109
I20241203 07:10:48 1958666 dinov2 helpers.py:102]   [580/634]  eta: 0:03:35    time: 3.974337  data: 0.001310  max mem: 4109
I20241203 07:11:28 1958666 dinov2 helpers.py:102]   [590/634]  eta: 0:02:55    time: 3.976271  data: 0.001679  max mem: 4109
I20241203 07:12:08 1958666 dinov2 helpers.py:102]   [600/634]  eta: 0:02:15    time: 3.975891  data: 0.001753  max mem: 4109
I20241203 07:12:47 1958666 dinov2 helpers.py:102]   [610/634]  eta: 0:01:35    time: 3.964731  data: 0.001321  max mem: 4109
I20241203 07:13:22 1958666 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.726182  data: 0.001269  max mem: 4109
I20241203 07:13:51 1958666 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.160536  data: 0.001021  max mem: 4109
I20241203 07:14:03 1958666 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 3.261958  data: 0.000634  max mem: 4109
I20241203 07:14:03 1958666 dinov2 helpers.py:130]  Total time: 0:41:57 (3.971576 s / it)
I20241203 07:14:03 1958666 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:14:03 1958666 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:14:04 1958666 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:14:04 1958666 dinov2 loaders.py:151] sampler: distributed
I20241203 07:14:04 1958666 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:14:04 1958666 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:30 1958666 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:45,153) - Submitted job triggered an exception
E20241203 07:14:45 1958666 submitit submission.py:68] Submitted job triggered an exception
