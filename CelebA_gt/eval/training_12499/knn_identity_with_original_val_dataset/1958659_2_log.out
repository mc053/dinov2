submitit INFO (2024-12-03 06:31:07,551) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=2(8), node=0(1), global_rank=2(8))
submitit INFO (2024-12-03 06:31:07,551) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:16 1958662 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:16 1958662 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:16 1958662 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:16 1958662 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:16 1958662 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:44 1958662 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:49 1958662 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:50 1958662 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:31:58 1958662 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:31:58 1958662 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:00 1958662 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:00 1958662 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:00 1958662 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:00 1958662 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:00 1958662 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:00 1958662 dinov2 loaders.py:223] # of batches: 634
I20241203 06:32:34 1958662 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:32:34 1958662 dinov2 helpers.py:102]   [  0/634]  eta: 5:57:55    time: 33.872391  data: 10.146656  max mem: 3463
I20241203 06:32:45 1958662 dinov2 helpers.py:102]   [ 10/634]  eta: 0:42:42    time: 4.107335  data: 0.924480  max mem: 4109
I20241203 06:33:03 1958662 dinov2 helpers.py:102]   [ 20/634]  eta: 0:30:34    time: 1.442869  data: 0.001853  max mem: 4109
I20241203 06:33:33 1958662 dinov2 helpers.py:102]   [ 30/634]  eta: 0:30:19    time: 2.410455  data: 0.001165  max mem: 4109
I20241203 06:34:12 1958662 dinov2 helpers.py:102]   [ 40/634]  eta: 0:31:59    time: 3.487801  data: 0.000920  max mem: 4109
I20241203 06:34:52 1958662 dinov2 helpers.py:102]   [ 50/634]  eta: 0:32:48    time: 3.926471  data: 0.001043  max mem: 4109
I20241203 06:35:31 1958662 dinov2 helpers.py:102]   [ 60/634]  eta: 0:33:10    time: 3.950913  data: 0.001470  max mem: 4109
I20241203 06:36:11 1958662 dinov2 helpers.py:102]   [ 70/634]  eta: 0:33:14    time: 3.955387  data: 0.002876  max mem: 4109
I20241203 06:36:50 1958662 dinov2 helpers.py:102]   [ 80/634]  eta: 0:33:07    time: 3.957534  data: 0.002482  max mem: 4109
I20241203 06:37:30 1958662 dinov2 helpers.py:102]   [ 90/634]  eta: 0:32:54    time: 3.965940  data: 0.001074  max mem: 4109
I20241203 06:38:10 1958662 dinov2 helpers.py:102]   [100/634]  eta: 0:32:36    time: 3.972026  data: 0.001844  max mem: 4109
I20241203 06:38:50 1958662 dinov2 helpers.py:102]   [110/634]  eta: 0:32:14    time: 3.976229  data: 0.001625  max mem: 4109
I20241203 06:39:29 1958662 dinov2 helpers.py:102]   [120/634]  eta: 0:31:49    time: 3.975880  data: 0.000678  max mem: 4109
I20241203 06:40:09 1958662 dinov2 helpers.py:102]   [130/634]  eta: 0:31:22    time: 3.974540  data: 0.000827  max mem: 4109
I20241203 06:40:49 1958662 dinov2 helpers.py:102]   [140/634]  eta: 0:30:53    time: 3.974389  data: 0.001027  max mem: 4109
I20241203 06:41:29 1958662 dinov2 helpers.py:102]   [150/634]  eta: 0:30:23    time: 3.974495  data: 0.000834  max mem: 4109
I20241203 06:42:08 1958662 dinov2 helpers.py:102]   [160/634]  eta: 0:29:51    time: 3.974113  data: 0.000708  max mem: 4109
I20241203 06:42:48 1958662 dinov2 helpers.py:102]   [170/634]  eta: 0:29:19    time: 3.972504  data: 0.001144  max mem: 4109
I20241203 06:43:28 1958662 dinov2 helpers.py:102]   [180/634]  eta: 0:28:45    time: 3.972486  data: 0.001214  max mem: 4109
I20241203 06:44:08 1958662 dinov2 helpers.py:102]   [190/634]  eta: 0:28:11    time: 3.974237  data: 0.000948  max mem: 4109
I20241203 06:44:47 1958662 dinov2 helpers.py:102]   [200/634]  eta: 0:27:37    time: 3.974607  data: 0.000882  max mem: 4109
I20241203 06:45:27 1958662 dinov2 helpers.py:102]   [210/634]  eta: 0:27:02    time: 3.973899  data: 0.001008  max mem: 4109
I20241203 06:46:07 1958662 dinov2 helpers.py:102]   [220/634]  eta: 0:26:26    time: 3.974115  data: 0.000890  max mem: 4109
I20241203 06:46:47 1958662 dinov2 helpers.py:102]   [230/634]  eta: 0:25:50    time: 3.974131  data: 0.000580  max mem: 4109
I20241203 06:47:26 1958662 dinov2 helpers.py:102]   [240/634]  eta: 0:25:14    time: 3.973542  data: 0.001482  max mem: 4109
I20241203 06:48:06 1958662 dinov2 helpers.py:102]   [250/634]  eta: 0:24:38    time: 3.973685  data: 0.001465  max mem: 4109
I20241203 06:48:46 1958662 dinov2 helpers.py:102]   [260/634]  eta: 0:24:01    time: 3.974326  data: 0.000568  max mem: 4109
I20241203 06:49:26 1958662 dinov2 helpers.py:102]   [270/634]  eta: 0:23:24    time: 3.974239  data: 0.000554  max mem: 4109
I20241203 06:50:05 1958662 dinov2 helpers.py:102]   [280/634]  eta: 0:22:47    time: 3.973846  data: 0.000502  max mem: 4109
I20241203 06:50:45 1958662 dinov2 helpers.py:102]   [290/634]  eta: 0:22:10    time: 3.973875  data: 0.000872  max mem: 4109
I20241203 06:51:25 1958662 dinov2 helpers.py:102]   [300/634]  eta: 0:21:32    time: 3.971412  data: 0.000967  max mem: 4109
I20241203 06:52:04 1958662 dinov2 helpers.py:102]   [310/634]  eta: 0:20:54    time: 3.969722  data: 0.000747  max mem: 4109
I20241203 06:52:44 1958662 dinov2 helpers.py:102]   [320/634]  eta: 0:20:17    time: 3.966356  data: 0.001211  max mem: 4109
I20241203 06:53:24 1958662 dinov2 helpers.py:102]   [330/634]  eta: 0:19:39    time: 3.961462  data: 0.001226  max mem: 4109
I20241203 06:54:03 1958662 dinov2 helpers.py:102]   [340/634]  eta: 0:19:00    time: 3.957375  data: 0.000862  max mem: 4109
I20241203 06:54:43 1958662 dinov2 helpers.py:102]   [350/634]  eta: 0:18:22    time: 3.956607  data: 0.000969  max mem: 4109
I20241203 06:55:22 1958662 dinov2 helpers.py:102]   [360/634]  eta: 0:17:44    time: 3.962864  data: 0.001484  max mem: 4109
I20241203 06:56:02 1958662 dinov2 helpers.py:102]   [370/634]  eta: 0:17:06    time: 3.965966  data: 0.001328  max mem: 4109
I20241203 06:56:42 1958662 dinov2 helpers.py:102]   [380/634]  eta: 0:16:27    time: 3.969659  data: 0.002009  max mem: 4109
I20241203 06:57:22 1958662 dinov2 helpers.py:102]   [390/634]  eta: 0:15:49    time: 3.973328  data: 0.005804  max mem: 4109
I20241203 06:58:01 1958662 dinov2 helpers.py:102]   [400/634]  eta: 0:15:11    time: 3.973254  data: 0.007248  max mem: 4109
I20241203 06:58:41 1958662 dinov2 helpers.py:102]   [410/634]  eta: 0:14:32    time: 3.973456  data: 0.007188  max mem: 4109
I20241203 06:59:21 1958662 dinov2 helpers.py:102]   [420/634]  eta: 0:13:54    time: 3.973591  data: 0.005019  max mem: 4109
I20241203 07:00:01 1958662 dinov2 helpers.py:102]   [430/634]  eta: 0:13:15    time: 3.975909  data: 0.002563  max mem: 4109
I20241203 07:00:40 1958662 dinov2 helpers.py:102]   [440/634]  eta: 0:12:36    time: 3.975951  data: 0.002323  max mem: 4109
I20241203 07:01:20 1958662 dinov2 helpers.py:102]   [450/634]  eta: 0:11:58    time: 3.973882  data: 0.002324  max mem: 4109
I20241203 07:02:00 1958662 dinov2 helpers.py:102]   [460/634]  eta: 0:11:19    time: 3.973975  data: 0.002819  max mem: 4109
I20241203 07:02:40 1958662 dinov2 helpers.py:102]   [470/634]  eta: 0:10:40    time: 3.975198  data: 0.003878  max mem: 4109
I20241203 07:03:19 1958662 dinov2 helpers.py:102]   [480/634]  eta: 0:10:01    time: 3.976058  data: 0.004408  max mem: 4109
I20241203 07:03:59 1958662 dinov2 helpers.py:102]   [490/634]  eta: 0:09:22    time: 3.975062  data: 0.002036  max mem: 4109
I20241203 07:04:39 1958662 dinov2 helpers.py:102]   [500/634]  eta: 0:08:43    time: 3.974122  data: 0.002809  max mem: 4109
I20241203 07:05:19 1958662 dinov2 helpers.py:102]   [510/634]  eta: 0:08:05    time: 3.974203  data: 0.004244  max mem: 4109
I20241203 07:05:58 1958662 dinov2 helpers.py:102]   [520/634]  eta: 0:07:26    time: 3.974401  data: 0.003222  max mem: 4109
I20241203 07:06:38 1958662 dinov2 helpers.py:102]   [530/634]  eta: 0:06:47    time: 3.973780  data: 0.002540  max mem: 4109
I20241203 07:07:18 1958662 dinov2 helpers.py:102]   [540/634]  eta: 0:06:07    time: 3.975504  data: 0.002860  max mem: 4109
I20241203 07:07:58 1958662 dinov2 helpers.py:102]   [550/634]  eta: 0:05:28    time: 3.975906  data: 0.002328  max mem: 4109
I20241203 07:08:37 1958662 dinov2 helpers.py:102]   [560/634]  eta: 0:04:49    time: 3.974163  data: 0.001699  max mem: 4109
I20241203 07:09:17 1958662 dinov2 helpers.py:102]   [570/634]  eta: 0:04:10    time: 3.973637  data: 0.002598  max mem: 4109
I20241203 07:09:57 1958662 dinov2 helpers.py:102]   [580/634]  eta: 0:03:31    time: 3.973293  data: 0.003302  max mem: 4109
I20241203 07:10:36 1958662 dinov2 helpers.py:102]   [590/634]  eta: 0:02:52    time: 3.974625  data: 0.005145  max mem: 4109
I20241203 07:11:16 1958662 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.975858  data: 0.003547  max mem: 4109
I20241203 07:11:56 1958662 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.974926  data: 0.001815  max mem: 4109
I20241203 07:12:36 1958662 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.976056  data: 0.003058  max mem: 4109
I20241203 07:13:12 1958662 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.798827  data: 0.001801  max mem: 4109
I20241203 07:13:29 1958662 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.058088  data: 0.001436  max mem: 4109
I20241203 07:13:29 1958662 dinov2 helpers.py:130]  Total time: 0:41:29 (3.926770 s / it)
I20241203 07:13:29 1958662 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:13:29 1958662 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:13:30 1958662 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:13:30 1958662 dinov2 loaders.py:151] sampler: distributed
I20241203 07:13:30 1958662 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:13:30 1958662 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:28 1958662 dinov2 knn.py:299] Start the k-NN classification.
I20241203 07:14:37 1958662 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:58    time: 9.212373  data: 8.385818  max mem: 8496
I20241203 07:14:54 1958662 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:02:40    time: 2.362208  data: 1.589380  max mem: 8536
I20241203 07:15:04 1958662 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:01:40    time: 1.350190  data: 0.458144  max mem: 8536
I20241203 07:15:15 1958662 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:01:12    time: 1.032495  data: 0.006830  max mem: 8536
I20241203 07:15:25 1958662 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:00:52    time: 1.041621  data: 0.006104  max mem: 8536
I20241203 07:15:35 1958662 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:00:37    time: 1.041307  data: 0.004230  max mem: 8536
I20241203 07:15:46 1958662 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:00:22    time: 1.035041  data: 0.002199  max mem: 8536
I20241203 07:15:56 1958662 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:09    time: 1.034474  data: 0.000961  max mem: 8536
I20241203 07:16:02 1958662 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:01    time: 0.972387  data: 0.000837  max mem: 8536
I20241203 07:16:02 1958662 dinov2 helpers.py:130] Test: Total time: 0:01:34 (1.207008 s / it)
I20241203 07:16:02 1958662 dinov2 utils.py:79] Averaged stats: 
I20241203 07:16:02 1958662 dinov2 knn.py:367] ('full', 10) classifier result: Top1: 0.00 Top5: 0.00
I20241203 07:16:02 1958662 dinov2 knn.py:367] ('full', 20) classifier result: Top1: 0.00 Top5: 0.00
I20241203 07:16:02 1958662 dinov2 knn.py:367] ('full', 100) classifier result: Top1: 0.00 Top5: 0.00
I20241203 07:16:02 1958662 dinov2 knn.py:367] ('full', 200) classifier result: Top1: 0.00 Top5: 0.00
submitit INFO (2024-12-03 07:16:02,936) - Job completed successfully
I20241203 07:16:02 1958662 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-03 07:16:02,937) - Exiting after successful completion
I20241203 07:16:02 1958662 submitit submission.py:61] Exiting after successful completion
