submitit INFO (2024-12-03 06:31:07,599) - Starting with JobEnvironment(job_id=1958659, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-03 06:31:07,600) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn/1958659_submitted.pkl
I20241203 06:31:16 1958665 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 06:31:16 1958665 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 06:31:16 1958665 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 06:31:16 1958665 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 06:31:16 1958665 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 06:31:53 1958665 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 06:31:58 1958665 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 06:31:58 1958665 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 06:32:15 1958665 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 06:32:15 1958665 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 06:32:19 1958665 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 06:32:19 1958665 dinov2 knn.py:260] Extracting features for train set...
I20241203 06:32:19 1958665 dinov2 loaders.py:151] sampler: distributed
I20241203 06:32:19 1958665 dinov2 loaders.py:210] using PyTorch data loader
W20241203 06:32:19 1958665 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 06:32:19 1958665 dinov2 loaders.py:223] # of batches: 634
I20241203 06:33:16 1958665 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 06:33:16 1958665 dinov2 helpers.py:102]   [  0/634]  eta: 10:09:41    time: 57.700058  data: 15.897041  max mem: 3464
I20241203 06:33:49 1958665 dinov2 helpers.py:102]   [ 10/634]  eta: 1:24:58    time: 8.171024  data: 1.446256  max mem: 4109
I20241203 06:34:28 1958665 dinov2 helpers.py:102]   [ 20/634]  eta: 1:02:58    time: 3.577119  data: 0.001120  max mem: 4109
I20241203 06:35:08 1958665 dinov2 helpers.py:102]   [ 30/634]  eta: 0:54:50    time: 3.950490  data: 0.001171  max mem: 4109
I20241203 06:35:47 1958665 dinov2 helpers.py:102]   [ 40/634]  eta: 0:50:22    time: 3.968152  data: 0.001294  max mem: 4109
I20241203 06:36:27 1958665 dinov2 helpers.py:102]   [ 50/634]  eta: 0:47:23    time: 3.971296  data: 0.001458  max mem: 4109
I20241203 06:37:07 1958665 dinov2 helpers.py:102]   [ 60/634]  eta: 0:45:10    time: 3.971574  data: 0.001384  max mem: 4109
I20241203 06:37:47 1958665 dinov2 helpers.py:102]   [ 70/634]  eta: 0:43:24    time: 3.980057  data: 0.001127  max mem: 4109
I20241203 06:38:27 1958665 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:56    time: 3.992130  data: 0.001001  max mem: 4109
I20241203 06:39:06 1958665 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:37    time: 3.994149  data: 0.001896  max mem: 4109
I20241203 06:39:46 1958665 dinov2 helpers.py:102]   [100/634]  eta: 0:39:27    time: 3.991251  data: 0.002758  max mem: 4109
I20241203 06:40:26 1958665 dinov2 helpers.py:102]   [110/634]  eta: 0:38:21    time: 3.991364  data: 0.002297  max mem: 4109
I20241203 06:41:06 1958665 dinov2 helpers.py:102]   [120/634]  eta: 0:37:20    time: 3.990504  data: 0.001637  max mem: 4109
I20241203 06:41:46 1958665 dinov2 helpers.py:102]   [130/634]  eta: 0:36:22    time: 3.983275  data: 0.001421  max mem: 4109
I20241203 06:42:26 1958665 dinov2 helpers.py:102]   [140/634]  eta: 0:35:26    time: 3.978692  data: 0.001623  max mem: 4109
I20241203 06:43:06 1958665 dinov2 helpers.py:102]   [150/634]  eta: 0:34:33    time: 3.977207  data: 0.001597  max mem: 4109
I20241203 06:43:45 1958665 dinov2 helpers.py:102]   [160/634]  eta: 0:33:41    time: 3.974119  data: 0.002102  max mem: 4109
I20241203 06:44:25 1958665 dinov2 helpers.py:102]   [170/634]  eta: 0:32:50    time: 3.973607  data: 0.001671  max mem: 4109
I20241203 06:45:05 1958665 dinov2 helpers.py:102]   [180/634]  eta: 0:32:01    time: 3.973901  data: 0.000644  max mem: 4109
I20241203 06:45:44 1958665 dinov2 helpers.py:102]   [190/634]  eta: 0:31:13    time: 3.974595  data: 0.001150  max mem: 4109
I20241203 06:46:24 1958665 dinov2 helpers.py:102]   [200/634]  eta: 0:30:25    time: 3.973844  data: 0.001368  max mem: 4109
I20241203 06:47:04 1958665 dinov2 helpers.py:102]   [210/634]  eta: 0:29:38    time: 3.973061  data: 0.000844  max mem: 4109
I20241203 06:47:44 1958665 dinov2 helpers.py:102]   [220/634]  eta: 0:28:52    time: 3.973806  data: 0.000540  max mem: 4109
I20241203 06:48:23 1958665 dinov2 helpers.py:102]   [230/634]  eta: 0:28:07    time: 3.974134  data: 0.000754  max mem: 4109
I20241203 06:49:03 1958665 dinov2 helpers.py:102]   [240/634]  eta: 0:27:22    time: 3.974351  data: 0.000802  max mem: 4109
I20241203 06:49:43 1958665 dinov2 helpers.py:102]   [250/634]  eta: 0:26:37    time: 3.976810  data: 0.000539  max mem: 4109
I20241203 06:50:23 1958665 dinov2 helpers.py:102]   [260/634]  eta: 0:25:53    time: 3.976315  data: 0.000666  max mem: 4109
I20241203 06:51:02 1958665 dinov2 helpers.py:102]   [270/634]  eta: 0:25:09    time: 3.973425  data: 0.001044  max mem: 4109
I20241203 06:51:42 1958665 dinov2 helpers.py:102]   [280/634]  eta: 0:24:25    time: 3.973125  data: 0.001026  max mem: 4109
I20241203 06:52:22 1958665 dinov2 helpers.py:102]   [290/634]  eta: 0:23:42    time: 3.972982  data: 0.002712  max mem: 4109
I20241203 06:53:02 1958665 dinov2 helpers.py:102]   [300/634]  eta: 0:22:59    time: 3.974206  data: 0.002888  max mem: 4109
I20241203 06:53:41 1958665 dinov2 helpers.py:102]   [310/634]  eta: 0:22:16    time: 3.974339  data: 0.001641  max mem: 4109
I20241203 06:54:21 1958665 dinov2 helpers.py:102]   [320/634]  eta: 0:21:33    time: 3.973379  data: 0.001338  max mem: 4109
I20241203 06:55:01 1958665 dinov2 helpers.py:102]   [330/634]  eta: 0:20:51    time: 3.973785  data: 0.001357  max mem: 4109
I20241203 06:55:41 1958665 dinov2 helpers.py:102]   [340/634]  eta: 0:20:08    time: 3.974929  data: 0.002255  max mem: 4109
I20241203 06:56:20 1958665 dinov2 helpers.py:102]   [350/634]  eta: 0:19:26    time: 3.975064  data: 0.001826  max mem: 4109
I20241203 06:57:00 1958665 dinov2 helpers.py:102]   [360/634]  eta: 0:18:44    time: 3.974974  data: 0.001179  max mem: 4109
I20241203 06:57:40 1958665 dinov2 helpers.py:102]   [370/634]  eta: 0:18:02    time: 3.975344  data: 0.001183  max mem: 4109
I20241203 06:58:20 1958665 dinov2 helpers.py:102]   [380/634]  eta: 0:17:20    time: 3.974417  data: 0.001602  max mem: 4109
I20241203 06:58:59 1958665 dinov2 helpers.py:102]   [390/634]  eta: 0:16:38    time: 3.975429  data: 0.001299  max mem: 4109
I20241203 06:59:39 1958665 dinov2 helpers.py:102]   [400/634]  eta: 0:15:57    time: 3.977413  data: 0.001005  max mem: 4109
I20241203 07:00:19 1958665 dinov2 helpers.py:102]   [410/634]  eta: 0:15:15    time: 3.981198  data: 0.001041  max mem: 4109
I20241203 07:00:59 1958665 dinov2 helpers.py:102]   [420/634]  eta: 0:14:34    time: 3.985939  data: 0.001148  max mem: 4109
I20241203 07:01:39 1958665 dinov2 helpers.py:102]   [430/634]  eta: 0:13:53    time: 3.983960  data: 0.001308  max mem: 4109
I20241203 07:02:19 1958665 dinov2 helpers.py:102]   [440/634]  eta: 0:13:11    time: 3.985616  data: 0.001207  max mem: 4109
I20241203 07:02:58 1958665 dinov2 helpers.py:102]   [450/634]  eta: 0:12:30    time: 3.988531  data: 0.001100  max mem: 4109
I20241203 07:03:38 1958665 dinov2 helpers.py:102]   [460/634]  eta: 0:11:49    time: 3.987840  data: 0.001334  max mem: 4109
I20241203 07:04:18 1958665 dinov2 helpers.py:102]   [470/634]  eta: 0:11:08    time: 3.984993  data: 0.001356  max mem: 4109
I20241203 07:04:58 1958665 dinov2 helpers.py:102]   [480/634]  eta: 0:10:27    time: 3.985720  data: 0.000803  max mem: 4109
I20241203 07:05:38 1958665 dinov2 helpers.py:102]   [490/634]  eta: 0:09:46    time: 3.992206  data: 0.000670  max mem: 4109
I20241203 07:06:18 1958665 dinov2 helpers.py:102]   [500/634]  eta: 0:09:05    time: 3.992125  data: 0.000810  max mem: 4109
I20241203 07:06:58 1958665 dinov2 helpers.py:102]   [510/634]  eta: 0:08:24    time: 3.990264  data: 0.000972  max mem: 4109
I20241203 07:07:38 1958665 dinov2 helpers.py:102]   [520/634]  eta: 0:07:43    time: 3.990373  data: 0.001095  max mem: 4109
I20241203 07:08:18 1958665 dinov2 helpers.py:102]   [530/634]  eta: 0:07:02    time: 3.990361  data: 0.001007  max mem: 4109
I20241203 07:08:58 1958665 dinov2 helpers.py:102]   [540/634]  eta: 0:06:22    time: 3.990345  data: 0.001030  max mem: 4109
I20241203 07:09:37 1958665 dinov2 helpers.py:102]   [550/634]  eta: 0:05:41    time: 3.990483  data: 0.001686  max mem: 4109
I20241203 07:10:17 1958665 dinov2 helpers.py:102]   [560/634]  eta: 0:05:00    time: 3.991435  data: 0.002171  max mem: 4109
I20241203 07:10:57 1958665 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.989526  data: 0.001915  max mem: 4109
I20241203 07:11:37 1958665 dinov2 helpers.py:102]   [580/634]  eta: 0:03:39    time: 3.988582  data: 0.001069  max mem: 4109
I20241203 07:12:17 1958665 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.991154  data: 0.000663  max mem: 4109
I20241203 07:12:56 1958665 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.921829  data: 0.001169  max mem: 4109
I20241203 07:13:30 1958665 dinov2 helpers.py:102]   [610/634]  eta: 0:01:37    time: 3.672129  data: 0.001285  max mem: 4109
I20241203 07:13:57 1958665 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.056256  data: 0.000801  max mem: 4109
I20241203 07:14:19 1958665 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.410332  data: 0.001390  max mem: 4109
I20241203 07:14:25 1958665 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.298410  data: 0.001320  max mem: 4109
I20241203 07:14:25 1958665 dinov2 helpers.py:130]  Total time: 0:42:06 (3.984896 s / it)
I20241203 07:14:25 1958665 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 07:14:25 1958665 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 07:14:25 1958665 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 07:14:25 1958665 dinov2 loaders.py:151] sampler: distributed
I20241203 07:14:25 1958665 dinov2 loaders.py:210] using PyTorch data loader
I20241203 07:14:25 1958665 dinov2 loaders.py:223] # of batches: 78
I20241203 07:14:30 1958665 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 07:14:44,167) - Submitted job triggered an exception
E20241203 07:14:44 1958665 submitit submission.py:68] Submitted job triggered an exception
