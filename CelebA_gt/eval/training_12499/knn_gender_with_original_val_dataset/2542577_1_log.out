submitit INFO (2024-12-04 09:27:28,378) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-04 09:27:28,379) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:37 2542579 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:37 2542579 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:37 2542579 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:37 2542579 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:37 2542579 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:12 2542579 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:16 2542579 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:17 2542579 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:31 2542579 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:31 2542579 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:37 2542579 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:37 2542579 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:37 2542579 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:37 2542579 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:37 2542579 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:37 2542579 dinov2 loaders.py:223] # of batches: 634
I20241204 09:29:31 2542579 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:29:31 2542579 dinov2 helpers.py:102]   [  0/634]  eta: 9:24:51    time: 53.456341  data: 13.724692  max mem: 3463
I20241204 09:30:03 2542579 dinov2 helpers.py:102]   [ 10/634]  eta: 1:20:58    time: 7.785364  data: 1.249971  max mem: 4109
I20241204 09:30:42 2542579 dinov2 helpers.py:102]   [ 20/634]  eta: 1:00:58    time: 3.582774  data: 0.001683  max mem: 4109
I20241204 09:31:22 2542579 dinov2 helpers.py:102]   [ 30/634]  eta: 0:53:29    time: 3.953817  data: 0.000781  max mem: 4109
I20241204 09:32:02 2542579 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:21    time: 3.966787  data: 0.000781  max mem: 4109
I20241204 09:32:41 2542579 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:36    time: 3.973326  data: 0.000845  max mem: 4109
I20241204 09:33:21 2542579 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:31    time: 3.973631  data: 0.001570  max mem: 4109
I20241204 09:34:01 2542579 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:51    time: 3.976729  data: 0.001737  max mem: 4109
I20241204 09:34:41 2542579 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:25    time: 3.977600  data: 0.001157  max mem: 4109
I20241204 09:35:21 2542579 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:10    time: 3.976549  data: 0.001114  max mem: 4109
I20241204 09:36:00 2542579 dinov2 helpers.py:102]   [100/634]  eta: 0:39:02    time: 3.976124  data: 0.001174  max mem: 4109
I20241204 09:36:40 2542579 dinov2 helpers.py:102]   [110/634]  eta: 0:37:58    time: 3.974236  data: 0.001291  max mem: 4109
I20241204 09:37:20 2542579 dinov2 helpers.py:102]   [120/634]  eta: 0:36:59    time: 3.975541  data: 0.001244  max mem: 4109
I20241204 09:38:00 2542579 dinov2 helpers.py:102]   [130/634]  eta: 0:36:03    time: 3.978371  data: 0.000894  max mem: 4109
I20241204 09:38:39 2542579 dinov2 helpers.py:102]   [140/634]  eta: 0:35:09    time: 3.979488  data: 0.000790  max mem: 4109
I20241204 09:39:19 2542579 dinov2 helpers.py:102]   [150/634]  eta: 0:34:17    time: 3.977064  data: 0.001256  max mem: 4109
I20241204 09:39:59 2542579 dinov2 helpers.py:102]   [160/634]  eta: 0:33:26    time: 3.976081  data: 0.001139  max mem: 4109
I20241204 09:40:39 2542579 dinov2 helpers.py:102]   [170/634]  eta: 0:32:37    time: 3.977816  data: 0.000901  max mem: 4109
I20241204 09:41:18 2542579 dinov2 helpers.py:102]   [180/634]  eta: 0:31:49    time: 3.979699  data: 0.000901  max mem: 4109
I20241204 09:41:58 2542579 dinov2 helpers.py:102]   [190/634]  eta: 0:31:02    time: 3.981290  data: 0.000760  max mem: 4109
I20241204 09:42:38 2542579 dinov2 helpers.py:102]   [200/634]  eta: 0:30:15    time: 3.977808  data: 0.000681  max mem: 4109
I20241204 09:43:18 2542579 dinov2 helpers.py:102]   [210/634]  eta: 0:29:29    time: 3.977088  data: 0.000973  max mem: 4109
I20241204 09:43:58 2542579 dinov2 helpers.py:102]   [220/634]  eta: 0:28:44    time: 3.979547  data: 0.001118  max mem: 4109
I20241204 09:44:37 2542579 dinov2 helpers.py:102]   [230/634]  eta: 0:27:59    time: 3.977824  data: 0.000861  max mem: 4109
I20241204 09:45:17 2542579 dinov2 helpers.py:102]   [240/634]  eta: 0:27:14    time: 3.977909  data: 0.001044  max mem: 4109
I20241204 09:45:57 2542579 dinov2 helpers.py:102]   [250/634]  eta: 0:26:30    time: 3.981424  data: 0.001105  max mem: 4109
I20241204 09:46:37 2542579 dinov2 helpers.py:102]   [260/634]  eta: 0:25:47    time: 3.984272  data: 0.000785  max mem: 4109
I20241204 09:47:17 2542579 dinov2 helpers.py:102]   [270/634]  eta: 0:25:03    time: 3.982396  data: 0.000697  max mem: 4109
I20241204 09:47:57 2542579 dinov2 helpers.py:102]   [280/634]  eta: 0:24:20    time: 3.981442  data: 0.001057  max mem: 4109
I20241204 09:48:36 2542579 dinov2 helpers.py:102]   [290/634]  eta: 0:23:37    time: 3.983319  data: 0.002152  max mem: 4109
I20241204 09:49:16 2542579 dinov2 helpers.py:102]   [300/634]  eta: 0:22:54    time: 3.982225  data: 0.001891  max mem: 4109
I20241204 09:49:56 2542579 dinov2 helpers.py:102]   [310/634]  eta: 0:22:12    time: 3.984140  data: 0.001137  max mem: 4109
I20241204 09:50:36 2542579 dinov2 helpers.py:102]   [320/634]  eta: 0:21:29    time: 3.985943  data: 0.001198  max mem: 4109
I20241204 09:51:16 2542579 dinov2 helpers.py:102]   [330/634]  eta: 0:20:47    time: 3.979370  data: 0.000822  max mem: 4109
I20241204 09:51:55 2542579 dinov2 helpers.py:102]   [340/634]  eta: 0:20:05    time: 3.976020  data: 0.001095  max mem: 4109
I20241204 09:52:35 2542579 dinov2 helpers.py:102]   [350/634]  eta: 0:19:23    time: 3.978577  data: 0.001144  max mem: 4109
I20241204 09:53:15 2542579 dinov2 helpers.py:102]   [360/634]  eta: 0:18:41    time: 3.980195  data: 0.000922  max mem: 4109
I20241204 09:53:55 2542579 dinov2 helpers.py:102]   [370/634]  eta: 0:17:59    time: 3.979553  data: 0.000817  max mem: 4109
I20241204 09:54:35 2542579 dinov2 helpers.py:102]   [380/634]  eta: 0:17:18    time: 3.982266  data: 0.000760  max mem: 4109
I20241204 09:55:14 2542579 dinov2 helpers.py:102]   [390/634]  eta: 0:16:36    time: 3.985008  data: 0.001151  max mem: 4109
I20241204 09:55:54 2542579 dinov2 helpers.py:102]   [400/634]  eta: 0:15:55    time: 3.984278  data: 0.001146  max mem: 4109
I20241204 09:56:34 2542579 dinov2 helpers.py:102]   [410/634]  eta: 0:15:13    time: 3.987015  data: 0.002216  max mem: 4109
I20241204 09:57:14 2542579 dinov2 helpers.py:102]   [420/634]  eta: 0:14:32    time: 3.988831  data: 0.002366  max mem: 4109
I20241204 09:57:54 2542579 dinov2 helpers.py:102]   [430/634]  eta: 0:13:51    time: 3.989173  data: 0.001288  max mem: 4109
I20241204 09:58:34 2542579 dinov2 helpers.py:102]   [440/634]  eta: 0:13:10    time: 3.989211  data: 0.001122  max mem: 4109
I20241204 09:59:14 2542579 dinov2 helpers.py:102]   [450/634]  eta: 0:12:29    time: 3.989022  data: 0.000929  max mem: 4109
I20241204 09:59:54 2542579 dinov2 helpers.py:102]   [460/634]  eta: 0:11:48    time: 3.982742  data: 0.000913  max mem: 4109
I20241204 10:00:33 2542579 dinov2 helpers.py:102]   [470/634]  eta: 0:11:07    time: 3.978551  data: 0.000761  max mem: 4109
I20241204 10:01:13 2542579 dinov2 helpers.py:102]   [480/634]  eta: 0:10:26    time: 3.981093  data: 0.000887  max mem: 4109
I20241204 10:01:53 2542579 dinov2 helpers.py:102]   [490/634]  eta: 0:09:45    time: 3.979607  data: 0.000809  max mem: 4109
I20241204 10:02:33 2542579 dinov2 helpers.py:102]   [500/634]  eta: 0:09:04    time: 3.978626  data: 0.000556  max mem: 4109
I20241204 10:03:13 2542579 dinov2 helpers.py:102]   [510/634]  eta: 0:08:23    time: 3.977745  data: 0.002422  max mem: 4109
I20241204 10:03:52 2542579 dinov2 helpers.py:102]   [520/634]  eta: 0:07:42    time: 3.976967  data: 0.006557  max mem: 4109
I20241204 10:04:32 2542579 dinov2 helpers.py:102]   [530/634]  eta: 0:07:02    time: 3.976086  data: 0.007259  max mem: 4109
I20241204 10:05:12 2542579 dinov2 helpers.py:102]   [540/634]  eta: 0:06:21    time: 3.975997  data: 0.003485  max mem: 4109
I20241204 10:05:52 2542579 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.975332  data: 0.003660  max mem: 4109
I20241204 10:06:31 2542579 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.973176  data: 0.003421  max mem: 4109
I20241204 10:07:11 2542579 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.973857  data: 0.002775  max mem: 4109
I20241204 10:07:51 2542579 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.973644  data: 0.005304  max mem: 4109
I20241204 10:08:30 2542579 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.973036  data: 0.006680  max mem: 4109
I20241204 10:09:10 2542579 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.974147  data: 0.004906  max mem: 4109
I20241204 10:09:49 2542579 dinov2 helpers.py:102]   [610/634]  eta: 0:01:37    time: 3.927573  data: 0.003836  max mem: 4109
I20241204 10:10:27 2542579 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.826169  data: 0.003986  max mem: 4109
I20241204 10:11:06 2542579 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.850768  data: 0.001865  max mem: 4109
I20241204 10:11:23 2542579 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.159043  data: 0.000862  max mem: 4109
I20241204 10:11:24 2542579 dinov2 helpers.py:130]  Total time: 0:42:46 (4.048259 s / it)
I20241204 10:11:24 2542579 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:11:24 2542579 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:11:24 2542579 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:11:24 2542579 dinov2 loaders.py:151] sampler: distributed
I20241204 10:11:24 2542579 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:11:24 2542579 dinov2 loaders.py:223] # of batches: 78
I20241204 10:11:24 2542579 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:11:39 2542579 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:17:41    time: 13.607972  data: 10.121800  max mem: 4109
I20241204 10:12:19 2542579 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:31    time: 4.877626  data: 0.931445  max mem: 4109
I20241204 10:12:59 2542579 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:18    time: 4.000923  data: 0.009196  max mem: 4109
I20241204 10:13:39 2542579 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:26    time: 3.993243  data: 0.006726  max mem: 4109
I20241204 10:14:19 2542579 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:40    time: 3.997226  data: 0.004774  max mem: 4109
I20241204 10:14:55 2542579 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:55    time: 3.833454  data: 0.004746  max mem: 4109
I20241204 10:15:24 2542579 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 3.273891  data: 0.007037  max mem: 4109
I20241204 10:15:49 2542579 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:29    time: 2.684358  data: 0.004757  max mem: 4109
I20241204 10:16:01 2542579 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.213742  data: 0.002307  max mem: 4109
I20241204 10:16:01 2542579 dinov2 helpers.py:130] Test: Total time: 0:04:35 (3.533278 s / it)
I20241204 10:16:01 2542579 dinov2 utils.py:79] Averaged stats: 
I20241204 10:16:01 2542579 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.15
I20241204 10:16:01 2542579 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.03
I20241204 10:16:01 2542579 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:16:01 2542579 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.92
submitit INFO (2024-12-04 10:16:01,539) - Job completed successfully
I20241204 10:16:01 2542579 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:16:01,541) - Exiting after successful completion
I20241204 10:16:01 2542579 submitit submission.py:61] Exiting after successful completion
