submitit INFO (2024-12-04 08:36:13,527) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-04 08:36:13,527) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:22 2519722 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:22 2519722 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:22 2519722 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:22 2519722 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:22 2519722 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:52 2519722 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:36:56 2519722 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:36:56 2519722 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:02 2519722 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:02 2519722 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:05 2519722 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:05 2519722 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:05 2519722 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:05 2519722 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:05 2519722 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:05 2519722 dinov2 loaders.py:223] # of batches: 634
I20241204 08:37:36 2519722 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:37:36 2519722 dinov2 helpers.py:102]   [  0/634]  eta: 5:23:11    time: 30.585302  data: 9.102509  max mem: 3463
I20241204 08:37:44 2519722 dinov2 helpers.py:102]   [ 10/634]  eta: 0:36:29    time: 3.508850  data: 0.830129  max mem: 4109
I20241204 08:37:59 2519722 dinov2 helpers.py:102]   [ 20/634]  eta: 0:26:04    time: 1.146308  data: 0.001615  max mem: 4109
I20241204 08:38:17 2519722 dinov2 helpers.py:102]   [ 30/634]  eta: 0:23:27    time: 1.682464  data: 0.000325  max mem: 4109
I20241204 08:38:56 2519722 dinov2 helpers.py:102]   [ 40/634]  eta: 0:26:53    time: 2.893344  data: 0.000505  max mem: 4109
I20241204 08:39:36 2519722 dinov2 helpers.py:102]   [ 50/634]  eta: 0:28:47    time: 3.929790  data: 0.000618  max mem: 4109
I20241204 08:40:15 2519722 dinov2 helpers.py:102]   [ 60/634]  eta: 0:29:52    time: 3.953298  data: 0.000545  max mem: 4109
I20241204 08:40:55 2519722 dinov2 helpers.py:102]   [ 70/634]  eta: 0:30:27    time: 3.960418  data: 0.000665  max mem: 4109
I20241204 08:41:35 2519722 dinov2 helpers.py:102]   [ 80/634]  eta: 0:30:44    time: 3.965509  data: 0.000684  max mem: 4109
I20241204 08:42:14 2519722 dinov2 helpers.py:102]   [ 90/634]  eta: 0:30:49    time: 3.968729  data: 0.000584  max mem: 4109
I20241204 08:42:54 2519722 dinov2 helpers.py:102]   [100/634]  eta: 0:30:45    time: 3.968272  data: 0.000638  max mem: 4109
I20241204 08:43:34 2519722 dinov2 helpers.py:102]   [110/634]  eta: 0:30:35    time: 3.966062  data: 0.000678  max mem: 4109
I20241204 08:44:13 2519722 dinov2 helpers.py:102]   [120/634]  eta: 0:30:19    time: 3.962695  data: 0.000656  max mem: 4109
I20241204 08:44:53 2519722 dinov2 helpers.py:102]   [130/634]  eta: 0:30:00    time: 3.965576  data: 0.000671  max mem: 4109
I20241204 08:45:33 2519722 dinov2 helpers.py:102]   [140/634]  eta: 0:29:38    time: 3.967395  data: 0.000832  max mem: 4109
I20241204 08:46:12 2519722 dinov2 helpers.py:102]   [150/634]  eta: 0:29:14    time: 3.963568  data: 0.000789  max mem: 4109
I20241204 08:46:52 2519722 dinov2 helpers.py:102]   [160/634]  eta: 0:28:48    time: 3.962403  data: 0.000837  max mem: 4109
I20241204 08:47:32 2519722 dinov2 helpers.py:102]   [170/634]  eta: 0:28:20    time: 3.965221  data: 0.000845  max mem: 4109
I20241204 08:48:11 2519722 dinov2 helpers.py:102]   [180/634]  eta: 0:27:51    time: 3.964446  data: 0.000690  max mem: 4109
I20241204 08:48:51 2519722 dinov2 helpers.py:102]   [190/634]  eta: 0:27:20    time: 3.963601  data: 0.000660  max mem: 4109
I20241204 08:49:31 2519722 dinov2 helpers.py:102]   [200/634]  eta: 0:26:49    time: 3.964454  data: 0.001567  max mem: 4109
I20241204 08:50:10 2519722 dinov2 helpers.py:102]   [210/634]  eta: 0:26:17    time: 3.966300  data: 0.001553  max mem: 4109
I20241204 08:50:50 2519722 dinov2 helpers.py:102]   [220/634]  eta: 0:25:45    time: 3.968786  data: 0.000693  max mem: 4109
I20241204 08:51:30 2519722 dinov2 helpers.py:102]   [230/634]  eta: 0:25:12    time: 3.968103  data: 0.000754  max mem: 4109
I20241204 08:52:09 2519722 dinov2 helpers.py:102]   [240/634]  eta: 0:24:38    time: 3.966435  data: 0.000690  max mem: 4109
I20241204 08:52:49 2519722 dinov2 helpers.py:102]   [250/634]  eta: 0:24:04    time: 3.967048  data: 0.000704  max mem: 4109
I20241204 08:53:29 2519722 dinov2 helpers.py:102]   [260/634]  eta: 0:23:29    time: 3.966359  data: 0.001048  max mem: 4109
I20241204 08:54:08 2519722 dinov2 helpers.py:102]   [270/634]  eta: 0:22:54    time: 3.963537  data: 0.001277  max mem: 4109
I20241204 08:54:48 2519722 dinov2 helpers.py:102]   [280/634]  eta: 0:22:18    time: 3.963496  data: 0.000945  max mem: 4109
I20241204 08:55:27 2519722 dinov2 helpers.py:102]   [290/634]  eta: 0:21:43    time: 3.963490  data: 0.001052  max mem: 4109
I20241204 08:56:07 2519722 dinov2 helpers.py:102]   [300/634]  eta: 0:21:07    time: 3.965208  data: 0.001063  max mem: 4109
I20241204 08:56:47 2519722 dinov2 helpers.py:102]   [310/634]  eta: 0:20:31    time: 3.966993  data: 0.000697  max mem: 4109
I20241204 08:57:26 2519722 dinov2 helpers.py:102]   [320/634]  eta: 0:19:54    time: 3.964282  data: 0.000584  max mem: 4109
I20241204 08:58:06 2519722 dinov2 helpers.py:102]   [330/634]  eta: 0:19:18    time: 3.962300  data: 0.000587  max mem: 4109
I20241204 08:58:46 2519722 dinov2 helpers.py:102]   [340/634]  eta: 0:18:41    time: 3.962591  data: 0.000563  max mem: 4109
I20241204 08:59:25 2519722 dinov2 helpers.py:102]   [350/634]  eta: 0:18:04    time: 3.962444  data: 0.000939  max mem: 4109
I20241204 09:00:05 2519722 dinov2 helpers.py:102]   [360/634]  eta: 0:17:27    time: 3.963891  data: 0.001333  max mem: 4109
I20241204 09:00:45 2519722 dinov2 helpers.py:102]   [370/634]  eta: 0:16:50    time: 3.964865  data: 0.001320  max mem: 4109
I20241204 09:01:24 2519722 dinov2 helpers.py:102]   [380/634]  eta: 0:16:12    time: 3.961114  data: 0.001987  max mem: 4109
I20241204 09:02:04 2519722 dinov2 helpers.py:102]   [390/634]  eta: 0:15:35    time: 3.960140  data: 0.001797  max mem: 4109
I20241204 09:02:43 2519722 dinov2 helpers.py:102]   [400/634]  eta: 0:14:57    time: 3.957365  data: 0.000973  max mem: 4109
I20241204 09:03:23 2519722 dinov2 helpers.py:102]   [410/634]  eta: 0:14:20    time: 3.957117  data: 0.000965  max mem: 4109
I20241204 09:04:03 2519722 dinov2 helpers.py:102]   [420/634]  eta: 0:13:42    time: 3.960103  data: 0.000875  max mem: 4109
I20241204 09:04:42 2519722 dinov2 helpers.py:102]   [430/634]  eta: 0:13:04    time: 3.956350  data: 0.000766  max mem: 4109
I20241204 09:05:22 2519722 dinov2 helpers.py:102]   [440/634]  eta: 0:12:26    time: 3.955216  data: 0.000687  max mem: 4109
I20241204 09:06:01 2519722 dinov2 helpers.py:102]   [450/634]  eta: 0:11:48    time: 3.956323  data: 0.000647  max mem: 4109
I20241204 09:06:41 2519722 dinov2 helpers.py:102]   [460/634]  eta: 0:11:10    time: 3.956232  data: 0.000691  max mem: 4109
I20241204 09:07:20 2519722 dinov2 helpers.py:102]   [470/634]  eta: 0:10:32    time: 3.956395  data: 0.001654  max mem: 4109
I20241204 09:08:00 2519722 dinov2 helpers.py:102]   [480/634]  eta: 0:09:53    time: 3.954306  data: 0.001624  max mem: 4109
I20241204 09:08:39 2519722 dinov2 helpers.py:102]   [490/634]  eta: 0:09:15    time: 3.953992  data: 0.000589  max mem: 4109
I20241204 09:09:19 2519722 dinov2 helpers.py:102]   [500/634]  eta: 0:08:37    time: 3.954140  data: 0.000553  max mem: 4109
I20241204 09:09:58 2519722 dinov2 helpers.py:102]   [510/634]  eta: 0:07:58    time: 3.951853  data: 0.000819  max mem: 4109
I20241204 09:10:38 2519722 dinov2 helpers.py:102]   [520/634]  eta: 0:07:20    time: 3.952996  data: 0.001547  max mem: 4109
I20241204 09:11:18 2519722 dinov2 helpers.py:102]   [530/634]  eta: 0:06:42    time: 3.955862  data: 0.001414  max mem: 4109
I20241204 09:11:57 2519722 dinov2 helpers.py:102]   [540/634]  eta: 0:06:03    time: 3.956125  data: 0.000803  max mem: 4109
I20241204 09:12:37 2519722 dinov2 helpers.py:102]   [550/634]  eta: 0:05:24    time: 3.954337  data: 0.001000  max mem: 4109
I20241204 09:13:16 2519722 dinov2 helpers.py:102]   [560/634]  eta: 0:04:46    time: 3.955128  data: 0.000897  max mem: 4109
I20241204 09:13:56 2519722 dinov2 helpers.py:102]   [570/634]  eta: 0:04:07    time: 3.954866  data: 0.000948  max mem: 4109
I20241204 09:14:35 2519722 dinov2 helpers.py:102]   [580/634]  eta: 0:03:29    time: 3.954765  data: 0.001047  max mem: 4109
I20241204 09:15:15 2519722 dinov2 helpers.py:102]   [590/634]  eta: 0:02:50    time: 3.954664  data: 0.000956  max mem: 4109
I20241204 09:15:54 2519722 dinov2 helpers.py:102]   [600/634]  eta: 0:02:11    time: 3.955541  data: 0.001008  max mem: 4109
I20241204 09:16:34 2519722 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.955777  data: 0.000907  max mem: 4109
I20241204 09:17:14 2519722 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.955628  data: 0.001120  max mem: 4109
I20241204 09:17:53 2519722 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.957238  data: 0.001380  max mem: 4109
I20241204 09:18:13 2519722 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.336260  data: 0.001331  max mem: 4109
I20241204 09:18:13 2519722 dinov2 helpers.py:130]  Total time: 0:41:07 (3.892559 s / it)
I20241204 09:18:13 2519722 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:18:13 2519722 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:18:14 2519722 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:18:14 2519722 dinov2 loaders.py:151] sampler: distributed
I20241204 09:18:14 2519722 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:18:14 2519722 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:18:14,355) - Submitted job triggered an exception
E20241204 09:18:14 2519722 submitit submission.py:68] Submitted job triggered an exception
