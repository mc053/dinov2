submitit INFO (2024-12-04 08:36:13,508) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-04 08:36:13,509) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:22 2519724 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:22 2519724 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:22 2519724 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:22 2519724 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:22 2519724 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:57 2519724 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:37:02 2519724 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:37:02 2519724 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:14 2519724 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:14 2519724 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:20 2519724 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:20 2519724 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:20 2519724 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:20 2519724 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:20 2519724 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:20 2519724 dinov2 loaders.py:223] # of batches: 634
I20241204 08:38:15 2519724 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:38:15 2519724 dinov2 helpers.py:102]   [  0/634]  eta: 9:44:10    time: 55.284283  data: 15.184951  max mem: 3463
I20241204 08:38:47 2519724 dinov2 helpers.py:102]   [ 10/634]  eta: 1:22:30    time: 7.934210  data: 1.384621  max mem: 4109
I20241204 08:39:26 2519724 dinov2 helpers.py:102]   [ 20/634]  eta: 1:01:46    time: 3.573695  data: 0.002771  max mem: 4109
I20241204 08:40:06 2519724 dinov2 helpers.py:102]   [ 30/634]  eta: 0:54:00    time: 3.953311  data: 0.000925  max mem: 4109
I20241204 08:40:46 2519724 dinov2 helpers.py:102]   [ 40/634]  eta: 0:49:44    time: 3.963185  data: 0.001573  max mem: 4109
I20241204 08:41:25 2519724 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:53    time: 3.968292  data: 0.001674  max mem: 4109
I20241204 08:42:05 2519724 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:45    time: 3.970114  data: 0.000872  max mem: 4109
I20241204 08:42:45 2519724 dinov2 helpers.py:102]   [ 70/634]  eta: 0:43:02    time: 3.967315  data: 0.000903  max mem: 4109
I20241204 08:43:24 2519724 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:34    time: 3.963695  data: 0.001088  max mem: 4109
I20241204 08:44:04 2519724 dinov2 helpers.py:102]   [ 90/634]  eta: 0:40:17    time: 3.966191  data: 0.000916  max mem: 4109
I20241204 08:44:44 2519724 dinov2 helpers.py:102]   [100/634]  eta: 0:39:07    time: 3.967374  data: 0.000831  max mem: 4109
I20241204 08:45:23 2519724 dinov2 helpers.py:102]   [110/634]  eta: 0:38:03    time: 3.965730  data: 0.000982  max mem: 4109
I20241204 08:46:03 2519724 dinov2 helpers.py:102]   [120/634]  eta: 0:37:03    time: 3.964539  data: 0.002618  max mem: 4109
I20241204 08:46:43 2519724 dinov2 helpers.py:102]   [130/634]  eta: 0:36:05    time: 3.964233  data: 0.002462  max mem: 4109
I20241204 08:47:22 2519724 dinov2 helpers.py:102]   [140/634]  eta: 0:35:11    time: 3.965191  data: 0.001240  max mem: 4109
I20241204 08:48:02 2519724 dinov2 helpers.py:102]   [150/634]  eta: 0:34:18    time: 3.964369  data: 0.001339  max mem: 4109
I20241204 08:48:42 2519724 dinov2 helpers.py:102]   [160/634]  eta: 0:33:27    time: 3.966235  data: 0.000964  max mem: 4109
I20241204 08:49:21 2519724 dinov2 helpers.py:102]   [170/634]  eta: 0:32:38    time: 3.967133  data: 0.001108  max mem: 4109
I20241204 08:50:01 2519724 dinov2 helpers.py:102]   [180/634]  eta: 0:31:49    time: 3.963487  data: 0.001356  max mem: 4109
I20241204 08:50:41 2519724 dinov2 helpers.py:102]   [190/634]  eta: 0:31:01    time: 3.964767  data: 0.001530  max mem: 4109
I20241204 08:51:20 2519724 dinov2 helpers.py:102]   [200/634]  eta: 0:30:14    time: 3.964719  data: 0.001569  max mem: 4109
I20241204 08:52:00 2519724 dinov2 helpers.py:102]   [210/634]  eta: 0:29:28    time: 3.960690  data: 0.001194  max mem: 4109
I20241204 08:52:40 2519724 dinov2 helpers.py:102]   [220/634]  eta: 0:28:42    time: 3.961637  data: 0.001027  max mem: 4109
I20241204 08:53:19 2519724 dinov2 helpers.py:102]   [230/634]  eta: 0:27:57    time: 3.963622  data: 0.000848  max mem: 4109
I20241204 08:53:59 2519724 dinov2 helpers.py:102]   [240/634]  eta: 0:27:13    time: 3.964402  data: 0.000664  max mem: 4109
I20241204 08:54:38 2519724 dinov2 helpers.py:102]   [250/634]  eta: 0:26:29    time: 3.964235  data: 0.000984  max mem: 4109
I20241204 08:55:18 2519724 dinov2 helpers.py:102]   [260/634]  eta: 0:25:45    time: 3.964303  data: 0.001912  max mem: 4109
I20241204 08:55:58 2519724 dinov2 helpers.py:102]   [270/634]  eta: 0:25:01    time: 3.967067  data: 0.002047  max mem: 4109
I20241204 08:56:37 2519724 dinov2 helpers.py:102]   [280/634]  eta: 0:24:18    time: 3.967013  data: 0.001174  max mem: 4109
I20241204 08:57:17 2519724 dinov2 helpers.py:102]   [290/634]  eta: 0:23:35    time: 3.967136  data: 0.001047  max mem: 4109
I20241204 08:57:57 2519724 dinov2 helpers.py:102]   [300/634]  eta: 0:22:52    time: 3.966009  data: 0.001057  max mem: 4109
I20241204 08:58:36 2519724 dinov2 helpers.py:102]   [310/634]  eta: 0:22:10    time: 3.965791  data: 0.001079  max mem: 4109
I20241204 08:59:16 2519724 dinov2 helpers.py:102]   [320/634]  eta: 0:21:27    time: 3.966092  data: 0.002085  max mem: 4109
I20241204 08:59:56 2519724 dinov2 helpers.py:102]   [330/634]  eta: 0:20:45    time: 3.967759  data: 0.002140  max mem: 4109
I20241204 09:00:35 2519724 dinov2 helpers.py:102]   [340/634]  eta: 0:20:03    time: 3.964821  data: 0.001160  max mem: 4109
I20241204 09:01:15 2519724 dinov2 helpers.py:102]   [350/634]  eta: 0:19:21    time: 3.958481  data: 0.000906  max mem: 4109
I20241204 09:01:55 2519724 dinov2 helpers.py:102]   [360/634]  eta: 0:18:39    time: 3.958345  data: 0.001102  max mem: 4109
I20241204 09:02:34 2519724 dinov2 helpers.py:102]   [370/634]  eta: 0:17:57    time: 3.960038  data: 0.001366  max mem: 4109
I20241204 09:03:14 2519724 dinov2 helpers.py:102]   [380/634]  eta: 0:17:15    time: 3.959004  data: 0.001184  max mem: 4109
I20241204 09:03:53 2519724 dinov2 helpers.py:102]   [390/634]  eta: 0:16:34    time: 3.956247  data: 0.001667  max mem: 4109
I20241204 09:04:33 2519724 dinov2 helpers.py:102]   [400/634]  eta: 0:15:52    time: 3.958068  data: 0.001804  max mem: 4109
I20241204 09:05:12 2519724 dinov2 helpers.py:102]   [410/634]  eta: 0:15:11    time: 3.957265  data: 0.001146  max mem: 4109
I20241204 09:05:52 2519724 dinov2 helpers.py:102]   [420/634]  eta: 0:14:30    time: 3.955455  data: 0.000870  max mem: 4109
I20241204 09:06:32 2519724 dinov2 helpers.py:102]   [430/634]  eta: 0:13:49    time: 3.956199  data: 0.000887  max mem: 4109
I20241204 09:07:11 2519724 dinov2 helpers.py:102]   [440/634]  eta: 0:13:08    time: 3.957106  data: 0.001434  max mem: 4109
I20241204 09:07:51 2519724 dinov2 helpers.py:102]   [450/634]  eta: 0:12:27    time: 3.958773  data: 0.001402  max mem: 4109
I20241204 09:08:30 2519724 dinov2 helpers.py:102]   [460/634]  eta: 0:11:46    time: 3.957838  data: 0.001167  max mem: 4109
I20241204 09:09:10 2519724 dinov2 helpers.py:102]   [470/634]  eta: 0:11:05    time: 3.954226  data: 0.001602  max mem: 4109
I20241204 09:09:49 2519724 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.955524  data: 0.001380  max mem: 4109
I20241204 09:10:29 2519724 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.958295  data: 0.001291  max mem: 4109
I20241204 09:11:09 2519724 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.957124  data: 0.001860  max mem: 4109
I20241204 09:11:48 2519724 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.956248  data: 0.001519  max mem: 4109
I20241204 09:12:28 2519724 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.960917  data: 0.000998  max mem: 4109
I20241204 09:13:07 2519724 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.961440  data: 0.001630  max mem: 4109
I20241204 09:13:47 2519724 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.959427  data: 0.001594  max mem: 4109
I20241204 09:14:27 2519724 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.962774  data: 0.002365  max mem: 4109
I20241204 09:15:06 2519724 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.959110  data: 0.002226  max mem: 4109
I20241204 09:15:46 2519724 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.954672  data: 0.001005  max mem: 4109
I20241204 09:16:25 2519724 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.956224  data: 0.001142  max mem: 4109
I20241204 09:17:05 2519724 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.958260  data: 0.000947  max mem: 4109
I20241204 09:17:44 2519724 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.955933  data: 0.001055  max mem: 4109
I20241204 09:18:22 2519724 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.859070  data: 0.000974  max mem: 4109
I20241204 09:18:49 2519724 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.239907  data: 0.000704  max mem: 4109
I20241204 09:19:14 2519724 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.592086  data: 0.000585  max mem: 4109
I20241204 09:19:24 2519724 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.651591  data: 0.000511  max mem: 4109
I20241204 09:19:24 2519724 dinov2 helpers.py:130]  Total time: 0:42:04 (3.981964 s / it)
I20241204 09:19:24 2519724 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:19:24 2519724 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:19:24 2519724 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:19:24 2519724 dinov2 loaders.py:151] sampler: distributed
I20241204 09:19:24 2519724 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:19:24 2519724 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:19:24,940) - Submitted job triggered an exception
E20241204 09:19:24 2519724 submitit submission.py:68] Submitted job triggered an exception
