submitit INFO (2024-12-04 08:36:13,487) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-04 08:36:13,487) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:22 2519720 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:22 2519720 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:22 2519720 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:22 2519720 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:22 2519720 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:57 2519720 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:37:02 2519720 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:37:02 2519720 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:13 2519720 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:13 2519720 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:19 2519720 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:19 2519720 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:19 2519720 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:19 2519720 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:19 2519720 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:19 2519720 dinov2 loaders.py:223] # of batches: 634
I20241204 08:38:11 2519720 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:38:11 2519720 dinov2 helpers.py:102]   [  0/634]  eta: 9:09:52    time: 52.038765  data: 11.006682  max mem: 3463
I20241204 08:38:41 2519720 dinov2 helpers.py:102]   [ 10/634]  eta: 1:17:35    time: 7.461346  data: 1.004094  max mem: 4109
I20241204 08:39:21 2519720 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:11    time: 3.471323  data: 0.002229  max mem: 4109
I20241204 08:40:00 2519720 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:17    time: 3.947066  data: 0.000633  max mem: 4109
I20241204 08:40:40 2519720 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:26    time: 3.956126  data: 0.000629  max mem: 4109
I20241204 08:41:19 2519720 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:51    time: 3.961895  data: 0.001315  max mem: 4109
I20241204 08:41:59 2519720 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:53    time: 3.964753  data: 0.002004  max mem: 4109
I20241204 08:42:39 2519720 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:18    time: 3.964670  data: 0.002138  max mem: 4109
I20241204 08:43:18 2519720 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:56    time: 3.963691  data: 0.003026  max mem: 4109
I20241204 08:43:58 2519720 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:44    time: 3.962694  data: 0.002954  max mem: 4109
I20241204 08:44:38 2519720 dinov2 helpers.py:102]   [100/634]  eta: 0:38:38    time: 3.965863  data: 0.001460  max mem: 4109
I20241204 08:45:17 2519720 dinov2 helpers.py:102]   [110/634]  eta: 0:37:37    time: 3.966642  data: 0.000936  max mem: 4109
I20241204 08:45:57 2519720 dinov2 helpers.py:102]   [120/634]  eta: 0:36:39    time: 3.961435  data: 0.001066  max mem: 4109
I20241204 08:46:36 2519720 dinov2 helpers.py:102]   [130/634]  eta: 0:35:44    time: 3.960744  data: 0.000785  max mem: 4109
I20241204 08:47:16 2519720 dinov2 helpers.py:102]   [140/634]  eta: 0:34:51    time: 3.966215  data: 0.000569  max mem: 4109
I20241204 08:47:56 2519720 dinov2 helpers.py:102]   [150/634]  eta: 0:34:00    time: 3.964264  data: 0.000862  max mem: 4109
I20241204 08:48:35 2519720 dinov2 helpers.py:102]   [160/634]  eta: 0:33:11    time: 3.965491  data: 0.000834  max mem: 4109
I20241204 08:49:15 2519720 dinov2 helpers.py:102]   [170/634]  eta: 0:32:22    time: 3.967176  data: 0.000538  max mem: 4109
I20241204 08:49:55 2519720 dinov2 helpers.py:102]   [180/634]  eta: 0:31:35    time: 3.963303  data: 0.000550  max mem: 4109
I20241204 08:50:34 2519720 dinov2 helpers.py:102]   [190/634]  eta: 0:30:48    time: 3.964669  data: 0.000579  max mem: 4109
I20241204 08:51:14 2519720 dinov2 helpers.py:102]   [200/634]  eta: 0:30:02    time: 3.965673  data: 0.000774  max mem: 4109
I20241204 08:51:54 2519720 dinov2 helpers.py:102]   [210/634]  eta: 0:29:17    time: 3.963481  data: 0.000809  max mem: 4109
I20241204 08:52:33 2519720 dinov2 helpers.py:102]   [220/634]  eta: 0:28:32    time: 3.963366  data: 0.000943  max mem: 4109
I20241204 08:53:13 2519720 dinov2 helpers.py:102]   [230/634]  eta: 0:27:48    time: 3.962656  data: 0.000955  max mem: 4109
I20241204 08:53:53 2519720 dinov2 helpers.py:102]   [240/634]  eta: 0:27:04    time: 3.963627  data: 0.000663  max mem: 4109
I20241204 08:54:32 2519720 dinov2 helpers.py:102]   [250/634]  eta: 0:26:20    time: 3.967899  data: 0.000605  max mem: 4109
I20241204 08:55:12 2519720 dinov2 helpers.py:102]   [260/634]  eta: 0:25:37    time: 3.967863  data: 0.000646  max mem: 4109
I20241204 08:55:52 2519720 dinov2 helpers.py:102]   [270/634]  eta: 0:24:54    time: 3.964363  data: 0.000911  max mem: 4109
I20241204 08:56:31 2519720 dinov2 helpers.py:102]   [280/634]  eta: 0:24:11    time: 3.963420  data: 0.000828  max mem: 4109
I20241204 08:57:11 2519720 dinov2 helpers.py:102]   [290/634]  eta: 0:23:28    time: 3.967043  data: 0.000723  max mem: 4109
I20241204 08:57:51 2519720 dinov2 helpers.py:102]   [300/634]  eta: 0:22:46    time: 3.966949  data: 0.000779  max mem: 4109
I20241204 08:58:30 2519720 dinov2 helpers.py:102]   [310/634]  eta: 0:22:04    time: 3.962263  data: 0.000719  max mem: 4109
I20241204 08:59:10 2519720 dinov2 helpers.py:102]   [320/634]  eta: 0:21:22    time: 3.962425  data: 0.000666  max mem: 4109
I20241204 08:59:49 2519720 dinov2 helpers.py:102]   [330/634]  eta: 0:20:40    time: 3.959631  data: 0.000715  max mem: 4109
I20241204 09:00:29 2519720 dinov2 helpers.py:102]   [340/634]  eta: 0:19:58    time: 3.957673  data: 0.000798  max mem: 4109
I20241204 09:01:09 2519720 dinov2 helpers.py:102]   [350/634]  eta: 0:19:16    time: 3.959462  data: 0.000710  max mem: 4109
I20241204 09:01:48 2519720 dinov2 helpers.py:102]   [360/634]  eta: 0:18:34    time: 3.960185  data: 0.000689  max mem: 4109
I20241204 09:02:28 2519720 dinov2 helpers.py:102]   [370/634]  eta: 0:17:53    time: 3.960079  data: 0.000893  max mem: 4109
I20241204 09:03:07 2519720 dinov2 helpers.py:102]   [380/634]  eta: 0:17:12    time: 3.957236  data: 0.000956  max mem: 4109
I20241204 09:03:47 2519720 dinov2 helpers.py:102]   [390/634]  eta: 0:16:30    time: 3.958904  data: 0.000817  max mem: 4109
I20241204 09:04:26 2519720 dinov2 helpers.py:102]   [400/634]  eta: 0:15:49    time: 3.959142  data: 0.001035  max mem: 4109
I20241204 09:05:06 2519720 dinov2 helpers.py:102]   [410/634]  eta: 0:15:08    time: 3.954718  data: 0.001005  max mem: 4109
I20241204 09:05:46 2519720 dinov2 helpers.py:102]   [420/634]  eta: 0:14:27    time: 3.954383  data: 0.000919  max mem: 4109
I20241204 09:06:25 2519720 dinov2 helpers.py:102]   [430/634]  eta: 0:13:46    time: 3.956319  data: 0.001145  max mem: 4109
I20241204 09:07:05 2519720 dinov2 helpers.py:102]   [440/634]  eta: 0:13:05    time: 3.956181  data: 0.000982  max mem: 4109
I20241204 09:07:44 2519720 dinov2 helpers.py:102]   [450/634]  eta: 0:12:24    time: 3.954075  data: 0.001242  max mem: 4109
I20241204 09:08:24 2519720 dinov2 helpers.py:102]   [460/634]  eta: 0:11:43    time: 3.954313  data: 0.002133  max mem: 4109
I20241204 09:09:03 2519720 dinov2 helpers.py:102]   [470/634]  eta: 0:11:03    time: 3.954191  data: 0.001731  max mem: 4109
I20241204 09:09:43 2519720 dinov2 helpers.py:102]   [480/634]  eta: 0:10:22    time: 3.954261  data: 0.001566  max mem: 4109
I20241204 09:10:22 2519720 dinov2 helpers.py:102]   [490/634]  eta: 0:09:41    time: 3.954614  data: 0.002563  max mem: 4109
I20241204 09:11:02 2519720 dinov2 helpers.py:102]   [500/634]  eta: 0:09:01    time: 3.954582  data: 0.001713  max mem: 4109
I20241204 09:11:42 2519720 dinov2 helpers.py:102]   [510/634]  eta: 0:08:20    time: 3.956338  data: 0.000676  max mem: 4109
I20241204 09:12:21 2519720 dinov2 helpers.py:102]   [520/634]  eta: 0:07:39    time: 3.955019  data: 0.000875  max mem: 4109
I20241204 09:13:01 2519720 dinov2 helpers.py:102]   [530/634]  eta: 0:06:59    time: 3.953707  data: 0.000759  max mem: 4109
I20241204 09:13:40 2519720 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.954631  data: 0.000643  max mem: 4109
I20241204 09:14:20 2519720 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.954351  data: 0.000661  max mem: 4109
I20241204 09:14:59 2519720 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.954651  data: 0.000735  max mem: 4109
I20241204 09:15:39 2519720 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.954577  data: 0.000841  max mem: 4109
I20241204 09:16:18 2519720 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.955802  data: 0.000866  max mem: 4109
I20241204 09:16:58 2519720 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.955651  data: 0.000948  max mem: 4109
I20241204 09:17:37 2519720 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.955447  data: 0.001925  max mem: 4109
I20241204 09:18:17 2519720 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.934494  data: 0.002325  max mem: 4109
I20241204 09:18:45 2519720 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.366409  data: 0.002220  max mem: 4109
I20241204 09:19:10 2519720 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.646377  data: 0.001610  max mem: 4109
I20241204 09:19:21 2519720 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.777339  data: 0.000498  max mem: 4109
I20241204 09:19:22 2519720 dinov2 helpers.py:130]  Total time: 0:42:02 (3.978678 s / it)
I20241204 09:19:22 2519720 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:19:22 2519720 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:19:22 2519720 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:19:22 2519720 dinov2 loaders.py:151] sampler: distributed
I20241204 09:19:22 2519720 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:19:22 2519720 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:19:22,411) - Submitted job triggered an exception
E20241204 09:19:22 2519720 submitit submission.py:68] Submitted job triggered an exception
