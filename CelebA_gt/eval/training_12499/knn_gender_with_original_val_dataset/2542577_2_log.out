submitit INFO (2024-12-04 09:27:28,401) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=2(8), node=0(1), global_rank=2(8))
submitit INFO (2024-12-04 09:27:28,402) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:36 2542580 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:36 2542580 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:36 2542580 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:36 2542580 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:36 2542580 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:12 2542580 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:17 2542580 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:17 2542580 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:32 2542580 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:32 2542580 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:37 2542580 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:37 2542580 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:37 2542580 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:37 2542580 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:37 2542580 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:37 2542580 dinov2 loaders.py:223] # of batches: 634
I20241204 09:29:27 2542580 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:29:27 2542580 dinov2 helpers.py:102]   [  0/634]  eta: 8:52:39    time: 50.409206  data: 10.497227  max mem: 3464
I20241204 09:29:59 2542580 dinov2 helpers.py:102]   [ 10/634]  eta: 1:17:56    time: 7.494479  data: 0.954772  max mem: 4109
I20241204 09:30:39 2542580 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:26    time: 3.579020  data: 0.000577  max mem: 4109
I20241204 09:31:19 2542580 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:30    time: 3.963911  data: 0.000760  max mem: 4109
I20241204 09:31:58 2542580 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:39    time: 3.974757  data: 0.000862  max mem: 4109
I20241204 09:32:38 2542580 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:02    time: 3.976905  data: 0.000847  max mem: 4109
I20241204 09:33:18 2542580 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:05    time: 3.982545  data: 0.001204  max mem: 4109
I20241204 09:33:58 2542580 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:30    time: 3.991944  data: 0.001179  max mem: 4109
I20241204 09:34:38 2542580 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:08    time: 3.992026  data: 0.000822  max mem: 4109
I20241204 09:35:18 2542580 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:56    time: 3.989964  data: 0.001651  max mem: 4109
I20241204 09:35:58 2542580 dinov2 helpers.py:102]   [100/634]  eta: 0:38:50    time: 3.991069  data: 0.002467  max mem: 4109
I20241204 09:36:38 2542580 dinov2 helpers.py:102]   [110/634]  eta: 0:37:49    time: 3.991280  data: 0.002294  max mem: 4109
I20241204 09:37:18 2542580 dinov2 helpers.py:102]   [120/634]  eta: 0:36:51    time: 3.991227  data: 0.001956  max mem: 4109
I20241204 09:37:58 2542580 dinov2 helpers.py:102]   [130/634]  eta: 0:35:56    time: 3.991971  data: 0.001719  max mem: 4109
I20241204 09:38:37 2542580 dinov2 helpers.py:102]   [140/634]  eta: 0:35:03    time: 3.992374  data: 0.001253  max mem: 4109
I20241204 09:39:17 2542580 dinov2 helpers.py:102]   [150/634]  eta: 0:34:12    time: 3.993483  data: 0.000886  max mem: 4109
I20241204 09:39:57 2542580 dinov2 helpers.py:102]   [160/634]  eta: 0:33:22    time: 3.991879  data: 0.000861  max mem: 4109
I20241204 09:40:37 2542580 dinov2 helpers.py:102]   [170/634]  eta: 0:32:34    time: 3.992657  data: 0.001148  max mem: 4109
I20241204 09:41:17 2542580 dinov2 helpers.py:102]   [180/634]  eta: 0:31:46    time: 3.994965  data: 0.001168  max mem: 4109
I20241204 09:41:57 2542580 dinov2 helpers.py:102]   [190/634]  eta: 0:31:00    time: 3.993215  data: 0.000802  max mem: 4109
I20241204 09:42:37 2542580 dinov2 helpers.py:102]   [200/634]  eta: 0:30:13    time: 3.993109  data: 0.000660  max mem: 4109
I20241204 09:43:17 2542580 dinov2 helpers.py:102]   [210/634]  eta: 0:29:28    time: 3.991337  data: 0.001563  max mem: 4109
I20241204 09:43:57 2542580 dinov2 helpers.py:102]   [220/634]  eta: 0:28:43    time: 3.991253  data: 0.001636  max mem: 4109
I20241204 09:44:37 2542580 dinov2 helpers.py:102]   [230/634]  eta: 0:27:58    time: 3.994997  data: 0.001706  max mem: 4109
I20241204 09:45:17 2542580 dinov2 helpers.py:102]   [240/634]  eta: 0:27:14    time: 3.994983  data: 0.003774  max mem: 4109
I20241204 09:45:57 2542580 dinov2 helpers.py:102]   [250/634]  eta: 0:26:30    time: 3.992147  data: 0.002956  max mem: 4109
I20241204 09:46:37 2542580 dinov2 helpers.py:102]   [260/634]  eta: 0:25:47    time: 3.993146  data: 0.000905  max mem: 4109
I20241204 09:47:17 2542580 dinov2 helpers.py:102]   [270/634]  eta: 0:25:03    time: 3.994999  data: 0.001633  max mem: 4109
I20241204 09:47:57 2542580 dinov2 helpers.py:102]   [280/634]  eta: 0:24:20    time: 3.993348  data: 0.002138  max mem: 4109
I20241204 09:48:36 2542580 dinov2 helpers.py:102]   [290/634]  eta: 0:23:37    time: 3.992265  data: 0.001202  max mem: 4109
I20241204 09:49:16 2542580 dinov2 helpers.py:102]   [300/634]  eta: 0:22:55    time: 3.992956  data: 0.000680  max mem: 4109
I20241204 09:49:56 2542580 dinov2 helpers.py:102]   [310/634]  eta: 0:22:12    time: 3.992361  data: 0.001044  max mem: 4109
I20241204 09:50:36 2542580 dinov2 helpers.py:102]   [320/634]  eta: 0:21:30    time: 3.991302  data: 0.001037  max mem: 4109
I20241204 09:51:16 2542580 dinov2 helpers.py:102]   [330/634]  eta: 0:20:48    time: 3.991912  data: 0.000725  max mem: 4109
I20241204 09:51:56 2542580 dinov2 helpers.py:102]   [340/634]  eta: 0:20:06    time: 3.994795  data: 0.001220  max mem: 4109
I20241204 09:52:36 2542580 dinov2 helpers.py:102]   [350/634]  eta: 0:19:24    time: 3.992109  data: 0.001347  max mem: 4109
I20241204 09:53:16 2542580 dinov2 helpers.py:102]   [360/634]  eta: 0:18:42    time: 3.991128  data: 0.001051  max mem: 4109
I20241204 09:53:56 2542580 dinov2 helpers.py:102]   [370/634]  eta: 0:18:00    time: 3.993948  data: 0.000989  max mem: 4109
I20241204 09:54:36 2542580 dinov2 helpers.py:102]   [380/634]  eta: 0:17:19    time: 3.993181  data: 0.001124  max mem: 4109
I20241204 09:55:16 2542580 dinov2 helpers.py:102]   [390/634]  eta: 0:16:37    time: 3.993028  data: 0.001896  max mem: 4109
I20241204 09:55:56 2542580 dinov2 helpers.py:102]   [400/634]  eta: 0:15:56    time: 3.993265  data: 0.003094  max mem: 4109
I20241204 09:56:36 2542580 dinov2 helpers.py:102]   [410/634]  eta: 0:15:14    time: 3.992413  data: 0.002357  max mem: 4109
I20241204 09:57:16 2542580 dinov2 helpers.py:102]   [420/634]  eta: 0:14:33    time: 3.991487  data: 0.000906  max mem: 4109
I20241204 09:57:55 2542580 dinov2 helpers.py:102]   [430/634]  eta: 0:13:52    time: 3.992912  data: 0.001110  max mem: 4109
I20241204 09:58:35 2542580 dinov2 helpers.py:102]   [440/634]  eta: 0:13:11    time: 3.994427  data: 0.001206  max mem: 4109
I20241204 09:59:15 2542580 dinov2 helpers.py:102]   [450/634]  eta: 0:12:29    time: 3.993123  data: 0.001232  max mem: 4109
I20241204 09:59:55 2542580 dinov2 helpers.py:102]   [460/634]  eta: 0:11:48    time: 3.992511  data: 0.001502  max mem: 4109
I20241204 10:00:35 2542580 dinov2 helpers.py:102]   [470/634]  eta: 0:11:07    time: 3.992396  data: 0.001321  max mem: 4109
I20241204 10:01:15 2542580 dinov2 helpers.py:102]   [480/634]  eta: 0:10:26    time: 3.994787  data: 0.001143  max mem: 4109
I20241204 10:01:55 2542580 dinov2 helpers.py:102]   [490/634]  eta: 0:09:45    time: 3.994779  data: 0.001043  max mem: 4109
I20241204 10:02:35 2542580 dinov2 helpers.py:102]   [500/634]  eta: 0:09:05    time: 3.992028  data: 0.001023  max mem: 4109
I20241204 10:03:15 2542580 dinov2 helpers.py:102]   [510/634]  eta: 0:08:24    time: 3.993940  data: 0.002321  max mem: 4109
I20241204 10:03:55 2542580 dinov2 helpers.py:102]   [520/634]  eta: 0:07:43    time: 3.995992  data: 0.002078  max mem: 4109
I20241204 10:04:35 2542580 dinov2 helpers.py:102]   [530/634]  eta: 0:07:02    time: 3.997657  data: 0.000880  max mem: 4109
I20241204 10:05:15 2542580 dinov2 helpers.py:102]   [540/634]  eta: 0:06:21    time: 3.995529  data: 0.002475  max mem: 4109
I20241204 10:05:55 2542580 dinov2 helpers.py:102]   [550/634]  eta: 0:05:41    time: 3.991938  data: 0.002306  max mem: 4109
I20241204 10:06:35 2542580 dinov2 helpers.py:102]   [560/634]  eta: 0:05:00    time: 3.986283  data: 0.000773  max mem: 4109
I20241204 10:07:14 2542580 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.979743  data: 0.001078  max mem: 4109
I20241204 10:07:54 2542580 dinov2 helpers.py:102]   [580/634]  eta: 0:03:39    time: 3.982575  data: 0.001011  max mem: 4109
I20241204 10:08:34 2542580 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.987141  data: 0.000955  max mem: 4109
I20241204 10:09:14 2542580 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.981633  data: 0.000946  max mem: 4109
I20241204 10:09:52 2542580 dinov2 helpers.py:102]   [610/634]  eta: 0:01:37    time: 3.918026  data: 0.000595  max mem: 4109
I20241204 10:10:30 2542580 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.827867  data: 0.000574  max mem: 4109
I20241204 10:11:10 2542580 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.867142  data: 0.000597  max mem: 4109
I20241204 10:11:26 2542580 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.094205  data: 0.000530  max mem: 4109
I20241204 10:11:26 2542580 dinov2 helpers.py:130]  Total time: 0:42:49 (4.052512 s / it)
I20241204 10:11:26 2542580 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:11:26 2542580 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:11:26 2542580 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:11:26 2542580 dinov2 loaders.py:151] sampler: distributed
I20241204 10:11:26 2542580 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:11:26 2542580 dinov2 loaders.py:223] # of batches: 78
I20241204 10:11:27 2542580 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:11:43 2542580 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:19:40    time: 15.133628  data: 11.179066  max mem: 4109
I20241204 10:12:23 2542580 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:41    time: 5.026421  data: 1.022264  max mem: 4109
I20241204 10:13:03 2542580 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:23    time: 4.009419  data: 0.004803  max mem: 4109
I20241204 10:13:43 2542580 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:29    time: 3.998914  data: 0.005273  max mem: 4109
I20241204 10:14:23 2542580 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:42    time: 4.002196  data: 0.005299  max mem: 4109
I20241204 10:14:59 2542580 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:55    time: 3.797821  data: 0.003809  max mem: 4109
I20241204 10:15:27 2542580 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 3.214265  data: 0.007778  max mem: 4109
I20241204 10:15:52 2542580 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:29    time: 2.669774  data: 0.006864  max mem: 4109
I20241204 10:16:01 2542580 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.071018  data: 0.003723  max mem: 4109
I20241204 10:16:01 2542580 dinov2 helpers.py:130] Test: Total time: 0:04:33 (3.508012 s / it)
I20241204 10:16:01 2542580 dinov2 utils.py:79] Averaged stats: 
I20241204 10:16:01 2542580 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.11
I20241204 10:16:01 2542580 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.04
I20241204 10:16:01 2542580 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:16:01 2542580 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.93
submitit INFO (2024-12-04 10:16:01,937) - Job completed successfully
I20241204 10:16:01 2542580 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:16:01,938) - Exiting after successful completion
I20241204 10:16:01 2542580 submitit submission.py:61] Exiting after successful completion
