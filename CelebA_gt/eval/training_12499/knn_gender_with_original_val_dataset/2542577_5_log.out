submitit INFO (2024-12-04 09:27:28,356) - Starting with JobEnvironment(job_id=2542577, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-04 09:27:28,357) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2542577_submitted.pkl
I20241204 09:27:36 2542583 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 09:27:36 2542583 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 09:27:36 2542583 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 09:27:36 2542583 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 09:27:36 2542583 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 09:28:06 2542583 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 09:28:10 2542583 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 09:28:10 2542583 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 09:28:16 2542583 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 09:28:16 2542583 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 09:28:18 2542583 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 09:28:18 2542583 dinov2 knn.py:260] Extracting features for train set...
I20241204 09:28:18 2542583 dinov2 loaders.py:151] sampler: distributed
I20241204 09:28:18 2542583 dinov2 loaders.py:210] using PyTorch data loader
W20241204 09:28:18 2542583 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 09:28:18 2542583 dinov2 loaders.py:223] # of batches: 634
I20241204 09:28:49 2542583 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 09:28:49 2542583 dinov2 helpers.py:102]   [  0/634]  eta: 5:26:54    time: 30.937246  data: 12.085441  max mem: 3463
I20241204 09:28:54 2542583 dinov2 helpers.py:102]   [ 10/634]  eta: 0:33:20    time: 3.206051  data: 1.101923  max mem: 4109
I20241204 09:29:05 2542583 dinov2 helpers.py:102]   [ 20/634]  eta: 0:22:52    time: 0.799965  data: 0.230902  max mem: 4109
I20241204 09:29:20 2542583 dinov2 helpers.py:102]   [ 30/634]  eta: 0:20:06    time: 1.333365  data: 0.229320  max mem: 4109
I20241204 09:29:54 2542583 dinov2 helpers.py:102]   [ 40/634]  eta: 0:23:08    time: 2.444503  data: 0.000831  max mem: 4109
I20241204 09:30:34 2542583 dinov2 helpers.py:102]   [ 50/634]  eta: 0:25:49    time: 3.668434  data: 0.001595  max mem: 4109
I20241204 09:31:13 2542583 dinov2 helpers.py:102]   [ 60/634]  eta: 0:27:25    time: 3.954778  data: 0.001685  max mem: 4109
I20241204 09:31:53 2542583 dinov2 helpers.py:102]   [ 70/634]  eta: 0:28:25    time: 3.967384  data: 0.001237  max mem: 4109
I20241204 09:32:33 2542583 dinov2 helpers.py:102]   [ 80/634]  eta: 0:28:59    time: 3.974099  data: 0.000964  max mem: 4109
I20241204 09:33:12 2542583 dinov2 helpers.py:102]   [ 90/634]  eta: 0:29:18    time: 3.974520  data: 0.000948  max mem: 4109
I20241204 09:33:52 2542583 dinov2 helpers.py:102]   [100/634]  eta: 0:29:25    time: 3.974000  data: 0.001016  max mem: 4109
I20241204 09:34:32 2542583 dinov2 helpers.py:102]   [110/634]  eta: 0:29:24    time: 3.977572  data: 0.000864  max mem: 4109
I20241204 09:35:12 2542583 dinov2 helpers.py:102]   [120/634]  eta: 0:29:16    time: 3.981916  data: 0.000917  max mem: 4109
I20241204 09:35:52 2542583 dinov2 helpers.py:102]   [130/634]  eta: 0:29:03    time: 3.978565  data: 0.001098  max mem: 4109
I20241204 09:36:31 2542583 dinov2 helpers.py:102]   [140/634]  eta: 0:28:47    time: 3.975969  data: 0.001172  max mem: 4109
I20241204 09:37:11 2542583 dinov2 helpers.py:102]   [150/634]  eta: 0:28:27    time: 3.976804  data: 0.001098  max mem: 4109
I20241204 09:37:51 2542583 dinov2 helpers.py:102]   [160/634]  eta: 0:28:05    time: 3.976751  data: 0.000953  max mem: 4109
I20241204 09:38:31 2542583 dinov2 helpers.py:102]   [170/634]  eta: 0:27:41    time: 3.979522  data: 0.001342  max mem: 4109
I20241204 09:39:11 2542583 dinov2 helpers.py:102]   [180/634]  eta: 0:27:15    time: 3.979581  data: 0.001277  max mem: 4109
I20241204 09:39:50 2542583 dinov2 helpers.py:102]   [190/634]  eta: 0:26:48    time: 3.978845  data: 0.000887  max mem: 4109
I20241204 09:40:30 2542583 dinov2 helpers.py:102]   [200/634]  eta: 0:26:20    time: 3.979687  data: 0.001026  max mem: 4109
I20241204 09:41:10 2542583 dinov2 helpers.py:102]   [210/634]  eta: 0:25:50    time: 3.979561  data: 0.001378  max mem: 4109
I20241204 09:41:50 2542583 dinov2 helpers.py:102]   [220/634]  eta: 0:25:20    time: 3.983114  data: 0.001149  max mem: 4109
I20241204 09:42:30 2542583 dinov2 helpers.py:102]   [230/634]  eta: 0:24:48    time: 3.984893  data: 0.000663  max mem: 4109
I20241204 09:43:09 2542583 dinov2 helpers.py:102]   [240/634]  eta: 0:24:16    time: 3.985043  data: 0.000671  max mem: 4109
I20241204 09:43:49 2542583 dinov2 helpers.py:102]   [250/634]  eta: 0:23:44    time: 3.985945  data: 0.000949  max mem: 4109
I20241204 09:44:29 2542583 dinov2 helpers.py:102]   [260/634]  eta: 0:23:11    time: 3.981444  data: 0.001042  max mem: 4109
I20241204 09:45:09 2542583 dinov2 helpers.py:102]   [270/634]  eta: 0:22:37    time: 3.984195  data: 0.000952  max mem: 4109
I20241204 09:45:49 2542583 dinov2 helpers.py:102]   [280/634]  eta: 0:22:03    time: 3.991334  data: 0.001237  max mem: 4109
I20241204 09:46:29 2542583 dinov2 helpers.py:102]   [290/634]  eta: 0:21:29    time: 3.989571  data: 0.001278  max mem: 4109
I20241204 09:47:09 2542583 dinov2 helpers.py:102]   [300/634]  eta: 0:20:54    time: 3.985265  data: 0.001609  max mem: 4109
I20241204 09:47:48 2542583 dinov2 helpers.py:102]   [310/634]  eta: 0:20:19    time: 3.982395  data: 0.001655  max mem: 4109
I20241204 09:48:28 2542583 dinov2 helpers.py:102]   [320/634]  eta: 0:19:43    time: 3.983949  data: 0.000916  max mem: 4109
I20241204 09:49:08 2542583 dinov2 helpers.py:102]   [330/634]  eta: 0:19:07    time: 3.986768  data: 0.000720  max mem: 4109
I20241204 09:49:48 2542583 dinov2 helpers.py:102]   [340/634]  eta: 0:18:31    time: 3.987876  data: 0.000966  max mem: 4109
I20241204 09:50:28 2542583 dinov2 helpers.py:102]   [350/634]  eta: 0:17:55    time: 3.988661  data: 0.000950  max mem: 4109
I20241204 09:51:08 2542583 dinov2 helpers.py:102]   [360/634]  eta: 0:17:19    time: 3.987518  data: 0.000979  max mem: 4109
I20241204 09:51:48 2542583 dinov2 helpers.py:102]   [370/634]  eta: 0:16:42    time: 3.984948  data: 0.000885  max mem: 4109
I20241204 09:52:28 2542583 dinov2 helpers.py:102]   [380/634]  eta: 0:16:06    time: 3.985759  data: 0.001773  max mem: 4109
I20241204 09:53:07 2542583 dinov2 helpers.py:102]   [390/634]  eta: 0:15:29    time: 3.984849  data: 0.001868  max mem: 4109
I20241204 09:53:47 2542583 dinov2 helpers.py:102]   [400/634]  eta: 0:14:52    time: 3.982224  data: 0.000645  max mem: 4109
I20241204 09:54:27 2542583 dinov2 helpers.py:102]   [410/634]  eta: 0:14:14    time: 3.985719  data: 0.000906  max mem: 4109
I20241204 09:55:07 2542583 dinov2 helpers.py:102]   [420/634]  eta: 0:13:37    time: 3.986784  data: 0.001230  max mem: 4109
I20241204 09:55:47 2542583 dinov2 helpers.py:102]   [430/634]  eta: 0:13:00    time: 3.987041  data: 0.001018  max mem: 4109
I20241204 09:56:27 2542583 dinov2 helpers.py:102]   [440/634]  eta: 0:12:22    time: 3.989721  data: 0.000985  max mem: 4109
I20241204 09:57:07 2542583 dinov2 helpers.py:102]   [450/634]  eta: 0:11:45    time: 3.991469  data: 0.001034  max mem: 4109
I20241204 09:57:47 2542583 dinov2 helpers.py:102]   [460/634]  eta: 0:11:07    time: 3.991790  data: 0.001474  max mem: 4109
I20241204 09:58:26 2542583 dinov2 helpers.py:102]   [470/634]  eta: 0:10:29    time: 3.990182  data: 0.001425  max mem: 4109
I20241204 09:59:06 2542583 dinov2 helpers.py:102]   [480/634]  eta: 0:09:51    time: 3.987970  data: 0.000701  max mem: 4109
I20241204 09:59:46 2542583 dinov2 helpers.py:102]   [490/634]  eta: 0:09:13    time: 3.985923  data: 0.000828  max mem: 4109
I20241204 10:00:26 2542583 dinov2 helpers.py:102]   [500/634]  eta: 0:08:35    time: 3.987812  data: 0.000909  max mem: 4109
I20241204 10:01:06 2542583 dinov2 helpers.py:102]   [510/634]  eta: 0:07:57    time: 3.987802  data: 0.000646  max mem: 4109
I20241204 10:01:46 2542583 dinov2 helpers.py:102]   [520/634]  eta: 0:07:19    time: 3.986005  data: 0.000543  max mem: 4109
I20241204 10:02:26 2542583 dinov2 helpers.py:102]   [530/634]  eta: 0:06:40    time: 3.984884  data: 0.000611  max mem: 4109
I20241204 10:03:05 2542583 dinov2 helpers.py:102]   [540/634]  eta: 0:06:02    time: 3.983047  data: 0.001053  max mem: 4109
I20241204 10:03:45 2542583 dinov2 helpers.py:102]   [550/634]  eta: 0:05:24    time: 3.986886  data: 0.001165  max mem: 4109
I20241204 10:04:25 2542583 dinov2 helpers.py:102]   [560/634]  eta: 0:04:45    time: 3.988141  data: 0.000884  max mem: 4109
I20241204 10:05:05 2542583 dinov2 helpers.py:102]   [570/634]  eta: 0:04:07    time: 3.981637  data: 0.000768  max mem: 4109
I20241204 10:05:45 2542583 dinov2 helpers.py:102]   [580/634]  eta: 0:03:28    time: 3.977692  data: 0.000927  max mem: 4109
I20241204 10:06:25 2542583 dinov2 helpers.py:102]   [590/634]  eta: 0:02:50    time: 3.975617  data: 0.000961  max mem: 4109
I20241204 10:07:04 2542583 dinov2 helpers.py:102]   [600/634]  eta: 0:02:11    time: 3.973338  data: 0.000934  max mem: 4109
I20241204 10:07:44 2542583 dinov2 helpers.py:102]   [610/634]  eta: 0:01:32    time: 3.973492  data: 0.001121  max mem: 4109
I20241204 10:08:24 2542583 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.973627  data: 0.001639  max mem: 4109
I20241204 10:09:03 2542583 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.975324  data: 0.001825  max mem: 4109
I20241204 10:09:23 2542583 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.355420  data: 0.001462  max mem: 4109
I20241204 10:09:23 2542583 dinov2 helpers.py:130]  Total time: 0:41:05 (3.888169 s / it)
I20241204 10:09:23 2542583 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 10:09:23 2542583 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 10:09:24 2542583 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 10:09:24 2542583 dinov2 loaders.py:151] sampler: distributed
I20241204 10:09:24 2542583 dinov2 loaders.py:210] using PyTorch data loader
I20241204 10:09:24 2542583 dinov2 loaders.py:223] # of batches: 78
I20241204 10:09:24 2542583 dinov2 knn.py:299] Start the k-NN classification.
I20241204 10:09:34 2542583 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:12:14    time: 9.412954  data: 5.179598  max mem: 4109
I20241204 10:10:15 2542583 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:08    time: 4.535920  data: 0.472612  max mem: 4109
I20241204 10:10:55 2542583 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:08    time: 4.022791  data: 0.002247  max mem: 4109
I20241204 10:11:30 2542583 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:13    time: 3.740088  data: 0.004730  max mem: 4109
I20241204 10:12:07 2542583 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:30    time: 3.611634  data: 0.007248  max mem: 4109
I20241204 10:12:47 2542583 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:50    time: 3.871425  data: 0.004981  max mem: 4109
I20241204 10:13:27 2542583 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 4.012664  data: 0.004328  max mem: 4109
I20241204 10:14:08 2542583 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 4.025664  data: 0.004005  max mem: 4109
I20241204 10:14:34 2542583 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.920480  data: 0.001218  max mem: 4109
I20241204 10:14:34 2542583 dinov2 helpers.py:130] Test: Total time: 0:05:08 (3.957365 s / it)
I20241204 10:14:34 2542583 dinov2 utils.py:79] Averaged stats: 
I20241204 10:14:35 2542583 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 71.15
I20241204 10:14:35 2542583 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 72.03
I20241204 10:14:35 2542583 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 73.03
I20241204 10:14:35 2542583 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 72.92
submitit INFO (2024-12-04 10:14:36,107) - Job completed successfully
I20241204 10:14:36 2542583 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 10:14:36,126) - Exiting after successful completion
I20241204 10:14:36 2542583 submitit submission.py:61] Exiting after successful completion
