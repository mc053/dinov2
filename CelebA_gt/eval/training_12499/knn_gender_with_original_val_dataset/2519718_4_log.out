submitit INFO (2024-12-04 08:36:13,483) - Starting with JobEnvironment(job_id=2519718, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-04 08:36:13,483) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender/2519718_submitted.pkl
I20241204 08:36:21 2519723 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 08:36:21 2519723 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_12499/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 08:36:21 2519723 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 08:36:21 2519723 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_12499/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 08:36:21 2519723 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 08:36:51 2519723 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 08:36:55 2519723 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_12499/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 08:36:55 2519723 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 08:37:02 2519723 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 08:37:02 2519723 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 08:37:04 2519723 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 08:37:04 2519723 dinov2 knn.py:260] Extracting features for train set...
I20241204 08:37:04 2519723 dinov2 loaders.py:151] sampler: distributed
I20241204 08:37:04 2519723 dinov2 loaders.py:210] using PyTorch data loader
W20241204 08:37:04 2519723 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 08:37:04 2519723 dinov2 loaders.py:223] # of batches: 634
I20241204 08:37:36 2519723 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 08:37:36 2519723 dinov2 helpers.py:102]   [  0/634]  eta: 5:40:01    time: 32.179100  data: 9.397881  max mem: 3463
I20241204 08:37:46 2519723 dinov2 helpers.py:102]   [ 10/634]  eta: 0:39:05    time: 3.759194  data: 0.856293  max mem: 4109
I20241204 08:38:01 2519723 dinov2 helpers.py:102]   [ 20/634]  eta: 0:27:27    time: 1.207697  data: 0.002160  max mem: 4109
I20241204 08:38:23 2519723 dinov2 helpers.py:102]   [ 30/634]  eta: 0:25:26    time: 1.849202  data: 0.001712  max mem: 4109
I20241204 08:39:02 2519723 dinov2 helpers.py:102]   [ 40/634]  eta: 0:28:24    time: 3.065244  data: 0.001773  max mem: 4109
I20241204 08:39:41 2519723 dinov2 helpers.py:102]   [ 50/634]  eta: 0:29:59    time: 3.942727  data: 0.001879  max mem: 4109
I20241204 08:40:21 2519723 dinov2 helpers.py:102]   [ 60/634]  eta: 0:30:51    time: 3.958453  data: 0.001235  max mem: 4109
I20241204 08:41:01 2519723 dinov2 helpers.py:102]   [ 70/634]  eta: 0:31:18    time: 3.966768  data: 0.000829  max mem: 4109
I20241204 08:41:40 2519723 dinov2 helpers.py:102]   [ 80/634]  eta: 0:31:29    time: 3.971964  data: 0.000720  max mem: 4109
I20241204 08:42:20 2519723 dinov2 helpers.py:102]   [ 90/634]  eta: 0:31:28    time: 3.969294  data: 0.000946  max mem: 4109
I20241204 08:43:00 2519723 dinov2 helpers.py:102]   [100/634]  eta: 0:31:19    time: 3.965567  data: 0.000970  max mem: 4109
I20241204 08:43:39 2519723 dinov2 helpers.py:102]   [110/634]  eta: 0:31:05    time: 3.964481  data: 0.000894  max mem: 4109
I20241204 08:44:19 2519723 dinov2 helpers.py:102]   [120/634]  eta: 0:30:47    time: 3.965367  data: 0.001419  max mem: 4109
I20241204 08:44:59 2519723 dinov2 helpers.py:102]   [130/634]  eta: 0:30:25    time: 3.966527  data: 0.001496  max mem: 4109
I20241204 08:45:38 2519723 dinov2 helpers.py:102]   [140/634]  eta: 0:30:01    time: 3.966439  data: 0.000934  max mem: 4109
I20241204 08:46:18 2519723 dinov2 helpers.py:102]   [150/634]  eta: 0:29:35    time: 3.964451  data: 0.001973  max mem: 4109
I20241204 08:46:58 2519723 dinov2 helpers.py:102]   [160/634]  eta: 0:29:07    time: 3.962503  data: 0.001986  max mem: 4109
I20241204 08:47:37 2519723 dinov2 helpers.py:102]   [170/634]  eta: 0:28:37    time: 3.965281  data: 0.001072  max mem: 4109
I20241204 08:48:17 2519723 dinov2 helpers.py:102]   [180/634]  eta: 0:28:07    time: 3.963543  data: 0.001320  max mem: 4109
I20241204 08:48:57 2519723 dinov2 helpers.py:102]   [190/634]  eta: 0:27:36    time: 3.962643  data: 0.001061  max mem: 4109
I20241204 08:49:36 2519723 dinov2 helpers.py:102]   [200/634]  eta: 0:27:03    time: 3.966225  data: 0.001253  max mem: 4109
I20241204 08:50:16 2519723 dinov2 helpers.py:102]   [210/634]  eta: 0:26:30    time: 3.965536  data: 0.001404  max mem: 4109
I20241204 08:50:56 2519723 dinov2 helpers.py:102]   [220/634]  eta: 0:25:57    time: 3.963854  data: 0.000985  max mem: 4109
I20241204 08:51:35 2519723 dinov2 helpers.py:102]   [230/634]  eta: 0:25:23    time: 3.961695  data: 0.000743  max mem: 4109
I20241204 08:52:15 2519723 dinov2 helpers.py:102]   [240/634]  eta: 0:24:48    time: 3.961484  data: 0.000748  max mem: 4109
I20241204 08:52:54 2519723 dinov2 helpers.py:102]   [250/634]  eta: 0:24:13    time: 3.962629  data: 0.000765  max mem: 4109
I20241204 08:53:34 2519723 dinov2 helpers.py:102]   [260/634]  eta: 0:23:38    time: 3.966249  data: 0.001001  max mem: 4109
I20241204 08:54:14 2519723 dinov2 helpers.py:102]   [270/634]  eta: 0:23:02    time: 3.966115  data: 0.001205  max mem: 4109
I20241204 08:54:53 2519723 dinov2 helpers.py:102]   [280/634]  eta: 0:22:26    time: 3.962748  data: 0.001067  max mem: 4109
I20241204 08:55:33 2519723 dinov2 helpers.py:102]   [290/634]  eta: 0:21:50    time: 3.962955  data: 0.000896  max mem: 4109
I20241204 08:56:13 2519723 dinov2 helpers.py:102]   [300/634]  eta: 0:21:14    time: 3.961546  data: 0.000907  max mem: 4109
I20241204 08:56:52 2519723 dinov2 helpers.py:102]   [310/634]  eta: 0:20:37    time: 3.962170  data: 0.001020  max mem: 4109
I20241204 08:57:32 2519723 dinov2 helpers.py:102]   [320/634]  eta: 0:20:00    time: 3.964342  data: 0.000888  max mem: 4109
I20241204 08:58:12 2519723 dinov2 helpers.py:102]   [330/634]  eta: 0:19:23    time: 3.966813  data: 0.001027  max mem: 4109
I20241204 08:58:51 2519723 dinov2 helpers.py:102]   [340/634]  eta: 0:18:46    time: 3.963276  data: 0.000978  max mem: 4109
I20241204 08:59:31 2519723 dinov2 helpers.py:102]   [350/634]  eta: 0:18:09    time: 3.959726  data: 0.000700  max mem: 4109
I20241204 09:00:10 2519723 dinov2 helpers.py:102]   [360/634]  eta: 0:17:32    time: 3.961351  data: 0.000671  max mem: 4109
I20241204 09:00:50 2519723 dinov2 helpers.py:102]   [370/634]  eta: 0:16:54    time: 3.962172  data: 0.000843  max mem: 4109
I20241204 09:01:30 2519723 dinov2 helpers.py:102]   [380/634]  eta: 0:16:16    time: 3.961318  data: 0.000948  max mem: 4109
I20241204 09:02:09 2519723 dinov2 helpers.py:102]   [390/634]  eta: 0:15:39    time: 3.957494  data: 0.000898  max mem: 4109
I20241204 09:02:49 2519723 dinov2 helpers.py:102]   [400/634]  eta: 0:15:01    time: 3.960101  data: 0.000921  max mem: 4109
I20241204 09:03:28 2519723 dinov2 helpers.py:102]   [410/634]  eta: 0:14:23    time: 3.959733  data: 0.001320  max mem: 4109
I20241204 09:04:08 2519723 dinov2 helpers.py:102]   [420/634]  eta: 0:13:45    time: 3.955934  data: 0.001455  max mem: 4109
I20241204 09:04:48 2519723 dinov2 helpers.py:102]   [430/634]  eta: 0:13:07    time: 3.956303  data: 0.001143  max mem: 4109
I20241204 09:05:27 2519723 dinov2 helpers.py:102]   [440/634]  eta: 0:12:29    time: 3.959021  data: 0.000799  max mem: 4109
I20241204 09:06:07 2519723 dinov2 helpers.py:102]   [450/634]  eta: 0:11:50    time: 3.958166  data: 0.000619  max mem: 4109
I20241204 09:06:46 2519723 dinov2 helpers.py:102]   [460/634]  eta: 0:11:12    time: 3.956279  data: 0.000771  max mem: 4109
I20241204 09:07:26 2519723 dinov2 helpers.py:102]   [470/634]  eta: 0:10:34    time: 3.957059  data: 0.000692  max mem: 4109
I20241204 09:08:05 2519723 dinov2 helpers.py:102]   [480/634]  eta: 0:09:55    time: 3.953429  data: 0.002074  max mem: 4109
I20241204 09:08:45 2519723 dinov2 helpers.py:102]   [490/634]  eta: 0:09:17    time: 3.952436  data: 0.002191  max mem: 4109
I20241204 09:09:24 2519723 dinov2 helpers.py:102]   [500/634]  eta: 0:08:38    time: 3.953289  data: 0.000817  max mem: 4109
I20241204 09:10:04 2519723 dinov2 helpers.py:102]   [510/634]  eta: 0:08:00    time: 3.955301  data: 0.000954  max mem: 4109
I20241204 09:10:44 2519723 dinov2 helpers.py:102]   [520/634]  eta: 0:07:21    time: 3.957407  data: 0.001107  max mem: 4109
I20241204 09:11:23 2519723 dinov2 helpers.py:102]   [530/634]  eta: 0:06:43    time: 3.956447  data: 0.000884  max mem: 4109
I20241204 09:12:03 2519723 dinov2 helpers.py:102]   [540/634]  eta: 0:06:04    time: 3.954351  data: 0.000866  max mem: 4109
I20241204 09:12:42 2519723 dinov2 helpers.py:102]   [550/634]  eta: 0:05:25    time: 3.954938  data: 0.000824  max mem: 4109
I20241204 09:13:22 2519723 dinov2 helpers.py:102]   [560/634]  eta: 0:04:47    time: 3.955212  data: 0.001670  max mem: 4109
I20241204 09:14:01 2519723 dinov2 helpers.py:102]   [570/634]  eta: 0:04:08    time: 3.952979  data: 0.001952  max mem: 4109
I20241204 09:14:41 2519723 dinov2 helpers.py:102]   [580/634]  eta: 0:03:29    time: 3.954684  data: 0.001023  max mem: 4109
I20241204 09:15:20 2519723 dinov2 helpers.py:102]   [590/634]  eta: 0:02:50    time: 3.955621  data: 0.000910  max mem: 4109
I20241204 09:16:00 2519723 dinov2 helpers.py:102]   [600/634]  eta: 0:02:12    time: 3.960035  data: 0.001063  max mem: 4109
I20241204 09:16:40 2519723 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.959115  data: 0.001328  max mem: 4109
I20241204 09:17:19 2519723 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.955681  data: 0.001331  max mem: 4109
I20241204 09:17:59 2519723 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.955411  data: 0.001489  max mem: 4109
I20241204 09:18:18 2519723 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.304642  data: 0.001085  max mem: 4109
I20241204 09:18:18 2519723 dinov2 helpers.py:130]  Total time: 0:41:13 (3.901627 s / it)
I20241204 09:18:18 2519723 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 09:18:18 2519723 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 09:18:19 2519723 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 09:18:19 2519723 dinov2 loaders.py:151] sampler: distributed
I20241204 09:18:19 2519723 dinov2 loaders.py:210] using PyTorch data loader
I20241204 09:18:19 2519723 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-04 09:18:19,138) - Submitted job triggered an exception
E20241204 09:18:19 2519723 submitit submission.py:68] Submitted job triggered an exception
