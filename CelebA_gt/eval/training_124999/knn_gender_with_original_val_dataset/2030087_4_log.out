submitit INFO (2024-12-03 10:27:42,198) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-03 10:27:42,198) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:49 2030092 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:49 2030092 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:49 2030092 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:49 2030092 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:49 2030092 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:23 2030092 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:27 2030092 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:27 2030092 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:38 2030092 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:38 2030092 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:44 2030092 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:44 2030092 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:44 2030092 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:44 2030092 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:44 2030092 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:44 2030092 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:32 2030092 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:32 2030092 dinov2 helpers.py:102]   [  0/634]  eta: 8:28:16    time: 48.101757  data: 10.681955  max mem: 3463
I20241203 10:29:59 2030092 dinov2 helpers.py:102]   [ 10/634]  eta: 1:11:21    time: 6.861908  data: 0.971795  max mem: 4109
I20241203 10:30:38 2030092 dinov2 helpers.py:102]   [ 20/634]  eta: 0:55:51    time: 3.325622  data: 0.000843  max mem: 4109
I20241203 10:31:18 2030092 dinov2 helpers.py:102]   [ 30/634]  eta: 0:50:02    time: 3.930031  data: 0.001219  max mem: 4109
I20241203 10:31:57 2030092 dinov2 helpers.py:102]   [ 40/634]  eta: 0:46:45    time: 3.951143  data: 0.001303  max mem: 4109
I20241203 10:32:37 2030092 dinov2 helpers.py:102]   [ 50/634]  eta: 0:44:31    time: 3.959656  data: 0.000936  max mem: 4109
I20241203 10:33:17 2030092 dinov2 helpers.py:102]   [ 60/634]  eta: 0:42:48    time: 3.968449  data: 0.000855  max mem: 4109
I20241203 10:33:57 2030092 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:24    time: 3.973287  data: 0.001142  max mem: 4109
I20241203 10:34:36 2030092 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:10    time: 3.973409  data: 0.001300  max mem: 4109
I20241203 10:35:16 2030092 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:04    time: 3.973449  data: 0.000872  max mem: 4109
I20241203 10:35:56 2030092 dinov2 helpers.py:102]   [100/634]  eta: 0:38:03    time: 3.973356  data: 0.000579  max mem: 4109
I20241203 10:36:35 2030092 dinov2 helpers.py:102]   [110/634]  eta: 0:37:06    time: 3.973282  data: 0.000685  max mem: 4109
I20241203 10:37:15 2030092 dinov2 helpers.py:102]   [120/634]  eta: 0:36:12    time: 3.973328  data: 0.000772  max mem: 4109
I20241203 10:37:55 2030092 dinov2 helpers.py:102]   [130/634]  eta: 0:35:20    time: 3.973322  data: 0.001493  max mem: 4109
I20241203 10:38:35 2030092 dinov2 helpers.py:102]   [140/634]  eta: 0:34:30    time: 3.973120  data: 0.001629  max mem: 4109
I20241203 10:39:14 2030092 dinov2 helpers.py:102]   [150/634]  eta: 0:33:41    time: 3.969141  data: 0.000850  max mem: 4109
I20241203 10:39:54 2030092 dinov2 helpers.py:102]   [160/634]  eta: 0:32:53    time: 3.969128  data: 0.000562  max mem: 4109
I20241203 10:40:34 2030092 dinov2 helpers.py:102]   [170/634]  eta: 0:32:06    time: 3.972953  data: 0.000724  max mem: 4109
I20241203 10:41:13 2030092 dinov2 helpers.py:102]   [180/634]  eta: 0:31:20    time: 3.971379  data: 0.000792  max mem: 4109
I20241203 10:41:53 2030092 dinov2 helpers.py:102]   [190/634]  eta: 0:30:34    time: 3.968466  data: 0.000868  max mem: 4109
I20241203 10:42:33 2030092 dinov2 helpers.py:102]   [200/634]  eta: 0:29:50    time: 3.969976  data: 0.001025  max mem: 4109
I20241203 10:43:13 2030092 dinov2 helpers.py:102]   [210/634]  eta: 0:29:05    time: 3.971639  data: 0.001628  max mem: 4109
I20241203 10:43:52 2030092 dinov2 helpers.py:102]   [220/634]  eta: 0:28:21    time: 3.970753  data: 0.001756  max mem: 4109
I20241203 10:44:32 2030092 dinov2 helpers.py:102]   [230/634]  eta: 0:27:38    time: 3.972028  data: 0.000939  max mem: 4109
I20241203 10:45:12 2030092 dinov2 helpers.py:102]   [240/634]  eta: 0:26:55    time: 3.973370  data: 0.001070  max mem: 4109
I20241203 10:45:52 2030092 dinov2 helpers.py:102]   [250/634]  eta: 0:26:12    time: 3.975732  data: 0.001161  max mem: 4109
I20241203 10:46:31 2030092 dinov2 helpers.py:102]   [260/634]  eta: 0:25:29    time: 3.976054  data: 0.000669  max mem: 4109
I20241203 10:47:11 2030092 dinov2 helpers.py:102]   [270/634]  eta: 0:24:47    time: 3.977460  data: 0.000749  max mem: 4109
I20241203 10:47:51 2030092 dinov2 helpers.py:102]   [280/634]  eta: 0:24:05    time: 3.981709  data: 0.000895  max mem: 4109
I20241203 10:48:31 2030092 dinov2 helpers.py:102]   [290/634]  eta: 0:23:23    time: 3.981340  data: 0.001901  max mem: 4109
I20241203 10:49:10 2030092 dinov2 helpers.py:102]   [300/634]  eta: 0:22:41    time: 3.977247  data: 0.001915  max mem: 4109
I20241203 10:49:50 2030092 dinov2 helpers.py:102]   [310/634]  eta: 0:21:59    time: 3.978931  data: 0.001245  max mem: 4109
I20241203 10:50:30 2030092 dinov2 helpers.py:102]   [320/634]  eta: 0:21:17    time: 3.983307  data: 0.001327  max mem: 4109
I20241203 10:51:10 2030092 dinov2 helpers.py:102]   [330/634]  eta: 0:20:36    time: 3.978877  data: 0.000976  max mem: 4109
I20241203 10:51:50 2030092 dinov2 helpers.py:102]   [340/634]  eta: 0:19:54    time: 3.977035  data: 0.000901  max mem: 4109
I20241203 10:52:30 2030092 dinov2 helpers.py:102]   [350/634]  eta: 0:19:13    time: 3.984356  data: 0.000842  max mem: 4109
I20241203 10:53:09 2030092 dinov2 helpers.py:102]   [360/634]  eta: 0:18:32    time: 3.983696  data: 0.001000  max mem: 4109
I20241203 10:53:49 2030092 dinov2 helpers.py:102]   [370/634]  eta: 0:17:51    time: 3.984528  data: 0.000848  max mem: 4109
I20241203 10:54:29 2030092 dinov2 helpers.py:102]   [380/634]  eta: 0:17:10    time: 3.986190  data: 0.001095  max mem: 4109
I20241203 10:55:09 2030092 dinov2 helpers.py:102]   [390/634]  eta: 0:16:29    time: 3.987067  data: 0.001153  max mem: 4109
I20241203 10:55:49 2030092 dinov2 helpers.py:102]   [400/634]  eta: 0:15:48    time: 3.989075  data: 0.000721  max mem: 4109
I20241203 10:56:29 2030092 dinov2 helpers.py:102]   [410/634]  eta: 0:15:07    time: 3.981863  data: 0.001034  max mem: 4109
I20241203 10:57:09 2030092 dinov2 helpers.py:102]   [420/634]  eta: 0:14:26    time: 3.982517  data: 0.002248  max mem: 4109
I20241203 10:57:48 2030092 dinov2 helpers.py:102]   [430/634]  eta: 0:13:45    time: 3.985223  data: 0.002498  max mem: 4109
I20241203 10:58:28 2030092 dinov2 helpers.py:102]   [440/634]  eta: 0:13:04    time: 3.982559  data: 0.001276  max mem: 4109
I20241203 10:59:08 2030092 dinov2 helpers.py:102]   [450/634]  eta: 0:12:24    time: 3.982629  data: 0.001073  max mem: 4109
I20241203 10:59:48 2030092 dinov2 helpers.py:102]   [460/634]  eta: 0:11:43    time: 3.983566  data: 0.001450  max mem: 4109
I20241203 11:00:28 2030092 dinov2 helpers.py:102]   [470/634]  eta: 0:11:02    time: 3.986298  data: 0.001309  max mem: 4109
I20241203 11:01:08 2030092 dinov2 helpers.py:102]   [480/634]  eta: 0:10:22    time: 3.986207  data: 0.001281  max mem: 4109
I20241203 11:01:47 2030092 dinov2 helpers.py:102]   [490/634]  eta: 0:09:41    time: 3.981700  data: 0.001007  max mem: 4109
I20241203 11:02:27 2030092 dinov2 helpers.py:102]   [500/634]  eta: 0:09:01    time: 3.981705  data: 0.000841  max mem: 4109
I20241203 11:03:07 2030092 dinov2 helpers.py:102]   [510/634]  eta: 0:08:20    time: 3.987225  data: 0.001161  max mem: 4109
I20241203 11:03:47 2030092 dinov2 helpers.py:102]   [520/634]  eta: 0:07:40    time: 3.985415  data: 0.001071  max mem: 4109
I20241203 11:04:27 2030092 dinov2 helpers.py:102]   [530/634]  eta: 0:06:59    time: 3.978898  data: 0.001325  max mem: 4109
I20241203 11:05:07 2030092 dinov2 helpers.py:102]   [540/634]  eta: 0:06:19    time: 3.981728  data: 0.001525  max mem: 4109
I20241203 11:05:46 2030092 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.982711  data: 0.001256  max mem: 4109
I20241203 11:06:26 2030092 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.980905  data: 0.001040  max mem: 4109
I20241203 11:07:06 2030092 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.984465  data: 0.001994  max mem: 4109
I20241203 11:07:46 2030092 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.986230  data: 0.002661  max mem: 4109
I20241203 11:08:26 2030092 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.987910  data: 0.002287  max mem: 4109
I20241203 11:09:06 2030092 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.983412  data: 0.001606  max mem: 4109
I20241203 11:09:45 2030092 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.981736  data: 0.000973  max mem: 4109
I20241203 11:10:19 2030092 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.651650  data: 0.000968  max mem: 4109
I20241203 11:10:47 2030092 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.080657  data: 0.000755  max mem: 4109
I20241203 11:10:57 2030092 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 3.038879  data: 0.000583  max mem: 4109
I20241203 11:10:58 2030092 dinov2 helpers.py:130]  Total time: 0:42:13 (3.996638 s / it)
I20241203 11:10:58 2030092 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:10:58 2030092 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:10:58 2030092 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:10:58 2030092 dinov2 loaders.py:151] sampler: distributed
I20241203 11:10:58 2030092 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:10:58 2030092 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:10:58,590) - Submitted job triggered an exception
E20241203 11:10:58 2030092 submitit submission.py:68] Submitted job triggered an exception
