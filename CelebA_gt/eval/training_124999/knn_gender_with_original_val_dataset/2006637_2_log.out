submitit INFO (2024-12-03 08:52:42,204) - Starting with JobEnvironment(job_id=2006637, hostname=tars, local_rank=2(8), node=0(1), global_rank=2(8))
submitit INFO (2024-12-03 08:52:42,204) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2006637_submitted.pkl
I20241203 08:52:50 2006640 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 08:52:50 2006640 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 08:52:50 2006640 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 08:52:50 2006640 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 08:52:50 2006640 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 08:53:24 2006640 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 08:53:29 2006640 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 08:53:29 2006640 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 08:53:40 2006640 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 08:53:40 2006640 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 08:53:46 2006640 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 08:53:46 2006640 dinov2 knn.py:260] Extracting features for train set...
I20241203 08:53:46 2006640 dinov2 loaders.py:151] sampler: distributed
I20241203 08:53:46 2006640 dinov2 loaders.py:210] using PyTorch data loader
W20241203 08:53:46 2006640 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 08:53:46 2006640 dinov2 loaders.py:223] # of batches: 634
I20241203 08:54:31 2006640 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 08:54:31 2006640 dinov2 helpers.py:102]   [  0/634]  eta: 7:52:59    time: 44.763302  data: 12.147860  max mem: 3463
I20241203 08:55:02 2006640 dinov2 helpers.py:102]   [ 10/634]  eta: 1:11:32    time: 6.878449  data: 1.105604  max mem: 4109
I20241203 08:55:41 2006640 dinov2 helpers.py:102]   [ 20/634]  eta: 0:56:06    time: 3.519510  data: 0.001923  max mem: 4109
I20241203 08:56:21 2006640 dinov2 helpers.py:102]   [ 30/634]  eta: 0:50:15    time: 3.954242  data: 0.001690  max mem: 4109
I20241203 08:57:00 2006640 dinov2 helpers.py:102]   [ 40/634]  eta: 0:46:55    time: 3.958767  data: 0.001441  max mem: 4109
I20241203 08:57:40 2006640 dinov2 helpers.py:102]   [ 50/634]  eta: 0:44:39    time: 3.964629  data: 0.001668  max mem: 4109
I20241203 08:58:20 2006640 dinov2 helpers.py:102]   [ 60/634]  eta: 0:42:56    time: 3.972303  data: 0.001000  max mem: 4109
I20241203 08:59:00 2006640 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:30    time: 3.973554  data: 0.000829  max mem: 4109
I20241203 08:59:39 2006640 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:16    time: 3.973644  data: 0.000916  max mem: 4109
I20241203 09:00:19 2006640 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:09    time: 3.972538  data: 0.000702  max mem: 4109
I20241203 09:00:59 2006640 dinov2 helpers.py:102]   [100/634]  eta: 0:38:07    time: 3.971358  data: 0.000843  max mem: 4109
I20241203 09:01:39 2006640 dinov2 helpers.py:102]   [110/634]  eta: 0:37:09    time: 3.970416  data: 0.000971  max mem: 4109
I20241203 09:02:18 2006640 dinov2 helpers.py:102]   [120/634]  eta: 0:36:15    time: 3.971340  data: 0.000764  max mem: 4109
I20241203 09:02:58 2006640 dinov2 helpers.py:102]   [130/634]  eta: 0:35:22    time: 3.971125  data: 0.000609  max mem: 4109
I20241203 09:03:38 2006640 dinov2 helpers.py:102]   [140/634]  eta: 0:34:32    time: 3.969186  data: 0.000536  max mem: 4109
I20241203 09:04:17 2006640 dinov2 helpers.py:102]   [150/634]  eta: 0:33:43    time: 3.969455  data: 0.000814  max mem: 4109
I20241203 09:04:57 2006640 dinov2 helpers.py:102]   [160/634]  eta: 0:32:55    time: 3.969820  data: 0.001027  max mem: 4109
I20241203 09:05:37 2006640 dinov2 helpers.py:102]   [170/634]  eta: 0:32:08    time: 3.969889  data: 0.000990  max mem: 4109
I20241203 09:06:16 2006640 dinov2 helpers.py:102]   [180/634]  eta: 0:31:21    time: 3.969929  data: 0.000918  max mem: 4109
I20241203 09:06:56 2006640 dinov2 helpers.py:102]   [190/634]  eta: 0:30:36    time: 3.971795  data: 0.000743  max mem: 4109
I20241203 09:07:36 2006640 dinov2 helpers.py:102]   [200/634]  eta: 0:29:51    time: 3.971612  data: 0.000914  max mem: 4109
I20241203 09:08:16 2006640 dinov2 helpers.py:102]   [210/634]  eta: 0:29:07    time: 3.969776  data: 0.000970  max mem: 4109
I20241203 09:08:55 2006640 dinov2 helpers.py:102]   [220/634]  eta: 0:28:23    time: 3.970508  data: 0.000863  max mem: 4109
I20241203 09:09:35 2006640 dinov2 helpers.py:102]   [230/634]  eta: 0:27:39    time: 3.971291  data: 0.002263  max mem: 4109
I20241203 09:10:15 2006640 dinov2 helpers.py:102]   [240/634]  eta: 0:26:56    time: 3.972640  data: 0.002561  max mem: 4109
I20241203 09:10:54 2006640 dinov2 helpers.py:102]   [250/634]  eta: 0:26:13    time: 3.973571  data: 0.001179  max mem: 4109
I20241203 09:11:34 2006640 dinov2 helpers.py:102]   [260/634]  eta: 0:25:30    time: 3.974570  data: 0.000796  max mem: 4109
I20241203 09:12:14 2006640 dinov2 helpers.py:102]   [270/634]  eta: 0:24:47    time: 3.974910  data: 0.000989  max mem: 4109
I20241203 09:12:54 2006640 dinov2 helpers.py:102]   [280/634]  eta: 0:24:05    time: 3.974103  data: 0.001328  max mem: 4109
I20241203 09:13:33 2006640 dinov2 helpers.py:102]   [290/634]  eta: 0:23:23    time: 3.974257  data: 0.001382  max mem: 4109
I20241203 09:14:13 2006640 dinov2 helpers.py:102]   [300/634]  eta: 0:22:41    time: 3.974293  data: 0.001740  max mem: 4109
I20241203 09:14:53 2006640 dinov2 helpers.py:102]   [310/634]  eta: 0:21:59    time: 3.974332  data: 0.001294  max mem: 4109
I20241203 09:15:33 2006640 dinov2 helpers.py:102]   [320/634]  eta: 0:21:18    time: 3.974168  data: 0.000507  max mem: 4109
I20241203 09:16:12 2006640 dinov2 helpers.py:102]   [330/634]  eta: 0:20:36    time: 3.974137  data: 0.000761  max mem: 4109
I20241203 09:16:52 2006640 dinov2 helpers.py:102]   [340/634]  eta: 0:19:54    time: 3.973933  data: 0.001125  max mem: 4109
I20241203 09:17:32 2006640 dinov2 helpers.py:102]   [350/634]  eta: 0:19:13    time: 3.973832  data: 0.000919  max mem: 4109
I20241203 09:18:12 2006640 dinov2 helpers.py:102]   [360/634]  eta: 0:18:32    time: 3.975490  data: 0.001117  max mem: 4109
I20241203 09:18:51 2006640 dinov2 helpers.py:102]   [370/634]  eta: 0:17:51    time: 3.975091  data: 0.001745  max mem: 4109
I20241203 09:19:31 2006640 dinov2 helpers.py:102]   [380/634]  eta: 0:17:09    time: 3.973294  data: 0.001406  max mem: 4109
I20241203 09:20:11 2006640 dinov2 helpers.py:102]   [390/634]  eta: 0:16:28    time: 3.973514  data: 0.001015  max mem: 4109
I20241203 09:20:51 2006640 dinov2 helpers.py:102]   [400/634]  eta: 0:15:47    time: 3.975437  data: 0.000883  max mem: 4109
I20241203 09:21:30 2006640 dinov2 helpers.py:102]   [410/634]  eta: 0:15:07    time: 3.975708  data: 0.000774  max mem: 4109
I20241203 09:22:10 2006640 dinov2 helpers.py:102]   [420/634]  eta: 0:14:26    time: 3.973881  data: 0.000626  max mem: 4109
I20241203 09:22:50 2006640 dinov2 helpers.py:102]   [430/634]  eta: 0:13:45    time: 3.973722  data: 0.000580  max mem: 4109
I20241203 09:23:30 2006640 dinov2 helpers.py:102]   [440/634]  eta: 0:13:04    time: 3.973975  data: 0.000672  max mem: 4109
I20241203 09:24:09 2006640 dinov2 helpers.py:102]   [450/634]  eta: 0:12:23    time: 3.976127  data: 0.001639  max mem: 4109
I20241203 09:24:49 2006640 dinov2 helpers.py:102]   [460/634]  eta: 0:11:43    time: 3.975918  data: 0.001596  max mem: 4109
I20241203 09:25:29 2006640 dinov2 helpers.py:102]   [470/634]  eta: 0:11:02    time: 3.973774  data: 0.000553  max mem: 4109
I20241203 09:26:09 2006640 dinov2 helpers.py:102]   [480/634]  eta: 0:10:21    time: 3.974007  data: 0.001034  max mem: 4109
I20241203 09:26:48 2006640 dinov2 helpers.py:102]   [490/634]  eta: 0:09:41    time: 3.974908  data: 0.001459  max mem: 4109
I20241203 09:27:28 2006640 dinov2 helpers.py:102]   [500/634]  eta: 0:09:00    time: 3.975784  data: 0.001159  max mem: 4109
I20241203 09:28:08 2006640 dinov2 helpers.py:102]   [510/634]  eta: 0:08:20    time: 3.974887  data: 0.002166  max mem: 4109
I20241203 09:28:48 2006640 dinov2 helpers.py:102]   [520/634]  eta: 0:07:39    time: 3.974081  data: 0.002136  max mem: 4109
I20241203 09:29:27 2006640 dinov2 helpers.py:102]   [530/634]  eta: 0:06:59    time: 3.973935  data: 0.000760  max mem: 4109
I20241203 09:30:07 2006640 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.973958  data: 0.001385  max mem: 4109
I20241203 09:30:47 2006640 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.974146  data: 0.001360  max mem: 4109
I20241203 09:31:27 2006640 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.974147  data: 0.000697  max mem: 4109
I20241203 09:32:06 2006640 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.973672  data: 0.000818  max mem: 4109
I20241203 09:32:46 2006640 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.973375  data: 0.000930  max mem: 4109
I20241203 09:33:26 2006640 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.972494  data: 0.001078  max mem: 4109
I20241203 09:34:05 2006640 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.969481  data: 0.001029  max mem: 4109
I20241203 09:34:45 2006640 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.969294  data: 0.002339  max mem: 4109
I20241203 09:35:23 2006640 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.869240  data: 0.002172  max mem: 4109
I20241203 09:35:49 2006640 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.211351  data: 0.000570  max mem: 4109
I20241203 09:36:01 2006640 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 3.209309  data: 0.000507  max mem: 4109
I20241203 09:36:01 2006640 dinov2 helpers.py:130]  Total time: 0:42:15 (3.998833 s / it)
I20241203 09:36:01 2006640 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 09:36:01 2006640 dinov2 utils.py:142] Labels shape: (162127,)
submitit ERROR (2024-12-03 09:36:02,382) - Submitted job triggered an exception
E20241203 09:36:02 2006640 submitit submission.py:68] Submitted job triggered an exception
