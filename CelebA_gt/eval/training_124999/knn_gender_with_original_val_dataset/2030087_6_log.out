submitit INFO (2024-12-03 10:27:42,218) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-03 10:27:42,218) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:49 2030094 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:49 2030094 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:49 2030094 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:49 2030094 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:49 2030094 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:19 2030094 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:24 2030094 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:25 2030094 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:32 2030094 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:32 2030094 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:36 2030094 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:36 2030094 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:36 2030094 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:36 2030094 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:36 2030094 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:36 2030094 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:24 2030094 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:24 2030094 dinov2 helpers.py:102]   [  0/634]  eta: 8:22:16    time: 47.534664  data: 13.198504  max mem: 3463
I20241203 10:29:44 2030094 dinov2 helpers.py:102]   [ 10/634]  eta: 1:03:52    time: 6.142469  data: 1.200824  max mem: 4109
I20241203 10:30:23 2030094 dinov2 helpers.py:102]   [ 20/634]  eta: 0:51:54    time: 2.949310  data: 0.000842  max mem: 4109
I20241203 10:31:02 2030094 dinov2 helpers.py:102]   [ 30/634]  eta: 0:47:22    time: 3.916000  data: 0.001704  max mem: 4109
I20241203 10:31:42 2030094 dinov2 helpers.py:102]   [ 40/634]  eta: 0:44:46    time: 3.944939  data: 0.001927  max mem: 4109
I20241203 10:32:21 2030094 dinov2 helpers.py:102]   [ 50/634]  eta: 0:42:55    time: 3.953175  data: 0.000848  max mem: 4109
I20241203 10:33:01 2030094 dinov2 helpers.py:102]   [ 60/634]  eta: 0:41:29    time: 3.957197  data: 0.000931  max mem: 4109
I20241203 10:33:41 2030094 dinov2 helpers.py:102]   [ 70/634]  eta: 0:40:17    time: 3.967331  data: 0.001151  max mem: 4109
I20241203 10:34:20 2030094 dinov2 helpers.py:102]   [ 80/634]  eta: 0:39:13    time: 3.973428  data: 0.000837  max mem: 4109
I20241203 10:35:00 2030094 dinov2 helpers.py:102]   [ 90/634]  eta: 0:38:14    time: 3.973565  data: 0.000587  max mem: 4109
I20241203 10:35:40 2030094 dinov2 helpers.py:102]   [100/634]  eta: 0:37:19    time: 3.973435  data: 0.000701  max mem: 4109
I20241203 10:36:20 2030094 dinov2 helpers.py:102]   [110/634]  eta: 0:36:26    time: 3.973277  data: 0.001163  max mem: 4109
I20241203 10:36:59 2030094 dinov2 helpers.py:102]   [120/634]  eta: 0:35:36    time: 3.973316  data: 0.001396  max mem: 4109
I20241203 10:37:39 2030094 dinov2 helpers.py:102]   [130/634]  eta: 0:34:47    time: 3.973281  data: 0.001329  max mem: 4109
I20241203 10:38:19 2030094 dinov2 helpers.py:102]   [140/634]  eta: 0:34:00    time: 3.973227  data: 0.001937  max mem: 4109
I20241203 10:38:59 2030094 dinov2 helpers.py:102]   [150/634]  eta: 0:33:14    time: 3.972890  data: 0.001642  max mem: 4109
I20241203 10:39:38 2030094 dinov2 helpers.py:102]   [160/634]  eta: 0:32:28    time: 3.972839  data: 0.000681  max mem: 4109
I20241203 10:40:18 2030094 dinov2 helpers.py:102]   [170/634]  eta: 0:31:43    time: 3.972974  data: 0.000781  max mem: 4109
I20241203 10:40:58 2030094 dinov2 helpers.py:102]   [180/634]  eta: 0:30:59    time: 3.972097  data: 0.000954  max mem: 4109
I20241203 10:41:37 2030094 dinov2 helpers.py:102]   [190/634]  eta: 0:30:15    time: 3.971203  data: 0.001606  max mem: 4109
I20241203 10:42:17 2030094 dinov2 helpers.py:102]   [200/634]  eta: 0:29:32    time: 3.971851  data: 0.001765  max mem: 4109
I20241203 10:42:57 2030094 dinov2 helpers.py:102]   [210/634]  eta: 0:28:49    time: 3.971586  data: 0.001257  max mem: 4109
I20241203 10:43:37 2030094 dinov2 helpers.py:102]   [220/634]  eta: 0:28:06    time: 3.971566  data: 0.001134  max mem: 4109
I20241203 10:44:16 2030094 dinov2 helpers.py:102]   [230/634]  eta: 0:27:23    time: 3.972814  data: 0.001284  max mem: 4109
I20241203 10:44:56 2030094 dinov2 helpers.py:102]   [240/634]  eta: 0:26:41    time: 3.973146  data: 0.001229  max mem: 4109
I20241203 10:45:36 2030094 dinov2 helpers.py:102]   [250/634]  eta: 0:25:59    time: 3.974496  data: 0.001015  max mem: 4109
I20241203 10:46:16 2030094 dinov2 helpers.py:102]   [260/634]  eta: 0:25:17    time: 3.976028  data: 0.001096  max mem: 4109
I20241203 10:46:55 2030094 dinov2 helpers.py:102]   [270/634]  eta: 0:24:36    time: 3.979014  data: 0.001681  max mem: 4109
I20241203 10:47:35 2030094 dinov2 helpers.py:102]   [280/634]  eta: 0:23:54    time: 3.981688  data: 0.001962  max mem: 4109
I20241203 10:48:15 2030094 dinov2 helpers.py:102]   [290/634]  eta: 0:23:13    time: 3.978933  data: 0.001340  max mem: 4109
I20241203 10:48:55 2030094 dinov2 helpers.py:102]   [300/634]  eta: 0:22:31    time: 3.979796  data: 0.001542  max mem: 4109
I20241203 10:49:35 2030094 dinov2 helpers.py:102]   [310/634]  eta: 0:21:50    time: 3.981662  data: 0.001590  max mem: 4109
I20241203 10:50:14 2030094 dinov2 helpers.py:102]   [320/634]  eta: 0:21:09    time: 3.979903  data: 0.001596  max mem: 4109
I20241203 10:50:54 2030094 dinov2 helpers.py:102]   [330/634]  eta: 0:20:28    time: 3.981613  data: 0.002274  max mem: 4109
I20241203 10:51:34 2030094 dinov2 helpers.py:102]   [340/634]  eta: 0:19:47    time: 3.984229  data: 0.002081  max mem: 4109
I20241203 10:52:14 2030094 dinov2 helpers.py:102]   [350/634]  eta: 0:19:06    time: 3.986082  data: 0.001414  max mem: 4109
I20241203 10:52:54 2030094 dinov2 helpers.py:102]   [360/634]  eta: 0:18:26    time: 3.986394  data: 0.001250  max mem: 4109
I20241203 10:53:34 2030094 dinov2 helpers.py:102]   [370/634]  eta: 0:17:45    time: 3.986364  data: 0.001539  max mem: 4109
I20241203 10:54:14 2030094 dinov2 helpers.py:102]   [380/634]  eta: 0:17:04    time: 3.987015  data: 0.001194  max mem: 4109
I20241203 10:54:53 2030094 dinov2 helpers.py:102]   [390/634]  eta: 0:16:24    time: 3.982616  data: 0.000981  max mem: 4109
I20241203 10:55:33 2030094 dinov2 helpers.py:102]   [400/634]  eta: 0:15:43    time: 3.979146  data: 0.001091  max mem: 4109
I20241203 10:56:13 2030094 dinov2 helpers.py:102]   [410/634]  eta: 0:15:02    time: 3.981776  data: 0.000915  max mem: 4109
I20241203 10:56:53 2030094 dinov2 helpers.py:102]   [420/634]  eta: 0:14:22    time: 3.987113  data: 0.001163  max mem: 4109
I20241203 10:57:33 2030094 dinov2 helpers.py:102]   [430/634]  eta: 0:13:41    time: 3.991578  data: 0.001081  max mem: 4109
I20241203 10:58:13 2030094 dinov2 helpers.py:102]   [440/634]  eta: 0:13:01    time: 3.986109  data: 0.000807  max mem: 4109
I20241203 10:58:52 2030094 dinov2 helpers.py:102]   [450/634]  eta: 0:12:20    time: 3.982560  data: 0.000819  max mem: 4109
I20241203 10:59:32 2030094 dinov2 helpers.py:102]   [460/634]  eta: 0:11:40    time: 3.984388  data: 0.000679  max mem: 4109
I20241203 11:00:12 2030094 dinov2 helpers.py:102]   [470/634]  eta: 0:11:00    time: 3.984453  data: 0.001072  max mem: 4109
I20241203 11:00:52 2030094 dinov2 helpers.py:102]   [480/634]  eta: 0:10:19    time: 3.983477  data: 0.001595  max mem: 4109
I20241203 11:01:32 2030094 dinov2 helpers.py:102]   [490/634]  eta: 0:09:39    time: 3.983389  data: 0.001307  max mem: 4109
I20241203 11:02:12 2030094 dinov2 helpers.py:102]   [500/634]  eta: 0:08:59    time: 3.987186  data: 0.001011  max mem: 4109
I20241203 11:02:52 2030094 dinov2 helpers.py:102]   [510/634]  eta: 0:08:18    time: 3.990894  data: 0.000904  max mem: 4109
I20241203 11:03:32 2030094 dinov2 helpers.py:102]   [520/634]  eta: 0:07:38    time: 3.991675  data: 0.001131  max mem: 4109
I20241203 11:04:11 2030094 dinov2 helpers.py:102]   [530/634]  eta: 0:06:58    time: 3.986109  data: 0.001272  max mem: 4109
I20241203 11:04:51 2030094 dinov2 helpers.py:102]   [540/634]  eta: 0:06:17    time: 3.978111  data: 0.000908  max mem: 4109
I20241203 11:05:31 2030094 dinov2 helpers.py:102]   [550/634]  eta: 0:05:37    time: 3.977328  data: 0.000702  max mem: 4109
I20241203 11:06:11 2030094 dinov2 helpers.py:102]   [560/634]  eta: 0:04:57    time: 3.986230  data: 0.001749  max mem: 4109
I20241203 11:06:51 2030094 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.989941  data: 0.001766  max mem: 4109
I20241203 11:07:31 2030094 dinov2 helpers.py:102]   [580/634]  eta: 0:03:36    time: 3.988111  data: 0.000722  max mem: 4109
I20241203 11:08:11 2030094 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.989646  data: 0.001054  max mem: 4109
I20241203 11:08:50 2030094 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.986985  data: 0.001315  max mem: 4109
I20241203 11:09:30 2030094 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.986293  data: 0.001150  max mem: 4109
I20241203 11:10:07 2030094 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.844339  data: 0.001002  max mem: 4109
I20241203 11:10:37 2030094 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.344798  data: 0.000809  max mem: 4109
I20241203 11:10:50 2030094 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 3.376452  data: 0.000666  max mem: 4109
I20241203 11:10:50 2030094 dinov2 helpers.py:130]  Total time: 0:42:13 (3.996204 s / it)
I20241203 11:10:50 2030094 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:10:50 2030094 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:10:51 2030094 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:10:51 2030094 dinov2 loaders.py:151] sampler: distributed
I20241203 11:10:51 2030094 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:10:51 2030094 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:10:51,058) - Submitted job triggered an exception
E20241203 11:10:51 2030094 submitit submission.py:68] Submitted job triggered an exception
