submitit INFO (2024-12-03 10:27:42,206) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-03 10:27:42,207) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:50 2030095 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:50 2030095 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:50 2030095 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:50 2030095 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:50 2030095 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:20 2030095 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:25 2030095 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:25 2030095 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:31 2030095 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:31 2030095 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:32 2030095 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:32 2030095 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:32 2030095 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:32 2030095 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:32 2030095 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:32 2030095 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:05 2030095 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:05 2030095 dinov2 helpers.py:102]   [  0/634]  eta: 5:49:07    time: 33.040894  data: 8.822555  max mem: 3463
I20241203 10:29:14 2030095 dinov2 helpers.py:102]   [ 10/634]  eta: 0:38:56    time: 3.743688  data: 0.803844  max mem: 4109
I20241203 10:29:28 2030095 dinov2 helpers.py:102]   [ 20/634]  eta: 0:27:02    time: 1.123423  data: 0.001579  max mem: 4109
I20241203 10:29:59 2030095 dinov2 helpers.py:102]   [ 30/634]  eta: 0:28:00    time: 2.252255  data: 0.000866  max mem: 4109
I20241203 10:30:38 2030095 dinov2 helpers.py:102]   [ 40/634]  eta: 0:30:16    time: 3.493344  data: 0.000593  max mem: 4109
I20241203 10:31:17 2030095 dinov2 helpers.py:102]   [ 50/634]  eta: 0:31:27    time: 3.929957  data: 0.000858  max mem: 4109
I20241203 10:31:57 2030095 dinov2 helpers.py:102]   [ 60/634]  eta: 0:32:03    time: 3.950548  data: 0.000869  max mem: 4109
I20241203 10:32:36 2030095 dinov2 helpers.py:102]   [ 70/634]  eta: 0:32:17    time: 3.954335  data: 0.000692  max mem: 4109
I20241203 10:33:16 2030095 dinov2 helpers.py:102]   [ 80/634]  eta: 0:32:19    time: 3.962682  data: 0.000687  max mem: 4109
I20241203 10:33:56 2030095 dinov2 helpers.py:102]   [ 90/634]  eta: 0:32:13    time: 3.973176  data: 0.000757  max mem: 4109
I20241203 10:34:35 2030095 dinov2 helpers.py:102]   [100/634]  eta: 0:31:59    time: 3.973477  data: 0.000822  max mem: 4109
I20241203 10:35:15 2030095 dinov2 helpers.py:102]   [110/634]  eta: 0:31:41    time: 3.973562  data: 0.000717  max mem: 4109
I20241203 10:35:55 2030095 dinov2 helpers.py:102]   [120/634]  eta: 0:31:20    time: 3.973373  data: 0.000716  max mem: 4109
I20241203 10:36:35 2030095 dinov2 helpers.py:102]   [130/634]  eta: 0:30:55    time: 3.973248  data: 0.000628  max mem: 4109
I20241203 10:37:14 2030095 dinov2 helpers.py:102]   [140/634]  eta: 0:30:29    time: 3.973296  data: 0.001618  max mem: 4109
I20241203 10:37:54 2030095 dinov2 helpers.py:102]   [150/634]  eta: 0:30:00    time: 3.973295  data: 0.001687  max mem: 4109
I20241203 10:38:34 2030095 dinov2 helpers.py:102]   [160/634]  eta: 0:29:30    time: 3.973157  data: 0.001120  max mem: 4109
I20241203 10:39:14 2030095 dinov2 helpers.py:102]   [170/634]  eta: 0:28:59    time: 3.972850  data: 0.001149  max mem: 4109
I20241203 10:39:53 2030095 dinov2 helpers.py:102]   [180/634]  eta: 0:28:27    time: 3.969999  data: 0.000581  max mem: 4109
I20241203 10:40:33 2030095 dinov2 helpers.py:102]   [190/634]  eta: 0:27:55    time: 3.970179  data: 0.000736  max mem: 4109
I20241203 10:41:13 2030095 dinov2 helpers.py:102]   [200/634]  eta: 0:27:21    time: 3.973179  data: 0.000882  max mem: 4109
I20241203 10:41:52 2030095 dinov2 helpers.py:102]   [210/634]  eta: 0:26:47    time: 3.971165  data: 0.000771  max mem: 4109
I20241203 10:42:32 2030095 dinov2 helpers.py:102]   [220/634]  eta: 0:26:13    time: 3.969082  data: 0.001837  max mem: 4109
I20241203 10:43:12 2030095 dinov2 helpers.py:102]   [230/634]  eta: 0:25:38    time: 3.969844  data: 0.001727  max mem: 4109
I20241203 10:43:52 2030095 dinov2 helpers.py:102]   [240/634]  eta: 0:25:02    time: 3.970808  data: 0.001575  max mem: 4109
I20241203 10:44:31 2030095 dinov2 helpers.py:102]   [250/634]  eta: 0:24:26    time: 3.972008  data: 0.001630  max mem: 4109
I20241203 10:45:11 2030095 dinov2 helpers.py:102]   [260/634]  eta: 0:23:50    time: 3.973254  data: 0.000750  max mem: 4109
I20241203 10:45:51 2030095 dinov2 helpers.py:102]   [270/634]  eta: 0:23:14    time: 3.973854  data: 0.000780  max mem: 4109
I20241203 10:46:31 2030095 dinov2 helpers.py:102]   [280/634]  eta: 0:22:38    time: 3.977003  data: 0.001275  max mem: 4109
I20241203 10:47:10 2030095 dinov2 helpers.py:102]   [290/634]  eta: 0:22:01    time: 3.980820  data: 0.001276  max mem: 4109
I20241203 10:47:50 2030095 dinov2 helpers.py:102]   [300/634]  eta: 0:21:24    time: 3.979931  data: 0.000767  max mem: 4109
I20241203 10:48:30 2030095 dinov2 helpers.py:102]   [310/634]  eta: 0:20:47    time: 3.978048  data: 0.000923  max mem: 4109
I20241203 10:49:10 2030095 dinov2 helpers.py:102]   [320/634]  eta: 0:20:10    time: 3.979857  data: 0.001332  max mem: 4109
I20241203 10:49:50 2030095 dinov2 helpers.py:102]   [330/634]  eta: 0:19:33    time: 3.981647  data: 0.001398  max mem: 4109
I20241203 10:50:29 2030095 dinov2 helpers.py:102]   [340/634]  eta: 0:18:55    time: 3.980698  data: 0.001056  max mem: 4109
I20241203 10:51:09 2030095 dinov2 helpers.py:102]   [350/634]  eta: 0:18:17    time: 3.978899  data: 0.001047  max mem: 4109
I20241203 10:51:49 2030095 dinov2 helpers.py:102]   [360/634]  eta: 0:17:39    time: 3.977900  data: 0.001550  max mem: 4109
I20241203 10:52:29 2030095 dinov2 helpers.py:102]   [370/634]  eta: 0:17:02    time: 3.976233  data: 0.002065  max mem: 4109
I20241203 10:53:09 2030095 dinov2 helpers.py:102]   [380/634]  eta: 0:16:24    time: 3.979195  data: 0.001522  max mem: 4109
I20241203 10:53:48 2030095 dinov2 helpers.py:102]   [390/634]  eta: 0:15:46    time: 3.985422  data: 0.002107  max mem: 4109
I20241203 10:54:28 2030095 dinov2 helpers.py:102]   [400/634]  eta: 0:15:07    time: 3.984371  data: 0.002815  max mem: 4109
I20241203 10:55:08 2030095 dinov2 helpers.py:102]   [410/634]  eta: 0:14:29    time: 3.978046  data: 0.001442  max mem: 4109
I20241203 10:55:48 2030095 dinov2 helpers.py:102]   [420/634]  eta: 0:13:51    time: 3.976477  data: 0.000802  max mem: 4109
I20241203 10:56:28 2030095 dinov2 helpers.py:102]   [430/634]  eta: 0:13:12    time: 3.983685  data: 0.000827  max mem: 4109
I20241203 10:57:08 2030095 dinov2 helpers.py:102]   [440/634]  eta: 0:12:34    time: 3.988854  data: 0.001738  max mem: 4109
I20241203 10:57:47 2030095 dinov2 helpers.py:102]   [450/634]  eta: 0:11:55    time: 3.983372  data: 0.001926  max mem: 4109
I20241203 10:58:27 2030095 dinov2 helpers.py:102]   [460/634]  eta: 0:11:17    time: 3.982545  data: 0.000902  max mem: 4109
I20241203 10:59:07 2030095 dinov2 helpers.py:102]   [470/634]  eta: 0:10:38    time: 3.988066  data: 0.001506  max mem: 4109
I20241203 10:59:47 2030095 dinov2 helpers.py:102]   [480/634]  eta: 0:10:00    time: 3.989972  data: 0.001422  max mem: 4109
I20241203 11:00:27 2030095 dinov2 helpers.py:102]   [490/634]  eta: 0:09:21    time: 3.988899  data: 0.000730  max mem: 4109
I20241203 11:01:07 2030095 dinov2 helpers.py:102]   [500/634]  eta: 0:08:42    time: 3.985165  data: 0.000717  max mem: 4109
I20241203 11:01:47 2030095 dinov2 helpers.py:102]   [510/634]  eta: 0:08:03    time: 3.984431  data: 0.001119  max mem: 4109
I20241203 11:02:26 2030095 dinov2 helpers.py:102]   [520/634]  eta: 0:07:25    time: 3.982638  data: 0.001390  max mem: 4109
I20241203 11:03:06 2030095 dinov2 helpers.py:102]   [530/634]  eta: 0:06:46    time: 3.980028  data: 0.001315  max mem: 4109
I20241203 11:03:46 2030095 dinov2 helpers.py:102]   [540/634]  eta: 0:06:07    time: 3.985467  data: 0.001264  max mem: 4109
I20241203 11:04:26 2030095 dinov2 helpers.py:102]   [550/634]  eta: 0:05:28    time: 3.986177  data: 0.001632  max mem: 4109
I20241203 11:05:06 2030095 dinov2 helpers.py:102]   [560/634]  eta: 0:04:49    time: 3.984407  data: 0.001392  max mem: 4109
I20241203 11:05:46 2030095 dinov2 helpers.py:102]   [570/634]  eta: 0:04:10    time: 3.988169  data: 0.000804  max mem: 4109
I20241203 11:06:25 2030095 dinov2 helpers.py:102]   [580/634]  eta: 0:03:31    time: 3.987149  data: 0.000878  max mem: 4109
I20241203 11:07:05 2030095 dinov2 helpers.py:102]   [590/634]  eta: 0:02:52    time: 3.986224  data: 0.000721  max mem: 4109
I20241203 11:07:45 2030095 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.985418  data: 0.001348  max mem: 4109
I20241203 11:08:25 2030095 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.984296  data: 0.001441  max mem: 4109
I20241203 11:09:05 2030095 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.986125  data: 0.000957  max mem: 4109
I20241203 11:09:45 2030095 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.988139  data: 0.000864  max mem: 4109
I20241203 11:10:03 2030095 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.287658  data: 0.000737  max mem: 4109
I20241203 11:10:03 2030095 dinov2 helpers.py:130]  Total time: 0:41:30 (3.928520 s / it)
I20241203 11:10:03 2030095 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:10:03 2030095 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:10:04 2030095 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:10:04 2030095 dinov2 loaders.py:151] sampler: distributed
I20241203 11:10:04 2030095 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:10:04 2030095 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:10:04,400) - Submitted job triggered an exception
E20241203 11:10:04 2030095 submitit submission.py:68] Submitted job triggered an exception
