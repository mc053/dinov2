submitit INFO (2024-12-03 10:27:42,206) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-03 10:27:42,206) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:50 2030093 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:50 2030093 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:50 2030093 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:50 2030093 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:50 2030093 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:24 2030093 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:29 2030093 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:29 2030093 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:45 2030093 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:45 2030093 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:51 2030093 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:51 2030093 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:51 2030093 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:51 2030093 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:51 2030093 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:51 2030093 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:40 2030093 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:40 2030093 dinov2 helpers.py:102]   [  0/634]  eta: 8:43:53    time: 49.579433  data: 10.290846  max mem: 3463
I20241203 10:30:12 2030093 dinov2 helpers.py:102]   [ 10/634]  eta: 1:17:04    time: 7.411033  data: 0.937065  max mem: 4109
I20241203 10:30:52 2030093 dinov2 helpers.py:102]   [ 20/634]  eta: 0:58:51    time: 3.559814  data: 0.002327  max mem: 4109
I20241203 10:31:31 2030093 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:03    time: 3.940334  data: 0.002263  max mem: 4109
I20241203 10:32:11 2030093 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:15    time: 3.955279  data: 0.001203  max mem: 4109
I20241203 10:32:50 2030093 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:42    time: 3.958821  data: 0.001172  max mem: 4109
I20241203 10:33:30 2030093 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:47    time: 3.967854  data: 0.001093  max mem: 4109
I20241203 10:34:10 2030093 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:13    time: 3.973548  data: 0.000837  max mem: 4109
I20241203 10:34:50 2030093 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:53    time: 3.973608  data: 0.000891  max mem: 4109
I20241203 10:35:29 2030093 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:41    time: 3.973288  data: 0.001307  max mem: 4109
I20241203 10:36:09 2030093 dinov2 helpers.py:102]   [100/634]  eta: 0:38:36    time: 3.973236  data: 0.001988  max mem: 4109
I20241203 10:36:49 2030093 dinov2 helpers.py:102]   [110/634]  eta: 0:37:36    time: 3.973346  data: 0.001557  max mem: 4109
I20241203 10:37:29 2030093 dinov2 helpers.py:102]   [120/634]  eta: 0:36:38    time: 3.973323  data: 0.000923  max mem: 4109
I20241203 10:38:08 2030093 dinov2 helpers.py:102]   [130/634]  eta: 0:35:44    time: 3.973279  data: 0.000815  max mem: 4109
I20241203 10:38:48 2030093 dinov2 helpers.py:102]   [140/634]  eta: 0:34:51    time: 3.972043  data: 0.001192  max mem: 4109
I20241203 10:39:28 2030093 dinov2 helpers.py:102]   [150/634]  eta: 0:34:01    time: 3.971819  data: 0.001349  max mem: 4109
I20241203 10:40:07 2030093 dinov2 helpers.py:102]   [160/634]  eta: 0:33:11    time: 3.972858  data: 0.000901  max mem: 4109
I20241203 10:40:47 2030093 dinov2 helpers.py:102]   [170/634]  eta: 0:32:23    time: 3.973066  data: 0.000823  max mem: 4109
I20241203 10:41:27 2030093 dinov2 helpers.py:102]   [180/634]  eta: 0:31:36    time: 3.972161  data: 0.001483  max mem: 4109
I20241203 10:42:07 2030093 dinov2 helpers.py:102]   [190/634]  eta: 0:30:49    time: 3.968294  data: 0.001430  max mem: 4109
I20241203 10:42:46 2030093 dinov2 helpers.py:102]   [200/634]  eta: 0:30:03    time: 3.969023  data: 0.001213  max mem: 4109
I20241203 10:43:26 2030093 dinov2 helpers.py:102]   [210/634]  eta: 0:29:18    time: 3.972546  data: 0.001408  max mem: 4109
I20241203 10:44:06 2030093 dinov2 helpers.py:102]   [220/634]  eta: 0:28:33    time: 3.972837  data: 0.000990  max mem: 4109
I20241203 10:44:45 2030093 dinov2 helpers.py:102]   [230/634]  eta: 0:27:49    time: 3.973062  data: 0.001033  max mem: 4109
I20241203 10:45:25 2030093 dinov2 helpers.py:102]   [240/634]  eta: 0:27:05    time: 3.973296  data: 0.001267  max mem: 4109
I20241203 10:46:05 2030093 dinov2 helpers.py:102]   [250/634]  eta: 0:26:22    time: 3.978696  data: 0.001008  max mem: 4109
I20241203 10:46:45 2030093 dinov2 helpers.py:102]   [260/634]  eta: 0:25:38    time: 3.982674  data: 0.000815  max mem: 4109
I20241203 10:47:25 2030093 dinov2 helpers.py:102]   [270/634]  eta: 0:24:55    time: 3.980706  data: 0.002073  max mem: 4109
I20241203 10:48:04 2030093 dinov2 helpers.py:102]   [280/634]  eta: 0:24:13    time: 3.978910  data: 0.002978  max mem: 4109
I20241203 10:48:44 2030093 dinov2 helpers.py:102]   [290/634]  eta: 0:23:30    time: 3.977961  data: 0.001808  max mem: 4109
I20241203 10:49:24 2030093 dinov2 helpers.py:102]   [300/634]  eta: 0:22:48    time: 3.979817  data: 0.000991  max mem: 4109
I20241203 10:50:04 2030093 dinov2 helpers.py:102]   [310/634]  eta: 0:22:06    time: 3.981709  data: 0.001439  max mem: 4109
I20241203 10:50:44 2030093 dinov2 helpers.py:102]   [320/634]  eta: 0:21:24    time: 3.979007  data: 0.001794  max mem: 4109
I20241203 10:51:23 2030093 dinov2 helpers.py:102]   [330/634]  eta: 0:20:42    time: 3.981557  data: 0.002824  max mem: 4109
I20241203 10:52:03 2030093 dinov2 helpers.py:102]   [340/634]  eta: 0:20:00    time: 3.981494  data: 0.002924  max mem: 4109
I20241203 10:52:43 2030093 dinov2 helpers.py:102]   [350/634]  eta: 0:19:18    time: 3.981729  data: 0.002125  max mem: 4109
I20241203 10:53:23 2030093 dinov2 helpers.py:102]   [360/634]  eta: 0:18:37    time: 3.990018  data: 0.001827  max mem: 4109
I20241203 10:54:03 2030093 dinov2 helpers.py:102]   [370/634]  eta: 0:17:55    time: 3.988435  data: 0.001015  max mem: 4109
I20241203 10:54:43 2030093 dinov2 helpers.py:102]   [380/634]  eta: 0:17:14    time: 3.985167  data: 0.000712  max mem: 4109
I20241203 10:55:23 2030093 dinov2 helpers.py:102]   [390/634]  eta: 0:16:33    time: 3.983066  data: 0.000783  max mem: 4109
I20241203 10:56:02 2030093 dinov2 helpers.py:102]   [400/634]  eta: 0:15:52    time: 3.981834  data: 0.000752  max mem: 4109
I20241203 10:56:42 2030093 dinov2 helpers.py:102]   [410/634]  eta: 0:15:10    time: 3.986391  data: 0.001117  max mem: 4109
I20241203 10:57:22 2030093 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.984353  data: 0.001187  max mem: 4109
I20241203 10:58:02 2030093 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.980727  data: 0.000826  max mem: 4109
I20241203 10:58:42 2030093 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.983629  data: 0.000665  max mem: 4109
I20241203 10:59:22 2030093 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.987175  data: 0.000946  max mem: 4109
I20241203 11:00:02 2030093 dinov2 helpers.py:102]   [460/634]  eta: 0:11:46    time: 3.988841  data: 0.001052  max mem: 4109
I20241203 11:00:41 2030093 dinov2 helpers.py:102]   [470/634]  eta: 0:11:05    time: 3.986968  data: 0.000643  max mem: 4109
I20241203 11:01:21 2030093 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.982618  data: 0.000741  max mem: 4109
I20241203 11:02:01 2030093 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.982660  data: 0.001148  max mem: 4109
I20241203 11:02:41 2030093 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.987078  data: 0.001072  max mem: 4109
I20241203 11:03:21 2030093 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.984535  data: 0.001372  max mem: 4109
I20241203 11:04:01 2030093 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.980959  data: 0.002064  max mem: 4109
I20241203 11:04:40 2030093 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.981722  data: 0.001779  max mem: 4109
I20241203 11:05:20 2030093 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.980799  data: 0.001190  max mem: 4109
I20241203 11:06:00 2030093 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.980897  data: 0.000885  max mem: 4109
I20241203 11:06:40 2030093 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.985358  data: 0.000761  max mem: 4109
I20241203 11:07:20 2030093 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.988028  data: 0.001008  max mem: 4109
I20241203 11:08:00 2030093 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.986125  data: 0.001059  max mem: 4109
I20241203 11:08:39 2030093 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.981604  data: 0.000910  max mem: 4109
I20241203 11:09:19 2030093 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.984475  data: 0.000849  max mem: 4109
I20241203 11:09:58 2030093 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.942170  data: 0.000780  max mem: 4109
I20241203 11:10:29 2030093 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.480030  data: 0.000828  max mem: 4109
I20241203 11:10:55 2030093 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 2.817319  data: 0.000661  max mem: 4109
I20241203 11:11:03 2030093 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.736857  data: 0.000587  max mem: 4109
I20241203 11:11:03 2030093 dinov2 helpers.py:130]  Total time: 0:42:11 (3.993643 s / it)
I20241203 11:11:03 2030093 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:11:03 2030093 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:11:03 2030093 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:11:03 2030093 dinov2 loaders.py:151] sampler: distributed
I20241203 11:11:03 2030093 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:11:03 2030093 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:11:03,602) - Submitted job triggered an exception
E20241203 11:11:03 2030093 submitit submission.py:68] Submitted job triggered an exception
