submitit INFO (2024-12-04 10:17:37,563) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-04 10:17:37,563) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567768 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567768 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567768 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567768 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567768 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:21 2567768 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:25 2567768 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:26 2567768 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:30 2567768 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:30 2567768 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:33 2567768 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:33 2567768 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:33 2567768 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:33 2567768 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:33 2567768 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:33 2567768 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:14 2567768 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:14 2567768 dinov2 helpers.py:102]   [  0/634]  eta: 7:05:45    time: 40.292591  data: 15.981066  max mem: 3463
I20241204 10:19:40 2567768 dinov2 helpers.py:102]   [ 10/634]  eta: 1:03:23    time: 6.095150  data: 1.454896  max mem: 4109
I20241204 10:20:19 2567768 dinov2 helpers.py:102]   [ 20/634]  eta: 0:51:41    time: 3.288539  data: 0.001613  max mem: 4109
I20241204 10:20:59 2567768 dinov2 helpers.py:102]   [ 30/634]  eta: 0:47:15    time: 3.925143  data: 0.000963  max mem: 4109
I20241204 10:21:39 2567768 dinov2 helpers.py:102]   [ 40/634]  eta: 0:44:43    time: 3.958562  data: 0.000833  max mem: 4109
I20241204 10:22:18 2567768 dinov2 helpers.py:102]   [ 50/634]  eta: 0:42:56    time: 3.971804  data: 0.000778  max mem: 4109
I20241204 10:22:58 2567768 dinov2 helpers.py:102]   [ 60/634]  eta: 0:41:31    time: 3.974527  data: 0.000864  max mem: 4109
I20241204 10:23:38 2567768 dinov2 helpers.py:102]   [ 70/634]  eta: 0:40:19    time: 3.976889  data: 0.000944  max mem: 4109
I20241204 10:24:18 2567768 dinov2 helpers.py:102]   [ 80/634]  eta: 0:39:14    time: 3.977908  data: 0.001079  max mem: 4109
I20241204 10:24:57 2567768 dinov2 helpers.py:102]   [ 90/634]  eta: 0:38:16    time: 3.978638  data: 0.001034  max mem: 4109
I20241204 10:25:37 2567768 dinov2 helpers.py:102]   [100/634]  eta: 0:37:21    time: 3.983098  data: 0.001815  max mem: 4109
I20241204 10:26:17 2567768 dinov2 helpers.py:102]   [110/634]  eta: 0:36:29    time: 3.982371  data: 0.001699  max mem: 4109
I20241204 10:26:57 2567768 dinov2 helpers.py:102]   [120/634]  eta: 0:35:38    time: 3.978741  data: 0.001084  max mem: 4109
I20241204 10:27:37 2567768 dinov2 helpers.py:102]   [130/634]  eta: 0:34:50    time: 3.979722  data: 0.001258  max mem: 4109
I20241204 10:28:16 2567768 dinov2 helpers.py:102]   [140/634]  eta: 0:34:03    time: 3.981549  data: 0.001117  max mem: 4109
I20241204 10:28:56 2567768 dinov2 helpers.py:102]   [150/634]  eta: 0:33:16    time: 3.978703  data: 0.000952  max mem: 4109
I20241204 10:29:36 2567768 dinov2 helpers.py:102]   [160/634]  eta: 0:32:31    time: 3.977787  data: 0.001856  max mem: 4109
I20241204 10:30:16 2567768 dinov2 helpers.py:102]   [170/634]  eta: 0:31:46    time: 3.980349  data: 0.002749  max mem: 4109
I20241204 10:30:56 2567768 dinov2 helpers.py:102]   [180/634]  eta: 0:31:01    time: 3.978604  data: 0.001764  max mem: 4109
I20241204 10:31:35 2567768 dinov2 helpers.py:102]   [190/634]  eta: 0:30:18    time: 3.977847  data: 0.001115  max mem: 4109
I20241204 10:32:15 2567768 dinov2 helpers.py:102]   [200/634]  eta: 0:29:34    time: 3.980565  data: 0.000917  max mem: 4109
I20241204 10:32:55 2567768 dinov2 helpers.py:102]   [210/634]  eta: 0:28:51    time: 3.984083  data: 0.000923  max mem: 4109
I20241204 10:33:35 2567768 dinov2 helpers.py:102]   [220/634]  eta: 0:28:08    time: 3.984276  data: 0.001247  max mem: 4109
I20241204 10:34:15 2567768 dinov2 helpers.py:102]   [230/634]  eta: 0:27:26    time: 3.980543  data: 0.001336  max mem: 4109
I20241204 10:34:54 2567768 dinov2 helpers.py:102]   [240/634]  eta: 0:26:43    time: 3.976562  data: 0.001993  max mem: 4109
I20241204 10:35:34 2567768 dinov2 helpers.py:102]   [250/634]  eta: 0:26:01    time: 3.975522  data: 0.001897  max mem: 4109
I20241204 10:36:14 2567768 dinov2 helpers.py:102]   [260/634]  eta: 0:25:19    time: 3.978456  data: 0.001411  max mem: 4109
I20241204 10:36:54 2567768 dinov2 helpers.py:102]   [270/634]  eta: 0:24:38    time: 3.979525  data: 0.001308  max mem: 4109
I20241204 10:37:34 2567768 dinov2 helpers.py:102]   [280/634]  eta: 0:23:56    time: 3.979573  data: 0.001004  max mem: 4109
I20241204 10:38:13 2567768 dinov2 helpers.py:102]   [290/634]  eta: 0:23:15    time: 3.980549  data: 0.001141  max mem: 4109
I20241204 10:38:53 2567768 dinov2 helpers.py:102]   [300/634]  eta: 0:22:33    time: 3.983018  data: 0.001354  max mem: 4109
I20241204 10:39:33 2567768 dinov2 helpers.py:102]   [310/634]  eta: 0:21:52    time: 3.982963  data: 0.002092  max mem: 4109
I20241204 10:40:13 2567768 dinov2 helpers.py:102]   [320/634]  eta: 0:21:11    time: 3.983080  data: 0.001929  max mem: 4109
I20241204 10:40:53 2567768 dinov2 helpers.py:102]   [330/634]  eta: 0:20:30    time: 3.986869  data: 0.001469  max mem: 4109
I20241204 10:41:33 2567768 dinov2 helpers.py:102]   [340/634]  eta: 0:19:49    time: 3.989545  data: 0.001442  max mem: 4109
I20241204 10:42:13 2567768 dinov2 helpers.py:102]   [350/634]  eta: 0:19:08    time: 3.986808  data: 0.001234  max mem: 4109
I20241204 10:42:52 2567768 dinov2 helpers.py:102]   [360/634]  eta: 0:18:27    time: 3.984158  data: 0.001304  max mem: 4109
I20241204 10:43:32 2567768 dinov2 helpers.py:102]   [370/634]  eta: 0:17:46    time: 3.986877  data: 0.001007  max mem: 4109
I20241204 10:44:12 2567768 dinov2 helpers.py:102]   [380/634]  eta: 0:17:05    time: 3.988773  data: 0.001172  max mem: 4109
I20241204 10:44:52 2567768 dinov2 helpers.py:102]   [390/634]  eta: 0:16:25    time: 3.987759  data: 0.001243  max mem: 4109
I20241204 10:45:32 2567768 dinov2 helpers.py:102]   [400/634]  eta: 0:15:44    time: 3.988729  data: 0.002544  max mem: 4109
I20241204 10:46:12 2567768 dinov2 helpers.py:102]   [410/634]  eta: 0:15:03    time: 3.989880  data: 0.002342  max mem: 4109
I20241204 10:46:52 2567768 dinov2 helpers.py:102]   [420/634]  eta: 0:14:23    time: 3.989653  data: 0.000643  max mem: 4109
I20241204 10:47:32 2567768 dinov2 helpers.py:102]   [430/634]  eta: 0:13:42    time: 3.986713  data: 0.001072  max mem: 4109
I20241204 10:48:11 2567768 dinov2 helpers.py:102]   [440/634]  eta: 0:13:02    time: 3.986715  data: 0.001384  max mem: 4109
I20241204 10:48:51 2567768 dinov2 helpers.py:102]   [450/634]  eta: 0:12:21    time: 3.984829  data: 0.001389  max mem: 4109
I20241204 10:49:31 2567768 dinov2 helpers.py:102]   [460/634]  eta: 0:11:41    time: 3.978312  data: 0.001283  max mem: 4109
I20241204 10:50:11 2567768 dinov2 helpers.py:102]   [470/634]  eta: 0:11:00    time: 3.978485  data: 0.000983  max mem: 4109
I20241204 10:50:51 2567768 dinov2 helpers.py:102]   [480/634]  eta: 0:10:20    time: 3.980584  data: 0.000879  max mem: 4109
I20241204 10:51:31 2567768 dinov2 helpers.py:102]   [490/634]  eta: 0:09:39    time: 3.982374  data: 0.001092  max mem: 4109
I20241204 10:52:10 2567768 dinov2 helpers.py:102]   [500/634]  eta: 0:08:59    time: 3.986792  data: 0.001013  max mem: 4109
I20241204 10:52:50 2567768 dinov2 helpers.py:102]   [510/634]  eta: 0:08:19    time: 3.987757  data: 0.001104  max mem: 4109
I20241204 10:53:30 2567768 dinov2 helpers.py:102]   [520/634]  eta: 0:07:38    time: 3.986982  data: 0.001162  max mem: 4109
I20241204 10:54:10 2567768 dinov2 helpers.py:102]   [530/634]  eta: 0:06:58    time: 3.989474  data: 0.001124  max mem: 4109
I20241204 10:54:50 2567768 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.989872  data: 0.001139  max mem: 4109
I20241204 10:55:30 2567768 dinov2 helpers.py:102]   [550/634]  eta: 0:05:37    time: 3.987842  data: 0.001115  max mem: 4109
I20241204 10:56:10 2567768 dinov2 helpers.py:102]   [560/634]  eta: 0:04:57    time: 3.988373  data: 0.001166  max mem: 4109
I20241204 10:56:50 2567768 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.988792  data: 0.000864  max mem: 4109
I20241204 10:57:29 2567768 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.987824  data: 0.000885  max mem: 4109
I20241204 10:58:09 2567768 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.990966  data: 0.001082  max mem: 4109
I20241204 10:58:49 2567768 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.988687  data: 0.001584  max mem: 4109
I20241204 10:59:29 2567768 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.985420  data: 0.001612  max mem: 4109
I20241204 11:00:09 2567768 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.984971  data: 0.001676  max mem: 4109
I20241204 11:00:48 2567768 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.939843  data: 0.001736  max mem: 4109
I20241204 11:01:06 2567768 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.234214  data: 0.000852  max mem: 4109
I20241204 11:01:06 2567768 dinov2 helpers.py:130]  Total time: 0:42:32 (4.026685 s / it)
I20241204 11:01:06 2567768 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:01:06 2567768 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:07 2567768 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:07 2567768 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:07 2567768 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:07 2567768 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:07 2567768 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:20 2567768 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:16:42    time: 12.847313  data: 9.594322  max mem: 4109
I20241204 11:01:56 2567768 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:57    time: 4.379967  data: 0.873935  max mem: 4109
I20241204 11:02:36 2567768 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:04    time: 3.776331  data: 0.003475  max mem: 4109
I20241204 11:03:16 2567768 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:18    time: 4.002717  data: 0.004394  max mem: 4109
I20241204 11:03:56 2567768 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:35    time: 3.991307  data: 0.004101  max mem: 4109
I20241204 11:04:36 2567768 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:54    time: 3.999220  data: 0.004690  max mem: 4109
I20241204 11:05:16 2567768 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:13    time: 3.993263  data: 0.006128  max mem: 4109
I20241204 11:05:53 2567768 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:32    time: 3.859087  data: 0.006726  max mem: 4109
I20241204 11:06:10 2567768 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.301879  data: 0.004210  max mem: 4109
I20241204 11:06:10 2567768 dinov2 helpers.py:130] Test: Total time: 0:05:02 (3.872912 s / it)
I20241204 11:06:10 2567768 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:10 2567768 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:10 2567768 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:10 2567768 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:10 2567768 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:11,101) - Job completed successfully
I20241204 11:06:11 2567768 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:11,103) - Exiting after successful completion
I20241204 11:06:11 2567768 submitit submission.py:61] Exiting after successful completion
