submitit INFO (2024-12-04 10:17:37,555) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-04 10:17:37,555) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567764 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567764 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567764 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567764 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567764 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:16 2567764 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:21 2567764 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:22 2567764 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:29 2567764 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:29 2567764 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:32 2567764 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:32 2567764 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:32 2567764 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:32 2567764 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:32 2567764 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:32 2567764 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:18 2567764 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:18 2567764 dinov2 helpers.py:102]   [  0/634]  eta: 8:09:58    time: 46.369530  data: 21.595966  max mem: 3463
I20241204 10:19:48 2567764 dinov2 helpers.py:102]   [ 10/634]  eta: 1:11:36    time: 6.884968  data: 1.964331  max mem: 4109
I20241204 10:20:27 2567764 dinov2 helpers.py:102]   [ 20/634]  eta: 0:55:57    time: 3.423917  data: 0.001153  max mem: 4109
I20241204 10:21:06 2567764 dinov2 helpers.py:102]   [ 30/634]  eta: 0:50:07    time: 3.930807  data: 0.001031  max mem: 4109
I20241204 10:21:46 2567764 dinov2 helpers.py:102]   [ 40/634]  eta: 0:46:51    time: 3.959639  data: 0.001353  max mem: 4109
I20241204 10:22:26 2567764 dinov2 helpers.py:102]   [ 50/634]  eta: 0:44:36    time: 3.971185  data: 0.001291  max mem: 4109
I20241204 10:23:05 2567764 dinov2 helpers.py:102]   [ 60/634]  eta: 0:42:54    time: 3.978168  data: 0.001112  max mem: 4109
I20241204 10:23:45 2567764 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:29    time: 3.981302  data: 0.001378  max mem: 4109
I20241204 10:24:25 2567764 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:15    time: 3.979664  data: 0.001160  max mem: 4109
I20241204 10:25:05 2567764 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:09    time: 3.979679  data: 0.000922  max mem: 4109
I20241204 10:25:45 2567764 dinov2 helpers.py:102]   [100/634]  eta: 0:38:08    time: 3.978672  data: 0.000767  max mem: 4109
I20241204 10:26:24 2567764 dinov2 helpers.py:102]   [110/634]  eta: 0:37:10    time: 3.975999  data: 0.000761  max mem: 4109
I20241204 10:27:04 2567764 dinov2 helpers.py:102]   [120/634]  eta: 0:36:16    time: 3.976032  data: 0.000828  max mem: 4109
I20241204 10:27:44 2567764 dinov2 helpers.py:102]   [130/634]  eta: 0:35:24    time: 3.978645  data: 0.000908  max mem: 4109
I20241204 10:28:24 2567764 dinov2 helpers.py:102]   [140/634]  eta: 0:34:33    time: 3.979684  data: 0.001012  max mem: 4109
I20241204 10:29:04 2567764 dinov2 helpers.py:102]   [150/634]  eta: 0:33:44    time: 3.979789  data: 0.000854  max mem: 4109
I20241204 10:29:43 2567764 dinov2 helpers.py:102]   [160/634]  eta: 0:32:56    time: 3.979676  data: 0.000991  max mem: 4109
I20241204 10:30:23 2567764 dinov2 helpers.py:102]   [170/634]  eta: 0:32:09    time: 3.978954  data: 0.001041  max mem: 4109
I20241204 10:31:03 2567764 dinov2 helpers.py:102]   [180/634]  eta: 0:31:23    time: 3.977755  data: 0.000846  max mem: 4109
I20241204 10:31:43 2567764 dinov2 helpers.py:102]   [190/634]  eta: 0:30:38    time: 3.978312  data: 0.000964  max mem: 4109
I20241204 10:32:22 2567764 dinov2 helpers.py:102]   [200/634]  eta: 0:29:53    time: 3.980502  data: 0.001954  max mem: 4109
I20241204 10:33:02 2567764 dinov2 helpers.py:102]   [210/634]  eta: 0:29:09    time: 3.977858  data: 0.003971  max mem: 4109
I20241204 10:33:42 2567764 dinov2 helpers.py:102]   [220/634]  eta: 0:28:25    time: 3.977043  data: 0.003119  max mem: 4109
I20241204 10:34:22 2567764 dinov2 helpers.py:102]   [230/634]  eta: 0:27:41    time: 3.979673  data: 0.001858  max mem: 4109
I20241204 10:35:02 2567764 dinov2 helpers.py:102]   [240/634]  eta: 0:26:58    time: 3.976552  data: 0.002514  max mem: 4109
I20241204 10:35:41 2567764 dinov2 helpers.py:102]   [250/634]  eta: 0:26:14    time: 3.975577  data: 0.001825  max mem: 4109
I20241204 10:36:21 2567764 dinov2 helpers.py:102]   [260/634]  eta: 0:25:32    time: 3.975677  data: 0.000927  max mem: 4109
I20241204 10:37:01 2567764 dinov2 helpers.py:102]   [270/634]  eta: 0:24:49    time: 3.976786  data: 0.003635  max mem: 4109
I20241204 10:37:41 2567764 dinov2 helpers.py:102]   [280/634]  eta: 0:24:07    time: 3.979612  data: 0.003802  max mem: 4109
I20241204 10:38:21 2567764 dinov2 helpers.py:102]   [290/634]  eta: 0:23:25    time: 3.981290  data: 0.002511  max mem: 4109
I20241204 10:39:00 2567764 dinov2 helpers.py:102]   [300/634]  eta: 0:22:43    time: 3.979576  data: 0.002626  max mem: 4109
I20241204 10:39:40 2567764 dinov2 helpers.py:102]   [310/634]  eta: 0:22:01    time: 3.977801  data: 0.001302  max mem: 4109
I20241204 10:40:20 2567764 dinov2 helpers.py:102]   [320/634]  eta: 0:21:19    time: 3.985703  data: 0.001146  max mem: 4109
I20241204 10:41:00 2567764 dinov2 helpers.py:102]   [330/634]  eta: 0:20:38    time: 3.987624  data: 0.001083  max mem: 4109
I20241204 10:41:40 2567764 dinov2 helpers.py:102]   [340/634]  eta: 0:19:56    time: 3.979664  data: 0.001080  max mem: 4109
I20241204 10:42:19 2567764 dinov2 helpers.py:102]   [350/634]  eta: 0:19:15    time: 3.979743  data: 0.001068  max mem: 4109
I20241204 10:42:59 2567764 dinov2 helpers.py:102]   [360/634]  eta: 0:18:33    time: 3.984159  data: 0.000985  max mem: 4109
I20241204 10:43:39 2567764 dinov2 helpers.py:102]   [370/634]  eta: 0:17:52    time: 3.982353  data: 0.000816  max mem: 4109
I20241204 10:44:19 2567764 dinov2 helpers.py:102]   [380/634]  eta: 0:17:11    time: 3.983344  data: 0.002419  max mem: 4109
I20241204 10:44:59 2567764 dinov2 helpers.py:102]   [390/634]  eta: 0:16:30    time: 3.986856  data: 0.002355  max mem: 4109
I20241204 10:45:39 2567764 dinov2 helpers.py:102]   [400/634]  eta: 0:15:49    time: 3.988787  data: 0.001243  max mem: 4109
I20241204 10:46:19 2567764 dinov2 helpers.py:102]   [410/634]  eta: 0:15:08    time: 3.990634  data: 0.001554  max mem: 4109
I20241204 10:46:58 2567764 dinov2 helpers.py:102]   [420/634]  eta: 0:14:27    time: 3.986992  data: 0.001030  max mem: 4109
I20241204 10:47:38 2567764 dinov2 helpers.py:102]   [430/634]  eta: 0:13:46    time: 3.980619  data: 0.000662  max mem: 4109
I20241204 10:48:18 2567764 dinov2 helpers.py:102]   [440/634]  eta: 0:13:05    time: 3.976234  data: 0.000643  max mem: 4109
I20241204 10:48:58 2567764 dinov2 helpers.py:102]   [450/634]  eta: 0:12:24    time: 3.973810  data: 0.000902  max mem: 4109
I20241204 10:49:38 2567764 dinov2 helpers.py:102]   [460/634]  eta: 0:11:44    time: 3.976135  data: 0.001075  max mem: 4109
I20241204 10:50:17 2567764 dinov2 helpers.py:102]   [470/634]  eta: 0:11:03    time: 3.982161  data: 0.000807  max mem: 4109
I20241204 10:50:57 2567764 dinov2 helpers.py:102]   [480/634]  eta: 0:10:22    time: 3.980633  data: 0.000970  max mem: 4109
I20241204 10:51:37 2567764 dinov2 helpers.py:102]   [490/634]  eta: 0:09:42    time: 3.979664  data: 0.000905  max mem: 4109
I20241204 10:52:17 2567764 dinov2 helpers.py:102]   [500/634]  eta: 0:09:01    time: 3.984937  data: 0.000685  max mem: 4109
I20241204 10:52:57 2567764 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.983376  data: 0.001016  max mem: 4109
I20241204 10:53:36 2567764 dinov2 helpers.py:102]   [520/634]  eta: 0:07:40    time: 3.982419  data: 0.000995  max mem: 4109
I20241204 10:54:16 2567764 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.985020  data: 0.001162  max mem: 4109
I20241204 10:54:56 2567764 dinov2 helpers.py:102]   [540/634]  eta: 0:06:19    time: 3.987741  data: 0.002104  max mem: 4109
I20241204 10:55:36 2567764 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.989535  data: 0.002024  max mem: 4109
I20241204 10:56:16 2567764 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.989708  data: 0.001217  max mem: 4109
I20241204 10:56:56 2567764 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.989733  data: 0.001187  max mem: 4109
I20241204 10:57:36 2567764 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.986897  data: 0.001503  max mem: 4109
I20241204 10:58:16 2567764 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.982347  data: 0.001322  max mem: 4109
I20241204 10:58:55 2567764 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.981453  data: 0.001829  max mem: 4109
I20241204 10:59:35 2567764 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.982280  data: 0.002011  max mem: 4109
I20241204 11:00:15 2567764 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.985049  data: 0.001744  max mem: 4109
I20241204 11:00:53 2567764 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.913484  data: 0.001867  max mem: 4109
I20241204 11:01:11 2567764 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.170334  data: 0.001709  max mem: 4109
I20241204 11:01:11 2567764 dinov2 helpers.py:130]  Total time: 0:42:39 (4.036804 s / it)
I20241204 11:01:11 2567764 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:01:11 2567764 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:11 2567764 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:11 2567764 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:11 2567764 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:11 2567764 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:12 2567764 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:26 2567764 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:18:00    time: 13.849030  data: 10.854595  max mem: 4109
I20241204 11:02:04 2567764 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:21    time: 4.728251  data: 0.989726  max mem: 4109
I20241204 11:02:45 2567764 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:14    time: 3.917035  data: 0.005618  max mem: 4109
I20241204 11:03:25 2567764 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:25    time: 4.021289  data: 0.008178  max mem: 4109
I20241204 11:04:05 2567764 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:39    time: 4.010627  data: 0.006503  max mem: 4109
I20241204 11:04:45 2567764 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:56    time: 3.993792  data: 0.006730  max mem: 4109
I20241204 11:05:25 2567764 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:14    time: 3.999975  data: 0.007754  max mem: 4109
I20241204 11:06:00 2567764 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:32    time: 3.781650  data: 0.004334  max mem: 4109
I20241204 11:06:14 2567764 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.038847  data: 0.001881  max mem: 4109
I20241204 11:06:14 2567764 dinov2 helpers.py:130] Test: Total time: 0:05:01 (3.861040 s / it)
I20241204 11:06:14 2567764 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:14 2567764 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:14 2567764 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:14 2567764 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:14 2567764 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:14,639) - Job completed successfully
I20241204 11:06:14 2567764 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:14,641) - Exiting after successful completion
I20241204 11:06:14 2567764 submitit submission.py:61] Exiting after successful completion
