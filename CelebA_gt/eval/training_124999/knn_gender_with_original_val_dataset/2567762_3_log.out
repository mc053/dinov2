submitit INFO (2024-12-04 10:17:37,608) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-04 10:17:37,608) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567766 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567766 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567766 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567766 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567766 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:20 2567766 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:25 2567766 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:25 2567766 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:30 2567766 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:30 2567766 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:31 2567766 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:31 2567766 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:31 2567766 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:31 2567766 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:31 2567766 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:31 2567766 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:08 2567766 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:08 2567766 dinov2 helpers.py:102]   [  0/634]  eta: 6:25:22    time: 36.470161  data: 16.507523  max mem: 3463
I20241204 10:19:24 2567766 dinov2 helpers.py:102]   [ 10/634]  eta: 0:49:34    time: 4.766097  data: 1.502056  max mem: 4109
I20241204 10:20:02 2567766 dinov2 helpers.py:102]   [ 20/634]  eta: 0:44:05    time: 2.701017  data: 0.001105  max mem: 4109
I20241204 10:20:41 2567766 dinov2 helpers.py:102]   [ 30/634]  eta: 0:42:08    time: 3.867385  data: 0.001315  max mem: 4109
I20241204 10:21:21 2567766 dinov2 helpers.py:102]   [ 40/634]  eta: 0:40:54    time: 3.946605  data: 0.002658  max mem: 4109
I20241204 10:22:00 2567766 dinov2 helpers.py:102]   [ 50/634]  eta: 0:39:54    time: 3.968614  data: 0.002221  max mem: 4109
I20241204 10:22:40 2567766 dinov2 helpers.py:102]   [ 60/634]  eta: 0:39:01    time: 3.973149  data: 0.000848  max mem: 4109
I20241204 10:23:20 2567766 dinov2 helpers.py:102]   [ 70/634]  eta: 0:38:13    time: 3.979430  data: 0.000750  max mem: 4109
I20241204 10:24:00 2567766 dinov2 helpers.py:102]   [ 80/634]  eta: 0:37:26    time: 3.982190  data: 0.001673  max mem: 4109
I20241204 10:24:40 2567766 dinov2 helpers.py:102]   [ 90/634]  eta: 0:36:42    time: 3.981260  data: 0.001733  max mem: 4109
I20241204 10:25:19 2567766 dinov2 helpers.py:102]   [100/634]  eta: 0:35:57    time: 3.978861  data: 0.001315  max mem: 4109
I20241204 10:25:59 2567766 dinov2 helpers.py:102]   [110/634]  eta: 0:35:14    time: 3.980661  data: 0.001549  max mem: 4109
I20241204 10:26:39 2567766 dinov2 helpers.py:102]   [120/634]  eta: 0:34:32    time: 3.983102  data: 0.001386  max mem: 4109
I20241204 10:27:19 2567766 dinov2 helpers.py:102]   [130/634]  eta: 0:33:49    time: 3.979474  data: 0.000988  max mem: 4109
I20241204 10:27:59 2567766 dinov2 helpers.py:102]   [140/634]  eta: 0:33:07    time: 3.980482  data: 0.000910  max mem: 4109
I20241204 10:28:38 2567766 dinov2 helpers.py:102]   [150/634]  eta: 0:32:26    time: 3.980632  data: 0.001116  max mem: 4109
I20241204 10:29:18 2567766 dinov2 helpers.py:102]   [160/634]  eta: 0:31:44    time: 3.977176  data: 0.001427  max mem: 4109
I20241204 10:29:58 2567766 dinov2 helpers.py:102]   [170/634]  eta: 0:31:03    time: 3.977790  data: 0.001467  max mem: 4109
I20241204 10:30:38 2567766 dinov2 helpers.py:102]   [180/634]  eta: 0:30:22    time: 3.980536  data: 0.001471  max mem: 4109
I20241204 10:31:18 2567766 dinov2 helpers.py:102]   [190/634]  eta: 0:29:41    time: 3.979537  data: 0.002142  max mem: 4109
I20241204 10:31:57 2567766 dinov2 helpers.py:102]   [200/634]  eta: 0:29:00    time: 3.977726  data: 0.001864  max mem: 4109
I20241204 10:32:37 2567766 dinov2 helpers.py:102]   [210/634]  eta: 0:28:19    time: 3.976974  data: 0.000957  max mem: 4109
I20241204 10:33:17 2567766 dinov2 helpers.py:102]   [220/634]  eta: 0:27:39    time: 3.977745  data: 0.000606  max mem: 4109
I20241204 10:33:57 2567766 dinov2 helpers.py:102]   [230/634]  eta: 0:26:58    time: 3.980659  data: 0.001009  max mem: 4109
I20241204 10:34:36 2567766 dinov2 helpers.py:102]   [240/634]  eta: 0:26:17    time: 3.978604  data: 0.001299  max mem: 4109
I20241204 10:35:16 2567766 dinov2 helpers.py:102]   [250/634]  eta: 0:25:37    time: 3.975464  data: 0.001079  max mem: 4109
I20241204 10:35:56 2567766 dinov2 helpers.py:102]   [260/634]  eta: 0:24:57    time: 3.975517  data: 0.001389  max mem: 4109
I20241204 10:36:36 2567766 dinov2 helpers.py:102]   [270/634]  eta: 0:24:16    time: 3.976731  data: 0.001668  max mem: 4109
I20241204 10:37:16 2567766 dinov2 helpers.py:102]   [280/634]  eta: 0:23:36    time: 3.979632  data: 0.001130  max mem: 4109
I20241204 10:37:55 2567766 dinov2 helpers.py:102]   [290/634]  eta: 0:22:56    time: 3.978644  data: 0.001364  max mem: 4109
I20241204 10:38:35 2567766 dinov2 helpers.py:102]   [300/634]  eta: 0:22:15    time: 3.983085  data: 0.002176  max mem: 4109
I20241204 10:39:15 2567766 dinov2 helpers.py:102]   [310/634]  eta: 0:21:35    time: 3.986783  data: 0.002272  max mem: 4109
I20241204 10:39:55 2567766 dinov2 helpers.py:102]   [320/634]  eta: 0:20:55    time: 3.985755  data: 0.002020  max mem: 4109
I20241204 10:40:35 2567766 dinov2 helpers.py:102]   [330/634]  eta: 0:20:15    time: 3.984950  data: 0.001432  max mem: 4109
I20241204 10:41:15 2567766 dinov2 helpers.py:102]   [340/634]  eta: 0:19:35    time: 3.986344  data: 0.000890  max mem: 4109
I20241204 10:41:54 2567766 dinov2 helpers.py:102]   [350/634]  eta: 0:18:55    time: 3.985021  data: 0.000844  max mem: 4109
I20241204 10:42:34 2567766 dinov2 helpers.py:102]   [360/634]  eta: 0:18:15    time: 3.981952  data: 0.001013  max mem: 4109
I20241204 10:43:14 2567766 dinov2 helpers.py:102]   [370/634]  eta: 0:17:35    time: 3.983243  data: 0.001146  max mem: 4109
I20241204 10:43:54 2567766 dinov2 helpers.py:102]   [380/634]  eta: 0:16:55    time: 3.985053  data: 0.001069  max mem: 4109
I20241204 10:44:34 2567766 dinov2 helpers.py:102]   [390/634]  eta: 0:16:15    time: 3.986863  data: 0.001880  max mem: 4109
I20241204 10:45:14 2567766 dinov2 helpers.py:102]   [400/634]  eta: 0:15:35    time: 3.989688  data: 0.001997  max mem: 4109
I20241204 10:45:54 2567766 dinov2 helpers.py:102]   [410/634]  eta: 0:14:55    time: 3.986169  data: 0.000981  max mem: 4109
I20241204 10:46:33 2567766 dinov2 helpers.py:102]   [420/634]  eta: 0:14:15    time: 3.981616  data: 0.001074  max mem: 4109
I20241204 10:47:13 2567766 dinov2 helpers.py:102]   [430/634]  eta: 0:13:35    time: 3.985060  data: 0.000949  max mem: 4109
I20241204 10:47:53 2567766 dinov2 helpers.py:102]   [440/634]  eta: 0:12:55    time: 3.984248  data: 0.000757  max mem: 4109
I20241204 10:48:33 2567766 dinov2 helpers.py:102]   [450/634]  eta: 0:12:15    time: 3.980536  data: 0.001294  max mem: 4109
I20241204 10:49:13 2567766 dinov2 helpers.py:102]   [460/634]  eta: 0:11:35    time: 3.978154  data: 0.002408  max mem: 4109
I20241204 10:49:52 2567766 dinov2 helpers.py:102]   [470/634]  eta: 0:10:55    time: 3.978539  data: 0.003076  max mem: 4109
I20241204 10:50:32 2567766 dinov2 helpers.py:102]   [480/634]  eta: 0:10:15    time: 3.978692  data: 0.002263  max mem: 4109
I20241204 10:51:12 2567766 dinov2 helpers.py:102]   [490/634]  eta: 0:09:35    time: 3.981153  data: 0.001204  max mem: 4109
I20241204 10:51:52 2567766 dinov2 helpers.py:102]   [500/634]  eta: 0:08:55    time: 3.982257  data: 0.001162  max mem: 4109
I20241204 10:52:32 2567766 dinov2 helpers.py:102]   [510/634]  eta: 0:08:15    time: 3.981558  data: 0.001534  max mem: 4109
I20241204 10:53:12 2567766 dinov2 helpers.py:102]   [520/634]  eta: 0:07:35    time: 3.984346  data: 0.001261  max mem: 4109
I20241204 10:53:51 2567766 dinov2 helpers.py:102]   [530/634]  eta: 0:06:55    time: 3.987622  data: 0.000925  max mem: 4109
I20241204 10:54:31 2567766 dinov2 helpers.py:102]   [540/634]  eta: 0:06:15    time: 3.990321  data: 0.001061  max mem: 4109
I20241204 10:55:11 2567766 dinov2 helpers.py:102]   [550/634]  eta: 0:05:35    time: 3.991460  data: 0.001334  max mem: 4109
I20241204 10:55:51 2567766 dinov2 helpers.py:102]   [560/634]  eta: 0:04:55    time: 3.989634  data: 0.001420  max mem: 4109
I20241204 10:56:31 2567766 dinov2 helpers.py:102]   [570/634]  eta: 0:04:15    time: 3.986963  data: 0.001159  max mem: 4109
I20241204 10:57:11 2567766 dinov2 helpers.py:102]   [580/634]  eta: 0:03:35    time: 3.988777  data: 0.000831  max mem: 4109
I20241204 10:57:51 2567766 dinov2 helpers.py:102]   [590/634]  eta: 0:02:55    time: 3.990446  data: 0.000660  max mem: 4109
I20241204 10:58:31 2567766 dinov2 helpers.py:102]   [600/634]  eta: 0:02:15    time: 3.987801  data: 0.001816  max mem: 4109
I20241204 10:59:10 2567766 dinov2 helpers.py:102]   [610/634]  eta: 0:01:35    time: 3.981495  data: 0.002110  max mem: 4109
I20241204 10:59:50 2567766 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.980524  data: 0.001376  max mem: 4109
I20241204 11:00:30 2567766 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.981661  data: 0.001666  max mem: 4109
I20241204 11:00:49 2567766 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.321207  data: 0.001478  max mem: 4109
I20241204 11:00:49 2567766 dinov2 helpers.py:130]  Total time: 0:42:17 (4.003124 s / it)
I20241204 11:00:49 2567766 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:00:49 2567766 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:00:50 2567766 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:00:50 2567766 dinov2 loaders.py:151] sampler: distributed
I20241204 11:00:50 2567766 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:00:50 2567766 dinov2 loaders.py:223] # of batches: 78
I20241204 11:00:50 2567766 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:00:59 2567766 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:01    time: 8.482934  data: 4.437750  max mem: 4109
I20241204 11:01:30 2567766 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:02    time: 3.562334  data: 0.408381  max mem: 4109
I20241204 11:02:09 2567766 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:35    time: 3.482175  data: 0.007009  max mem: 4109
I20241204 11:02:49 2567766 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:02    time: 3.946615  data: 0.007710  max mem: 4109
I20241204 11:03:29 2567766 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:26    time: 4.010653  data: 0.007232  max mem: 4109
I20241204 11:04:09 2567766 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:48    time: 4.010630  data: 0.006670  max mem: 4109
I20241204 11:04:49 2567766 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 4.003793  data: 0.005757  max mem: 4109
I20241204 11:05:29 2567766 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 4.000634  data: 0.004185  max mem: 4109
I20241204 11:05:53 2567766 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.790541  data: 0.001939  max mem: 4109
I20241204 11:05:53 2567766 dinov2 helpers.py:130] Test: Total time: 0:05:02 (3.875221 s / it)
I20241204 11:05:53 2567766 dinov2 utils.py:79] Averaged stats: 
I20241204 11:05:54 2567766 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:05:54 2567766 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:05:54 2567766 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:05:54 2567766 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:05:55,181) - Job completed successfully
I20241204 11:05:55 2567766 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:05:55,194) - Exiting after successful completion
I20241204 11:05:55 2567766 submitit submission.py:61] Exiting after successful completion
