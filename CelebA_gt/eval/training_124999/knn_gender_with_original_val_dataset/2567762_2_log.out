submitit INFO (2024-12-04 10:17:37,552) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=2(8), node=0(1), global_rank=2(8))
submitit INFO (2024-12-04 10:17:37,552) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:45 2567765 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:45 2567765 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:45 2567765 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:45 2567765 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:45 2567765 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:16 2567765 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:21 2567765 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:21 2567765 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:29 2567765 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:29 2567765 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:31 2567765 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:31 2567765 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:31 2567765 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:31 2567765 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:31 2567765 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:31 2567765 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:13 2567765 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:13 2567765 dinov2 helpers.py:102]   [  0/634]  eta: 7:21:01    time: 41.737804  data: 23.863251  max mem: 3463
I20241204 10:19:40 2567765 dinov2 helpers.py:102]   [ 10/634]  eta: 1:04:27    time: 6.198268  data: 2.172335  max mem: 4109
I20241204 10:20:19 2567765 dinov2 helpers.py:102]   [ 20/634]  eta: 0:52:14    time: 3.272957  data: 0.002191  max mem: 4109
I20241204 10:20:58 2567765 dinov2 helpers.py:102]   [ 30/634]  eta: 0:47:37    time: 3.924309  data: 0.000844  max mem: 4109
I20241204 10:21:38 2567765 dinov2 helpers.py:102]   [ 40/634]  eta: 0:44:59    time: 3.956941  data: 0.000587  max mem: 4109
I20241204 10:22:17 2567765 dinov2 helpers.py:102]   [ 50/634]  eta: 0:43:08    time: 3.970003  data: 0.000768  max mem: 4109
I20241204 10:22:57 2567765 dinov2 helpers.py:102]   [ 60/634]  eta: 0:41:41    time: 3.975288  data: 0.000844  max mem: 4109
I20241204 10:23:37 2567765 dinov2 helpers.py:102]   [ 70/634]  eta: 0:40:27    time: 3.975843  data: 0.001057  max mem: 4109
I20241204 10:24:17 2567765 dinov2 helpers.py:102]   [ 80/634]  eta: 0:39:22    time: 3.980623  data: 0.000973  max mem: 4109
I20241204 10:24:57 2567765 dinov2 helpers.py:102]   [ 90/634]  eta: 0:38:23    time: 3.985008  data: 0.000924  max mem: 4109
I20241204 10:25:36 2567765 dinov2 helpers.py:102]   [100/634]  eta: 0:37:27    time: 3.980389  data: 0.000905  max mem: 4109
I20241204 10:26:16 2567765 dinov2 helpers.py:102]   [110/634]  eta: 0:36:34    time: 3.975976  data: 0.000713  max mem: 4109
I20241204 10:26:56 2567765 dinov2 helpers.py:102]   [120/634]  eta: 0:35:43    time: 3.976006  data: 0.000955  max mem: 4109
I20241204 10:27:36 2567765 dinov2 helpers.py:102]   [130/634]  eta: 0:34:54    time: 3.976063  data: 0.000935  max mem: 4109
I20241204 10:28:15 2567765 dinov2 helpers.py:102]   [140/634]  eta: 0:34:06    time: 3.974367  data: 0.000968  max mem: 4109
I20241204 10:28:55 2567765 dinov2 helpers.py:102]   [150/634]  eta: 0:33:19    time: 3.977027  data: 0.000875  max mem: 4109
I20241204 10:29:35 2567765 dinov2 helpers.py:102]   [160/634]  eta: 0:32:33    time: 3.978743  data: 0.000747  max mem: 4109
I20241204 10:30:15 2567765 dinov2 helpers.py:102]   [170/634]  eta: 0:31:48    time: 3.977678  data: 0.001016  max mem: 4109
I20241204 10:30:55 2567765 dinov2 helpers.py:102]   [180/634]  eta: 0:31:04    time: 3.979460  data: 0.001242  max mem: 4109
I20241204 10:31:34 2567765 dinov2 helpers.py:102]   [190/634]  eta: 0:30:20    time: 3.979638  data: 0.001057  max mem: 4109
I20241204 10:32:14 2567765 dinov2 helpers.py:102]   [200/634]  eta: 0:29:36    time: 3.976952  data: 0.000815  max mem: 4109
I20241204 10:32:54 2567765 dinov2 helpers.py:102]   [210/634]  eta: 0:28:53    time: 3.977801  data: 0.000821  max mem: 4109
I20241204 10:33:34 2567765 dinov2 helpers.py:102]   [220/634]  eta: 0:28:10    time: 3.979718  data: 0.000778  max mem: 4109
I20241204 10:34:14 2567765 dinov2 helpers.py:102]   [230/634]  eta: 0:27:27    time: 3.976985  data: 0.000877  max mem: 4109
I20241204 10:34:53 2567765 dinov2 helpers.py:102]   [240/634]  eta: 0:26:45    time: 3.975722  data: 0.001004  max mem: 4109
I20241204 10:35:33 2567765 dinov2 helpers.py:102]   [250/634]  eta: 0:26:02    time: 3.976487  data: 0.001082  max mem: 4109
I20241204 10:36:13 2567765 dinov2 helpers.py:102]   [260/634]  eta: 0:25:20    time: 3.976627  data: 0.001699  max mem: 4109
I20241204 10:36:53 2567765 dinov2 helpers.py:102]   [270/634]  eta: 0:24:39    time: 3.979528  data: 0.002532  max mem: 4109
I20241204 10:37:32 2567765 dinov2 helpers.py:102]   [280/634]  eta: 0:23:57    time: 3.979556  data: 0.001837  max mem: 4109
I20241204 10:38:12 2567765 dinov2 helpers.py:102]   [290/634]  eta: 0:23:15    time: 3.977625  data: 0.001745  max mem: 4109
I20241204 10:38:52 2567765 dinov2 helpers.py:102]   [300/634]  eta: 0:22:34    time: 3.978595  data: 0.001693  max mem: 4109
I20241204 10:39:32 2567765 dinov2 helpers.py:102]   [310/634]  eta: 0:21:53    time: 3.982264  data: 0.000740  max mem: 4109
I20241204 10:40:12 2567765 dinov2 helpers.py:102]   [320/634]  eta: 0:21:11    time: 3.983976  data: 0.001016  max mem: 4109
I20241204 10:40:52 2567765 dinov2 helpers.py:102]   [330/634]  eta: 0:20:30    time: 3.985912  data: 0.001402  max mem: 4109
I20241204 10:41:31 2567765 dinov2 helpers.py:102]   [340/634]  eta: 0:19:49    time: 3.985076  data: 0.001872  max mem: 4109
I20241204 10:42:11 2567765 dinov2 helpers.py:102]   [350/634]  eta: 0:19:08    time: 3.981464  data: 0.001660  max mem: 4109
I20241204 10:42:51 2567765 dinov2 helpers.py:102]   [360/634]  eta: 0:18:27    time: 3.985147  data: 0.001259  max mem: 4109
I20241204 10:43:31 2567765 dinov2 helpers.py:102]   [370/634]  eta: 0:17:47    time: 3.987762  data: 0.001167  max mem: 4109
I20241204 10:44:11 2567765 dinov2 helpers.py:102]   [380/634]  eta: 0:17:06    time: 3.984093  data: 0.000847  max mem: 4109
I20241204 10:44:51 2567765 dinov2 helpers.py:102]   [390/634]  eta: 0:16:25    time: 3.985064  data: 0.000725  max mem: 4109
I20241204 10:45:30 2567765 dinov2 helpers.py:102]   [400/634]  eta: 0:15:44    time: 3.981613  data: 0.000661  max mem: 4109
I20241204 10:46:10 2567765 dinov2 helpers.py:102]   [410/634]  eta: 0:15:04    time: 3.977217  data: 0.000972  max mem: 4109
I20241204 10:46:50 2567765 dinov2 helpers.py:102]   [420/634]  eta: 0:14:23    time: 3.981589  data: 0.000999  max mem: 4109
I20241204 10:47:30 2567765 dinov2 helpers.py:102]   [430/634]  eta: 0:13:42    time: 3.983165  data: 0.000721  max mem: 4109
I20241204 10:48:10 2567765 dinov2 helpers.py:102]   [440/634]  eta: 0:13:02    time: 3.981326  data: 0.001056  max mem: 4109
I20241204 10:48:49 2567765 dinov2 helpers.py:102]   [450/634]  eta: 0:12:21    time: 3.978451  data: 0.001220  max mem: 4109
I20241204 10:49:29 2567765 dinov2 helpers.py:102]   [460/634]  eta: 0:11:41    time: 3.977364  data: 0.002818  max mem: 4109
I20241204 10:50:09 2567765 dinov2 helpers.py:102]   [470/634]  eta: 0:11:00    time: 3.978533  data: 0.002813  max mem: 4109
I20241204 10:50:49 2567765 dinov2 helpers.py:102]   [480/634]  eta: 0:10:20    time: 3.976964  data: 0.001338  max mem: 4109
I20241204 10:51:29 2567765 dinov2 helpers.py:102]   [490/634]  eta: 0:09:39    time: 3.977904  data: 0.001527  max mem: 4109
I20241204 10:52:08 2567765 dinov2 helpers.py:102]   [500/634]  eta: 0:08:59    time: 3.982337  data: 0.001226  max mem: 4109
I20241204 10:52:48 2567765 dinov2 helpers.py:102]   [510/634]  eta: 0:08:19    time: 3.986803  data: 0.001803  max mem: 4109
I20241204 10:53:28 2567765 dinov2 helpers.py:102]   [520/634]  eta: 0:07:38    time: 3.984208  data: 0.002478  max mem: 4109
I20241204 10:54:08 2567765 dinov2 helpers.py:102]   [530/634]  eta: 0:06:58    time: 3.983234  data: 0.001790  max mem: 4109
I20241204 10:54:48 2567765 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.990428  data: 0.001256  max mem: 4109
I20241204 10:55:28 2567765 dinov2 helpers.py:102]   [550/634]  eta: 0:05:37    time: 3.991426  data: 0.001605  max mem: 4109
I20241204 10:56:08 2567765 dinov2 helpers.py:102]   [560/634]  eta: 0:04:57    time: 3.989766  data: 0.001402  max mem: 4109
I20241204 10:56:48 2567765 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.989688  data: 0.000871  max mem: 4109
I20241204 10:57:27 2567765 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.989538  data: 0.000919  max mem: 4109
I20241204 10:58:07 2567765 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.987013  data: 0.000996  max mem: 4109
I20241204 10:58:47 2567765 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.985061  data: 0.001011  max mem: 4109
I20241204 10:59:27 2567765 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.981308  data: 0.000806  max mem: 4109
I20241204 11:00:07 2567765 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.980520  data: 0.000563  max mem: 4109
I20241204 11:00:46 2567765 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.942874  data: 0.000813  max mem: 4109
I20241204 11:01:04 2567765 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.254066  data: 0.000767  max mem: 4109
I20241204 11:01:04 2567765 dinov2 helpers.py:130]  Total time: 0:42:33 (4.026920 s / it)
I20241204 11:01:04 2567765 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:01:04 2567765 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:05 2567765 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:05 2567765 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:05 2567765 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:05 2567765 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:05 2567765 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:16 2567765 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:13:18    time: 10.235083  data: 7.183554  max mem: 4109
I20241204 11:01:50 2567765 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:34    time: 4.037114  data: 0.662782  max mem: 4109
I20241204 11:02:30 2567765 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:52    time: 3.703632  data: 0.008727  max mem: 4109
I20241204 11:03:10 2567765 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:12    time: 3.984659  data: 0.005567  max mem: 4109
I20241204 11:03:50 2567765 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:31    time: 3.983709  data: 0.004528  max mem: 4109
I20241204 11:04:30 2567765 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:51    time: 3.990926  data: 0.007538  max mem: 4109
I20241204 11:05:10 2567765 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 3.999852  data: 0.013379  max mem: 4109
I20241204 11:05:48 2567765 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 3.913190  data: 0.009558  max mem: 4109
I20241204 11:06:07 2567765 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.462804  data: 0.002681  max mem: 4109
I20241204 11:06:07 2567765 dinov2 helpers.py:130] Test: Total time: 0:05:01 (3.862584 s / it)
I20241204 11:06:07 2567765 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:08 2567765 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:08 2567765 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:08 2567765 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:08 2567765 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:08,728) - Job completed successfully
I20241204 11:06:08 2567765 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:08,741) - Exiting after successful completion
I20241204 11:06:08 2567765 submitit submission.py:61] Exiting after successful completion
