submitit INFO (2024-12-03 07:39:19,661) - Starting with JobEnvironment(job_id=1983655, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-03 07:39:19,662) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn/1983655_submitted.pkl
I20241203 07:39:27 1983659 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 07:39:27 1983659 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 07:39:27 1983659 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 07:39:27 1983659 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 07:39:27 1983659 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 07:39:59 1983659 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 07:40:05 1983659 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 07:40:05 1983659 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 07:40:15 1983659 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 07:40:15 1983659 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 07:40:18 1983659 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 07:40:18 1983659 dinov2 knn.py:260] Extracting features for train set...
I20241203 07:40:18 1983659 dinov2 loaders.py:151] sampler: distributed
I20241203 07:40:18 1983659 dinov2 loaders.py:210] using PyTorch data loader
W20241203 07:40:18 1983659 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 07:40:18 1983659 dinov2 loaders.py:223] # of batches: 634
I20241203 07:41:02 1983659 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 07:41:02 1983659 dinov2 helpers.py:102]   [  0/634]  eta: 7:46:03    time: 44.106415  data: 14.486715  max mem: 3463
I20241203 07:41:32 1983659 dinov2 helpers.py:102]   [ 10/634]  eta: 1:09:55    time: 6.724230  data: 1.319361  max mem: 4109
I20241203 07:42:11 1983659 dinov2 helpers.py:102]   [ 20/634]  eta: 0:55:02    time: 3.441629  data: 0.001626  max mem: 4109
I20241203 07:42:50 1983659 dinov2 helpers.py:102]   [ 30/634]  eta: 0:49:26    time: 3.913167  data: 0.000909  max mem: 4109
I20241203 07:43:30 1983659 dinov2 helpers.py:102]   [ 40/634]  eta: 0:46:17    time: 3.938562  data: 0.000897  max mem: 4109
I20241203 07:44:09 1983659 dinov2 helpers.py:102]   [ 50/634]  eta: 0:44:08    time: 3.950919  data: 0.000869  max mem: 4109
I20241203 07:44:49 1983659 dinov2 helpers.py:102]   [ 60/634]  eta: 0:42:28    time: 3.956132  data: 0.000992  max mem: 4109
I20241203 07:45:28 1983659 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:05    time: 3.959257  data: 0.002012  max mem: 4109
I20241203 07:46:08 1983659 dinov2 helpers.py:102]   [ 80/634]  eta: 0:39:54    time: 3.964885  data: 0.002370  max mem: 4109
I20241203 07:46:48 1983659 dinov2 helpers.py:102]   [ 90/634]  eta: 0:38:50    time: 3.973418  data: 0.001497  max mem: 4109
I20241203 07:47:28 1983659 dinov2 helpers.py:102]   [100/634]  eta: 0:37:51    time: 3.975779  data: 0.001163  max mem: 4109
I20241203 07:48:08 1983659 dinov2 helpers.py:102]   [110/634]  eta: 0:36:56    time: 3.977065  data: 0.001715  max mem: 4109
I20241203 07:48:47 1983659 dinov2 helpers.py:102]   [120/634]  eta: 0:36:03    time: 3.977203  data: 0.002569  max mem: 4109
I20241203 07:49:27 1983659 dinov2 helpers.py:102]   [130/634]  eta: 0:35:11    time: 3.973908  data: 0.001942  max mem: 4109
I20241203 07:50:07 1983659 dinov2 helpers.py:102]   [140/634]  eta: 0:34:22    time: 3.970944  data: 0.001033  max mem: 4109
I20241203 07:50:46 1983659 dinov2 helpers.py:102]   [150/634]  eta: 0:33:33    time: 3.966890  data: 0.001920  max mem: 4109
I20241203 07:51:26 1983659 dinov2 helpers.py:102]   [160/634]  eta: 0:32:46    time: 3.963031  data: 0.001921  max mem: 4109
I20241203 07:52:05 1983659 dinov2 helpers.py:102]   [170/634]  eta: 0:31:59    time: 3.957182  data: 0.000652  max mem: 4109
I20241203 07:52:45 1983659 dinov2 helpers.py:102]   [180/634]  eta: 0:31:13    time: 3.957968  data: 0.000548  max mem: 4109
I20241203 07:53:25 1983659 dinov2 helpers.py:102]   [190/634]  eta: 0:30:28    time: 3.965587  data: 0.000925  max mem: 4109
I20241203 07:54:04 1983659 dinov2 helpers.py:102]   [200/634]  eta: 0:29:44    time: 3.969697  data: 0.001106  max mem: 4109
I20241203 07:54:44 1983659 dinov2 helpers.py:102]   [210/634]  eta: 0:29:00    time: 3.971826  data: 0.000926  max mem: 4109
I20241203 07:55:24 1983659 dinov2 helpers.py:102]   [220/634]  eta: 0:28:16    time: 3.973075  data: 0.000954  max mem: 4109
I20241203 07:56:04 1983659 dinov2 helpers.py:102]   [230/634]  eta: 0:27:33    time: 3.973449  data: 0.000924  max mem: 4109
I20241203 07:56:43 1983659 dinov2 helpers.py:102]   [240/634]  eta: 0:26:50    time: 3.973438  data: 0.001932  max mem: 4109
I20241203 07:57:23 1983659 dinov2 helpers.py:102]   [250/634]  eta: 0:26:08    time: 3.973527  data: 0.002764  max mem: 4109
I20241203 07:58:03 1983659 dinov2 helpers.py:102]   [260/634]  eta: 0:25:25    time: 3.973711  data: 0.001665  max mem: 4109
I20241203 07:58:43 1983659 dinov2 helpers.py:102]   [270/634]  eta: 0:24:43    time: 3.973632  data: 0.001551  max mem: 4109
I20241203 07:59:22 1983659 dinov2 helpers.py:102]   [280/634]  eta: 0:24:01    time: 3.973601  data: 0.001566  max mem: 4109
I20241203 08:00:02 1983659 dinov2 helpers.py:102]   [290/634]  eta: 0:23:19    time: 3.973732  data: 0.000962  max mem: 4109
I20241203 08:00:42 1983659 dinov2 helpers.py:102]   [300/634]  eta: 0:22:37    time: 3.974756  data: 0.001007  max mem: 4109
I20241203 08:01:22 1983659 dinov2 helpers.py:102]   [310/634]  eta: 0:21:56    time: 3.975662  data: 0.001066  max mem: 4109
I20241203 08:02:01 1983659 dinov2 helpers.py:102]   [320/634]  eta: 0:21:14    time: 3.976632  data: 0.000993  max mem: 4109
I20241203 08:02:41 1983659 dinov2 helpers.py:102]   [330/634]  eta: 0:20:33    time: 3.975889  data: 0.000759  max mem: 4109
I20241203 08:03:21 1983659 dinov2 helpers.py:102]   [340/634]  eta: 0:19:52    time: 3.974274  data: 0.000726  max mem: 4109
I20241203 08:04:01 1983659 dinov2 helpers.py:102]   [350/634]  eta: 0:19:11    time: 3.976087  data: 0.000695  max mem: 4109
I20241203 08:04:40 1983659 dinov2 helpers.py:102]   [360/634]  eta: 0:18:29    time: 3.975969  data: 0.000762  max mem: 4109
I20241203 08:05:20 1983659 dinov2 helpers.py:102]   [370/634]  eta: 0:17:48    time: 3.974249  data: 0.000707  max mem: 4109
I20241203 08:06:00 1983659 dinov2 helpers.py:102]   [380/634]  eta: 0:17:07    time: 3.977123  data: 0.001044  max mem: 4109
I20241203 08:06:40 1983659 dinov2 helpers.py:102]   [390/634]  eta: 0:16:27    time: 3.981729  data: 0.001270  max mem: 4109
I20241203 08:07:20 1983659 dinov2 helpers.py:102]   [400/634]  eta: 0:15:46    time: 3.978944  data: 0.001217  max mem: 4109
I20241203 08:07:59 1983659 dinov2 helpers.py:102]   [410/634]  eta: 0:15:05    time: 3.977006  data: 0.000998  max mem: 4109
I20241203 08:08:39 1983659 dinov2 helpers.py:102]   [420/634]  eta: 0:14:24    time: 3.977383  data: 0.000899  max mem: 4109
I20241203 08:09:19 1983659 dinov2 helpers.py:102]   [430/634]  eta: 0:13:43    time: 3.977171  data: 0.000918  max mem: 4109
I20241203 08:09:59 1983659 dinov2 helpers.py:102]   [440/634]  eta: 0:13:03    time: 3.978634  data: 0.000782  max mem: 4109
I20241203 08:10:38 1983659 dinov2 helpers.py:102]   [450/634]  eta: 0:12:22    time: 3.976211  data: 0.000921  max mem: 4109
I20241203 08:11:18 1983659 dinov2 helpers.py:102]   [460/634]  eta: 0:11:42    time: 3.976051  data: 0.000790  max mem: 4109
I20241203 08:11:58 1983659 dinov2 helpers.py:102]   [470/634]  eta: 0:11:01    time: 3.977743  data: 0.000894  max mem: 4109
I20241203 08:12:38 1983659 dinov2 helpers.py:102]   [480/634]  eta: 0:10:20    time: 3.975919  data: 0.001302  max mem: 4109
I20241203 08:13:17 1983659 dinov2 helpers.py:102]   [490/634]  eta: 0:09:40    time: 3.976043  data: 0.001119  max mem: 4109
I20241203 08:13:57 1983659 dinov2 helpers.py:102]   [500/634]  eta: 0:09:00    time: 3.976172  data: 0.000613  max mem: 4109
I20241203 08:14:37 1983659 dinov2 helpers.py:102]   [510/634]  eta: 0:08:19    time: 3.976474  data: 0.001751  max mem: 4109
I20241203 08:15:17 1983659 dinov2 helpers.py:102]   [520/634]  eta: 0:07:39    time: 3.976023  data: 0.002234  max mem: 4109
I20241203 08:15:57 1983659 dinov2 helpers.py:102]   [530/634]  eta: 0:06:58    time: 3.975927  data: 0.002359  max mem: 4109
I20241203 08:16:36 1983659 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.976136  data: 0.002056  max mem: 4109
I20241203 08:17:16 1983659 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.975877  data: 0.000953  max mem: 4109
I20241203 08:17:56 1983659 dinov2 helpers.py:102]   [560/634]  eta: 0:04:57    time: 3.980648  data: 0.000811  max mem: 4109
I20241203 08:18:36 1983659 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.978974  data: 0.000782  max mem: 4109
I20241203 08:19:15 1983659 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.974320  data: 0.000956  max mem: 4109
I20241203 08:19:55 1983659 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.973658  data: 0.001225  max mem: 4109
I20241203 08:20:35 1983659 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.973176  data: 0.001665  max mem: 4109
I20241203 08:21:15 1983659 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.972152  data: 0.001261  max mem: 4109
I20241203 08:21:52 1983659 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.854394  data: 0.000678  max mem: 4109
I20241203 08:22:24 1983659 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.464392  data: 0.000596  max mem: 4109
I20241203 08:22:37 1983659 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 3.502174  data: 0.000476  max mem: 4109
I20241203 08:22:37 1983659 dinov2 helpers.py:130]  Total time: 0:42:18 (4.004312 s / it)
I20241203 08:22:37 1983659 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 08:22:37 1983659 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 08:22:37 1983659 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 08:22:37 1983659 dinov2 loaders.py:151] sampler: distributed
I20241203 08:22:37 1983659 dinov2 loaders.py:210] using PyTorch data loader
I20241203 08:22:37 1983659 dinov2 loaders.py:223] # of batches: 78
I20241203 08:22:46 1983659 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 08:22:58,323) - Submitted job triggered an exception
E20241203 08:22:58 1983659 submitit submission.py:68] Submitted job triggered an exception
