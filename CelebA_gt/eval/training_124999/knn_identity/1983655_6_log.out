submitit INFO (2024-12-03 07:39:19,678) - Starting with JobEnvironment(job_id=1983655, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-03 07:39:19,678) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn/1983655_submitted.pkl
I20241203 07:39:27 1983664 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 07:39:27 1983664 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 07:39:27 1983664 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 07:39:27 1983664 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 07:39:27 1983664 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 07:39:59 1983664 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 07:40:05 1983664 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 07:40:05 1983664 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 07:40:15 1983664 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 07:40:15 1983664 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 07:40:19 1983664 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 07:40:19 1983664 dinov2 knn.py:260] Extracting features for train set...
I20241203 07:40:19 1983664 dinov2 loaders.py:151] sampler: distributed
I20241203 07:40:19 1983664 dinov2 loaders.py:210] using PyTorch data loader
W20241203 07:40:19 1983664 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 07:40:19 1983664 dinov2 loaders.py:223] # of batches: 634
I20241203 07:41:08 1983664 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 07:41:08 1983664 dinov2 helpers.py:102]   [  0/634]  eta: 8:41:37    time: 49.364937  data: 19.721922  max mem: 3463
I20241203 07:41:40 1983664 dinov2 helpers.py:102]   [ 10/634]  eta: 1:16:39    time: 7.371559  data: 1.795428  max mem: 4109
I20241203 07:42:19 1983664 dinov2 helpers.py:102]   [ 20/634]  eta: 0:58:32    time: 3.538451  data: 0.001867  max mem: 4109
I20241203 07:42:58 1983664 dinov2 helpers.py:102]   [ 30/634]  eta: 0:51:47    time: 3.920887  data: 0.000825  max mem: 4109
I20241203 07:43:38 1983664 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:03    time: 3.944955  data: 0.000961  max mem: 4109
I20241203 07:44:17 1983664 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:31    time: 3.953337  data: 0.000973  max mem: 4109
I20241203 07:44:57 1983664 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:37    time: 3.955072  data: 0.001474  max mem: 4109
I20241203 07:45:36 1983664 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:04    time: 3.959403  data: 0.001406  max mem: 4109
I20241203 07:46:16 1983664 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:45    time: 3.968182  data: 0.000558  max mem: 4109
I20241203 07:46:56 1983664 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:34    time: 3.974107  data: 0.001028  max mem: 4109
I20241203 07:47:36 1983664 dinov2 helpers.py:102]   [100/634]  eta: 0:38:30    time: 3.974370  data: 0.001370  max mem: 4109
I20241203 07:48:15 1983664 dinov2 helpers.py:102]   [110/634]  eta: 0:37:30    time: 3.974433  data: 0.001262  max mem: 4109
I20241203 07:48:55 1983664 dinov2 helpers.py:102]   [120/634]  eta: 0:36:33    time: 3.974990  data: 0.001675  max mem: 4109
I20241203 07:49:35 1983664 dinov2 helpers.py:102]   [130/634]  eta: 0:35:39    time: 3.974360  data: 0.001660  max mem: 4109
I20241203 07:50:15 1983664 dinov2 helpers.py:102]   [140/634]  eta: 0:34:47    time: 3.972023  data: 0.001770  max mem: 4109
I20241203 07:50:54 1983664 dinov2 helpers.py:102]   [150/634]  eta: 0:33:57    time: 3.968732  data: 0.001589  max mem: 4109
I20241203 07:51:34 1983664 dinov2 helpers.py:102]   [160/634]  eta: 0:33:08    time: 3.966455  data: 0.001076  max mem: 4109
I20241203 07:52:14 1983664 dinov2 helpers.py:102]   [170/634]  eta: 0:32:19    time: 3.965339  data: 0.001384  max mem: 4109
I20241203 07:52:53 1983664 dinov2 helpers.py:102]   [180/634]  eta: 0:31:32    time: 3.966264  data: 0.001246  max mem: 4109
I20241203 07:53:33 1983664 dinov2 helpers.py:102]   [190/634]  eta: 0:30:46    time: 3.968280  data: 0.000817  max mem: 4109
I20241203 07:54:13 1983664 dinov2 helpers.py:102]   [200/634]  eta: 0:30:00    time: 3.969656  data: 0.000762  max mem: 4109
I20241203 07:54:52 1983664 dinov2 helpers.py:102]   [210/634]  eta: 0:29:15    time: 3.971974  data: 0.000702  max mem: 4109
I20241203 07:55:32 1983664 dinov2 helpers.py:102]   [220/634]  eta: 0:28:31    time: 3.973123  data: 0.000754  max mem: 4109
I20241203 07:56:12 1983664 dinov2 helpers.py:102]   [230/634]  eta: 0:27:47    time: 3.973311  data: 0.001411  max mem: 4109
I20241203 07:56:52 1983664 dinov2 helpers.py:102]   [240/634]  eta: 0:27:03    time: 3.973659  data: 0.001603  max mem: 4109
I20241203 07:57:31 1983664 dinov2 helpers.py:102]   [250/634]  eta: 0:26:19    time: 3.973565  data: 0.000982  max mem: 4109
I20241203 07:58:11 1983664 dinov2 helpers.py:102]   [260/634]  eta: 0:25:36    time: 3.973589  data: 0.000762  max mem: 4109
I20241203 07:58:51 1983664 dinov2 helpers.py:102]   [270/634]  eta: 0:24:53    time: 3.973749  data: 0.000842  max mem: 4109
I20241203 07:59:31 1983664 dinov2 helpers.py:102]   [280/634]  eta: 0:24:11    time: 3.973520  data: 0.000944  max mem: 4109
I20241203 08:00:10 1983664 dinov2 helpers.py:102]   [290/634]  eta: 0:23:28    time: 3.975443  data: 0.000941  max mem: 4109
I20241203 08:00:50 1983664 dinov2 helpers.py:102]   [300/634]  eta: 0:22:46    time: 3.975698  data: 0.001698  max mem: 4109
I20241203 08:01:30 1983664 dinov2 helpers.py:102]   [310/634]  eta: 0:22:04    time: 3.974709  data: 0.001789  max mem: 4109
I20241203 08:02:10 1983664 dinov2 helpers.py:102]   [320/634]  eta: 0:21:22    time: 3.975677  data: 0.001008  max mem: 4109
I20241203 08:02:49 1983664 dinov2 helpers.py:102]   [330/634]  eta: 0:20:40    time: 3.976914  data: 0.000967  max mem: 4109
I20241203 08:03:29 1983664 dinov2 helpers.py:102]   [340/634]  eta: 0:19:58    time: 3.976165  data: 0.001299  max mem: 4109
I20241203 08:04:09 1983664 dinov2 helpers.py:102]   [350/634]  eta: 0:19:17    time: 3.974226  data: 0.001975  max mem: 4109
I20241203 08:04:49 1983664 dinov2 helpers.py:102]   [360/634]  eta: 0:18:35    time: 3.975948  data: 0.001666  max mem: 4109
I20241203 08:05:28 1983664 dinov2 helpers.py:102]   [370/634]  eta: 0:17:54    time: 3.976064  data: 0.000945  max mem: 4109
I20241203 08:06:08 1983664 dinov2 helpers.py:102]   [380/634]  eta: 0:17:12    time: 3.974551  data: 0.000951  max mem: 4109
I20241203 08:06:48 1983664 dinov2 helpers.py:102]   [390/634]  eta: 0:16:31    time: 3.974458  data: 0.001734  max mem: 4109
I20241203 08:07:28 1983664 dinov2 helpers.py:102]   [400/634]  eta: 0:15:50    time: 3.977041  data: 0.001803  max mem: 4109
I20241203 08:08:07 1983664 dinov2 helpers.py:102]   [410/634]  eta: 0:15:09    time: 3.977121  data: 0.001704  max mem: 4109
I20241203 08:08:47 1983664 dinov2 helpers.py:102]   [420/634]  eta: 0:14:28    time: 3.976156  data: 0.001433  max mem: 4109
I20241203 08:09:27 1983664 dinov2 helpers.py:102]   [430/634]  eta: 0:13:47    time: 3.976233  data: 0.000795  max mem: 4109
I20241203 08:10:07 1983664 dinov2 helpers.py:102]   [440/634]  eta: 0:13:06    time: 3.977136  data: 0.000903  max mem: 4109
I20241203 08:10:47 1983664 dinov2 helpers.py:102]   [450/634]  eta: 0:12:25    time: 3.978845  data: 0.000759  max mem: 4109
I20241203 08:11:26 1983664 dinov2 helpers.py:102]   [460/634]  eta: 0:11:44    time: 3.975984  data: 0.001529  max mem: 4109
I20241203 08:12:06 1983664 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.975970  data: 0.002015  max mem: 4109
I20241203 08:12:46 1983664 dinov2 helpers.py:102]   [480/634]  eta: 0:10:23    time: 3.977759  data: 0.001084  max mem: 4109
I20241203 08:13:26 1983664 dinov2 helpers.py:102]   [490/634]  eta: 0:09:42    time: 3.976094  data: 0.001048  max mem: 4109
I20241203 08:14:05 1983664 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.976170  data: 0.001868  max mem: 4109
I20241203 08:14:45 1983664 dinov2 helpers.py:102]   [510/634]  eta: 0:08:21    time: 3.976064  data: 0.001710  max mem: 4109
I20241203 08:15:25 1983664 dinov2 helpers.py:102]   [520/634]  eta: 0:07:40    time: 3.976057  data: 0.001055  max mem: 4109
I20241203 08:16:05 1983664 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.977865  data: 0.000929  max mem: 4109
I20241203 08:16:44 1983664 dinov2 helpers.py:102]   [540/634]  eta: 0:06:19    time: 3.978757  data: 0.001110  max mem: 4109
I20241203 08:17:24 1983664 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.978869  data: 0.001333  max mem: 4109
I20241203 08:18:04 1983664 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.977176  data: 0.001109  max mem: 4109
I20241203 08:18:44 1983664 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.978797  data: 0.000710  max mem: 4109
I20241203 08:19:24 1983664 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.977685  data: 0.000856  max mem: 4109
I20241203 08:20:03 1983664 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.972789  data: 0.001118  max mem: 4109
I20241203 08:20:43 1983664 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.972247  data: 0.000953  max mem: 4109
I20241203 08:21:23 1983664 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.972132  data: 0.001004  max mem: 4109
I20241203 08:21:59 1983664 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.807227  data: 0.001098  max mem: 4109
I20241203 08:22:30 1983664 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.366199  data: 0.001044  max mem: 4109
I20241203 08:22:39 1983664 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 3.212859  data: 0.000757  max mem: 4109
I20241203 08:22:39 1983664 dinov2 helpers.py:130]  Total time: 0:42:20 (4.006947 s / it)
I20241203 08:22:39 1983664 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 08:22:39 1983664 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 08:22:39 1983664 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 08:22:39 1983664 dinov2 loaders.py:151] sampler: distributed
I20241203 08:22:39 1983664 dinov2 loaders.py:210] using PyTorch data loader
I20241203 08:22:39 1983664 dinov2 loaders.py:223] # of batches: 78
I20241203 08:22:46 1983664 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 08:22:57,966) - Submitted job triggered an exception
E20241203 08:22:57 1983664 submitit submission.py:68] Submitted job triggered an exception
