submitit INFO (2024-12-04 07:27:40,429) - Starting with JobEnvironment(job_id=2488412, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-04 07:27:40,429) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender/2488412_submitted.pkl
I20241204 07:27:48 2488422 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488422 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488422 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488422 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488422 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:23 2488422 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:28 2488422 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488422 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 07:28:34 2488422 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:41 2488422 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488422 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:41 2488422 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488422 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488422 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:41 2488422 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 07:28:46 2488422 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:46 2488422 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:46 2488422 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:46 2488422 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:46 2488422 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:46 2488422 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:46 2488422 dinov2 linear.py:338] Starting training from iteration 0
lr 4.999999921043166e-06
I20241204 07:29:13 2488422 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 23:09:18  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 27.404659  data: 20.905233  max mem: 2709
I20241204 07:29:15 2488422 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
lr 4.999990446229023e-06
I20241204 07:29:34 2488422 dinov2 helpers.py:102] Training  [   10/12500]  eta: 15:15:47  loss: 28.3534 (31.6803)  lr: 0.0000 (0.0000)  time: 4.399310  data: 1.938053  max mem: 3113
lr 4.999965180116501e-06
I20241204 07:29:55 2488422 dinov2 helpers.py:102] Training  [   20/12500]  eta: 11:27:17  loss: 28.3534 (28.9273)  lr: 0.0000 (0.0000)  time: 2.099283  data: 0.021459  max mem: 3113
lr 4.999924122865191e-06
I20241204 07:30:16 2488422 dinov2 helpers.py:102] Training  [   30/12500]  eta: 10:06:26  loss: 23.4213 (26.6113)  lr: 0.0000 (0.0000)  time: 2.103218  data: 0.001592  max mem: 3113
lr 4.999867274734432e-06
I20241204 07:30:37 2488422 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:25:42  loss: 23.4213 (25.7552)  lr: 0.0000 (0.0000)  time: 2.114882  data: 0.001414  max mem: 3113
lr 4.999794636083308e-06
I20241204 07:30:59 2488422 dinov2 helpers.py:102] Training  [   50/12500]  eta: 9:01:07  loss: 23.4213 (25.5184)  lr: 0.0000 (0.0000)  time: 2.127129  data: 0.001427  max mem: 3113
lr 4.999706207370645e-06
I20241204 07:31:20 2488422 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:44:11  loss: 23.4213 (24.9988)  lr: 0.0000 (0.0000)  time: 2.126867  data: 0.001802  max mem: 3113
lr 4.999601989155004e-06
I20241204 07:31:41 2488422 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:31:58  loss: 22.4366 (24.6785)  lr: 0.0000 (0.0000)  time: 2.123212  data: 0.001879  max mem: 3113
lr 4.999481982094688e-06
I20241204 07:32:03 2488422 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:22:44  loss: 22.7961 (24.4693)  lr: 0.0000 (0.0000)  time: 2.125093  data: 0.001780  max mem: 3113
lr 4.9993461869477276e-06
I20241204 07:32:24 2488422 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:15:26  loss: 22.4366 (24.0474)  lr: 0.0000 (0.0000)  time: 2.125919  data: 0.001653  max mem: 3113
lr 4.999194604571874e-06
I20241204 07:32:45 2488422 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:09:28  loss: 22.4366 (23.2664)  lr: 0.0000 (0.0000)  time: 2.124123  data: 0.001478  max mem: 3113
lr 4.999027235924608e-06
I20241204 07:33:06 2488422 dinov2 helpers.py:102] Training  [  110/12500]  eta: 8:04:31  loss: 22.4366 (23.4682)  lr: 0.0000 (0.0000)  time: 2.123392  data: 0.001374  max mem: 3113
lr 4.998844082063119e-06
I20241204 07:33:27 2488422 dinov2 helpers.py:102] Training  [  120/12500]  eta: 8:00:22  loss: 22.4366 (23.0133)  lr: 0.0000 (0.0000)  time: 2.124752  data: 0.001438  max mem: 3113
lr 4.998645144144304e-06
I20241204 07:33:49 2488422 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:56:47  loss: 22.3304 (22.9422)  lr: 0.0000 (0.0000)  time: 2.125485  data: 0.001984  max mem: 3113
lr 4.998430423424764e-06
I20241204 07:34:10 2488422 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:53:40  loss: 22.4366 (22.9390)  lr: 0.0000 (0.0000)  time: 2.125583  data: 0.002659  max mem: 3113
lr 4.9981999212607945e-06
I20241204 07:34:31 2488422 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:50:55  loss: 22.3304 (22.6978)  lr: 0.0000 (0.0000)  time: 2.125558  data: 0.002031  max mem: 3113
lr 4.997953639108375e-06
I20241204 07:34:53 2488422 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:48:27  loss: 22.3304 (22.4806)  lr: 0.0000 (0.0000)  time: 2.125210  data: 0.001926  max mem: 3113
lr 4.997691578523149e-06
I20241204 07:35:14 2488422 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:46:13  loss: 22.0177 (22.2354)  lr: 0.0000 (0.0000)  time: 2.123846  data: 0.002007  max mem: 3113
lr 4.9974137411604395e-06
I20241204 07:35:35 2488422 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:44:12  loss: 22.0177 (22.0306)  lr: 0.0000 (0.0000)  time: 2.124058  data: 0.002198  max mem: 3113
lr 4.9971201287752166e-06
I20241204 07:35:56 2488422 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:42:22  loss: 21.8810 (21.8644)  lr: 0.0000 (0.0000)  time: 2.124728  data: 0.003168  max mem: 3113
lr 4.996810743222097e-06
I20241204 07:36:17 2488422 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:40:39  loss: 20.2497 (21.6677)  lr: 0.0000 (0.0000)  time: 2.123378  data: 0.002540  max mem: 3113
lr 4.996485586455328e-06
I20241204 07:36:39 2488422 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:39:05  loss: 19.6635 (21.3674)  lr: 0.0000 (0.0000)  time: 2.123581  data: 0.001896  max mem: 3113
lr 4.996144660528775e-06
I20241204 07:37:00 2488422 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:37:37  loss: 19.4132 (21.2824)  lr: 0.0000 (0.0000)  time: 2.123911  data: 0.001987  max mem: 3113
lr 4.99578796759591e-06
I20241204 07:37:21 2488422 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:36:15  loss: 19.0806 (21.1466)  lr: 0.0000 (0.0000)  time: 2.123063  data: 0.001767  max mem: 3113
lr 4.995415509909803e-06
I20241204 07:37:42 2488422 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:34:59  loss: 19.0806 (21.2802)  lr: 0.0000 (0.0000)  time: 2.124565  data: 0.003635  max mem: 3113
lr 4.995027289823097e-06
I20241204 07:38:04 2488422 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:33:47  loss: 19.0053 (21.1557)  lr: 0.0000 (0.0000)  time: 2.125604  data: 0.003456  max mem: 3113
lr 4.9946233097880025e-06
I20241204 07:38:25 2488422 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:32:39  loss: 18.7059 (21.0447)  lr: 0.0000 (0.0000)  time: 2.124530  data: 0.002434  max mem: 3113
lr 4.994203572356276e-06
I20241204 07:38:46 2488422 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:31:34  loss: 18.3436 (20.9234)  lr: 0.0000 (0.0000)  time: 2.124020  data: 0.002979  max mem: 3113
lr 4.9937680801792065e-06
I20241204 07:39:07 2488422 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:30:31  loss: 18.3436 (20.8691)  lr: 0.0000 (0.0000)  time: 2.123629  data: 0.002053  max mem: 3113
lr 4.993316836007601e-06
I20241204 07:39:29 2488422 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:29:33  loss: 18.1600 (20.7686)  lr: 0.0000 (0.0000)  time: 2.124066  data: 0.002213  max mem: 3113
lr 4.992849842691759e-06
I20241204 07:39:50 2488422 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:28:37  loss: 18.1600 (20.6192)  lr: 0.0000 (0.0000)  time: 2.124971  data: 0.002243  max mem: 3113
lr 4.99236710318147e-06
I20241204 07:40:11 2488422 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:27:43  loss: 18.1600 (20.6042)  lr: 0.0000 (0.0000)  time: 2.126120  data: 0.002234  max mem: 3113
lr 4.991868620525976e-06
I20241204 07:40:32 2488422 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:26:51  loss: 18.1600 (20.4089)  lr: 0.0000 (0.0000)  time: 2.125617  data: 0.001985  max mem: 3113
lr 4.991354397873964e-06
I20241204 07:40:54 2488422 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:26:00  loss: 18.0669 (20.2835)  lr: 0.0000 (0.0000)  time: 2.123755  data: 0.001308  max mem: 3113
lr 4.990824438473544e-06
I20241204 07:41:15 2488422 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:25:12  loss: 18.0419 (20.1256)  lr: 0.0000 (0.0000)  time: 2.124204  data: 0.001657  max mem: 3113
lr 4.990278745672229e-06
I20241204 07:41:36 2488422 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:24:25  loss: 18.0243 (20.0453)  lr: 0.0000 (0.0000)  time: 2.124448  data: 0.001770  max mem: 3113
lr 4.98971732291691e-06
I20241204 07:41:57 2488422 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:23:39  loss: 17.8542 (19.9730)  lr: 0.0000 (0.0000)  time: 2.124184  data: 0.001489  max mem: 3113
lr 4.989140173753839e-06
I20241204 07:42:19 2488422 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:22:55  loss: 17.7355 (19.8749)  lr: 0.0000 (0.0000)  time: 2.124307  data: 0.001882  max mem: 3113
lr 4.988547301828603e-06
I20241204 07:42:40 2488422 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:22:12  loss: 17.6483 (19.7530)  lr: 0.0000 (0.0000)  time: 2.124377  data: 0.002012  max mem: 3113
lr 4.987938710886104e-06
I20241204 07:43:01 2488422 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:21:30  loss: 17.6483 (19.7513)  lr: 0.0000 (0.0000)  time: 2.125093  data: 0.001941  max mem: 3113
lr 4.9873144047705305e-06
I20241204 07:43:22 2488422 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:20:49  loss: 17.6483 (19.7418)  lr: 0.0000 (0.0000)  time: 2.124808  data: 0.001712  max mem: 3113
lr 4.986674387425343e-06
I20241204 07:43:44 2488422 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:20:10  loss: 17.6483 (19.6050)  lr: 0.0000 (0.0000)  time: 2.125081  data: 0.001970  max mem: 3113
lr 4.9860186628932356e-06
I20241204 07:44:05 2488422 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:19:30  loss: 17.3709 (19.4991)  lr: 0.0000 (0.0000)  time: 2.124314  data: 0.002697  max mem: 3113
lr 4.985347235316124e-06
I20241204 07:44:26 2488422 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:18:52  loss: 17.3709 (19.4702)  lr: 0.0000 (0.0000)  time: 2.123162  data: 0.003621  max mem: 3113
lr 4.984660108935109e-06
I20241204 07:44:47 2488422 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:18:15  loss: 17.3709 (19.4725)  lr: 0.0000 (0.0000)  time: 2.124297  data: 0.003288  max mem: 3113
lr 4.983957288090453e-06
I20241204 07:45:09 2488422 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:17:38  loss: 17.3709 (19.4830)  lr: 0.0000 (0.0000)  time: 2.124568  data: 0.001695  max mem: 3113
lr 4.9832387772215545e-06
I20241204 07:45:30 2488422 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:17:02  loss: 17.3709 (19.4623)  lr: 0.0000 (0.0000)  time: 2.124040  data: 0.001255  max mem: 3113
lr 4.982504580866918e-06
I20241204 07:45:51 2488422 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:16:26  loss: 17.2319 (19.3538)  lr: 0.0000 (0.0000)  time: 2.123861  data: 0.001219  max mem: 3113
lr 4.981754703664129e-06
I20241204 07:46:12 2488422 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:15:52  loss: 16.2446 (19.2759)  lr: 0.0000 (0.0000)  time: 2.125996  data: 0.002997  max mem: 3113
lr 4.980989150349819e-06
I20241204 07:46:34 2488422 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:15:18  loss: 16.2446 (19.2218)  lr: 0.0000 (0.0000)  time: 2.126049  data: 0.003200  max mem: 3113
lr 4.980207925759636e-06
I20241204 07:46:55 2488422 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:14:44  loss: 16.5730 (19.2194)  lr: 0.0000 (0.0000)  time: 2.124650  data: 0.002102  max mem: 3113
lr 4.979411034828223e-06
I20241204 07:47:16 2488422 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:14:11  loss: 16.5730 (19.2320)  lr: 0.0000 (0.0000)  time: 2.125265  data: 0.002804  max mem: 3113
lr 4.978598482589174e-06
I20241204 07:47:37 2488422 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:13:38  loss: 17.2319 (19.2441)  lr: 0.0000 (0.0000)  time: 2.125145  data: 0.002719  max mem: 3113
lr 4.977770274175011e-06
I20241204 07:47:59 2488422 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:13:06  loss: 17.2348 (19.2069)  lr: 0.0000 (0.0000)  time: 2.125132  data: 0.002170  max mem: 3113
lr 4.97692641481715e-06
I20241204 07:48:20 2488422 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:12:34  loss: 17.3709 (19.2336)  lr: 0.0000 (0.0000)  time: 2.125774  data: 0.001792  max mem: 3113
lr 4.976066909845862e-06
I20241204 07:48:41 2488422 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:12:03  loss: 17.3709 (19.1505)  lr: 0.0000 (0.0000)  time: 2.125552  data: 0.001487  max mem: 3113
lr 4.975191764690249e-06
I20241204 07:49:02 2488422 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:11:32  loss: 18.2288 (19.1649)  lr: 0.0000 (0.0000)  time: 2.124692  data: 0.001694  max mem: 3113
lr 4.974300984878205e-06
I20241204 07:49:24 2488422 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:11:01  loss: 18.5130 (19.2856)  lr: 0.0000 (0.0000)  time: 2.124168  data: 0.002229  max mem: 3113
lr 4.973394576036379e-06
I20241204 07:49:45 2488422 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:10:31  loss: 18.5130 (19.2699)  lr: 0.0000 (0.0000)  time: 2.125081  data: 0.001999  max mem: 3113
lr 4.97247254389014e-06
I20241204 07:50:06 2488422 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:10:00  loss: 18.3581 (19.2352)  lr: 0.0000 (0.0000)  time: 2.125732  data: 0.001559  max mem: 3113
lr 4.9715348942635445e-06
I20241204 07:50:27 2488422 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:09:31  loss: 18.3581 (19.2963)  lr: 0.0000 (0.0000)  time: 2.125588  data: 0.002690  max mem: 3113
lr 4.9705816330792985e-06
I20241204 07:50:49 2488422 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:09:01  loss: 18.3581 (19.2325)  lr: 0.0000 (0.0000)  time: 2.126100  data: 0.002559  max mem: 3113
lr 4.969612766358717e-06
I20241204 07:51:10 2488422 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:08:32  loss: 18.5130 (19.2533)  lr: 0.0000 (0.0000)  time: 2.125848  data: 0.001184  max mem: 3113
lr 4.9686283002216905e-06
I20241204 07:51:31 2488422 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:08:03  loss: 19.0997 (19.2902)  lr: 0.0000 (0.0000)  time: 2.124421  data: 0.002033  max mem: 3113
lr 4.967628240886639e-06
I20241204 07:51:52 2488422 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:07:34  loss: 18.8884 (19.2840)  lr: 0.0000 (0.0000)  time: 2.124512  data: 0.002642  max mem: 3113
lr 4.966612594670483e-06
I20241204 07:52:14 2488422 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:07:05  loss: 18.8884 (19.3079)  lr: 0.0000 (0.0000)  time: 2.124857  data: 0.001684  max mem: 3113
lr 4.965581367988594e-06
I20241204 07:52:35 2488422 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:06:37  loss: 18.8884 (19.2943)  lr: 0.0000 (0.0000)  time: 2.124582  data: 0.001089  max mem: 3113
lr 4.964534567354764e-06
I20241204 07:52:56 2488422 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:06:09  loss: 19.0997 (19.3012)  lr: 0.0000 (0.0000)  time: 2.126217  data: 0.001450  max mem: 3113
lr 4.96347219938115e-06
I20241204 07:53:17 2488422 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:05:41  loss: 19.0997 (19.2080)  lr: 0.0000 (0.0000)  time: 2.125657  data: 0.001576  max mem: 3113
lr 4.96239427077825e-06
I20241204 07:53:39 2488422 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:05:14  loss: 19.0997 (19.1619)  lr: 0.0000 (0.0000)  time: 2.124978  data: 0.002058  max mem: 3113
lr 4.961300788354844e-06
I20241204 07:54:00 2488422 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:04:46  loss: 18.8884 (19.0967)  lr: 0.0000 (0.0000)  time: 2.125606  data: 0.002036  max mem: 3113
lr 4.960191759017962e-06
I20241204 07:54:21 2488422 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:04:19  loss: 18.3949 (19.0409)  lr: 0.0000 (0.0000)  time: 2.125468  data: 0.001722  max mem: 3113
lr 4.959067189772836e-06
I20241204 07:54:42 2488422 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:03:52  loss: 18.3949 (19.0378)  lr: 0.0000 (0.0000)  time: 2.125230  data: 0.002040  max mem: 3113
lr 4.957927087722856e-06
I20241204 07:55:04 2488422 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:03:25  loss: 18.3949 (18.9731)  lr: 0.0000 (0.0000)  time: 2.124940  data: 0.001649  max mem: 3113
lr 4.956771460069526e-06
I20241204 07:55:25 2488422 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:02:57  loss: 18.3581 (18.9568)  lr: 0.0000 (0.0000)  time: 2.123901  data: 0.001241  max mem: 3113
lr 4.95560031411242e-06
I20241204 07:55:46 2488422 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:02:31  loss: 18.3581 (18.8799)  lr: 0.0000 (0.0000)  time: 2.124921  data: 0.001479  max mem: 3113
lr 4.9544136572491304e-06
I20241204 07:56:07 2488422 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:02:04  loss: 17.9727 (18.8682)  lr: 0.0000 (0.0000)  time: 2.126230  data: 0.001830  max mem: 3113
lr 4.953211496975229e-06
I20241204 07:56:29 2488422 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:01:38  loss: 17.7523 (18.8063)  lr: 0.0000 (0.0000)  time: 2.125436  data: 0.001935  max mem: 3113
lr 4.951993840884212e-06
I20241204 07:56:50 2488422 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:01:12  loss: 17.7523 (18.8491)  lr: 0.0000 (0.0000)  time: 2.125435  data: 0.002116  max mem: 3113
lr 4.950760696667457e-06
I20241204 07:57:11 2488422 dinov2 helpers.py:102] Training  [  790/12500]  eta: 7:00:45  loss: 17.9727 (18.8945)  lr: 0.0000 (0.0000)  time: 2.124557  data: 0.001681  max mem: 3113
lr 4.949512072114174e-06
I20241204 07:57:32 2488422 dinov2 helpers.py:102] Training  [  800/12500]  eta: 7:00:20  loss: 17.7523 (18.8601)  lr: 0.0000 (0.0000)  time: 2.125189  data: 0.001277  max mem: 3113
lr 4.948247975111351e-06
I20241204 07:57:54 2488422 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:59:54  loss: 17.9727 (18.8552)  lr: 0.0000 (0.0000)  time: 2.126242  data: 0.001582  max mem: 3113
lr 4.946968413643719e-06
I20241204 07:58:15 2488422 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:59:28  loss: 17.7523 (18.8286)  lr: 0.0000 (0.0000)  time: 2.126453  data: 0.001949  max mem: 3113
lr 4.945673395793676e-06
I20241204 07:58:36 2488422 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:59:03  loss: 17.7523 (18.8413)  lr: 0.0000 (0.0000)  time: 2.128327  data: 0.002406  max mem: 3113
lr 4.9443629297412615e-06
I20241204 07:58:58 2488422 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:58:37  loss: 17.7523 (18.8601)  lr: 0.0000 (0.0000)  time: 2.127198  data: 0.001924  max mem: 3113
lr 4.943037023764093e-06
I20241204 07:59:19 2488422 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:58:12  loss: 16.6466 (18.8289)  lr: 0.0000 (0.0000)  time: 2.125506  data: 0.001295  max mem: 3113
lr 4.941695686237312e-06
I20241204 07:59:40 2488422 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:57:47  loss: 16.6466 (18.8059)  lr: 0.0000 (0.0000)  time: 2.125586  data: 0.001392  max mem: 3113
lr 4.940338925633534e-06
I20241204 08:00:01 2488422 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:57:21  loss: 16.1788 (18.7375)  lr: 0.0000 (0.0000)  time: 2.125877  data: 0.001356  max mem: 3113
lr 4.938966750522798e-06
I20241204 08:00:23 2488422 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:56  loss: 16.1788 (18.6667)  lr: 0.0000 (0.0000)  time: 2.126137  data: 0.001335  max mem: 3113
lr 4.937579169572506e-06
I20241204 08:00:44 2488422 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:56:31  loss: 16.6466 (18.6853)  lr: 0.0000 (0.0000)  time: 2.125532  data: 0.001358  max mem: 3113
lr 4.936176191547377e-06
I20241204 08:01:05 2488422 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:56:06  loss: 16.8232 (18.6990)  lr: 0.0000 (0.0000)  time: 2.125636  data: 0.001345  max mem: 3113
lr 4.934757825309379e-06
I20241204 08:01:26 2488422 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:55:41  loss: 17.7523 (18.6943)  lr: 0.0000 (0.0000)  time: 2.125890  data: 0.001412  max mem: 3113
lr 4.933324079817689e-06
I20241204 08:01:48 2488422 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:55:17  loss: 16.8232 (18.6182)  lr: 0.0000 (0.0000)  time: 2.126326  data: 0.001623  max mem: 3113
lr 4.9318749641286164e-06
I20241204 08:02:09 2488422 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:54:52  loss: 16.8232 (18.5817)  lr: 0.0000 (0.0000)  time: 2.125487  data: 0.001799  max mem: 3113
lr 4.930410487395568e-06
I20241204 08:02:30 2488422 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:54:27  loss: 16.8232 (18.5670)  lr: 0.0000 (0.0000)  time: 2.125721  data: 0.002080  max mem: 3113
lr 4.928930658868971e-06
I20241204 08:02:51 2488422 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:54:02  loss: 16.8232 (18.5277)  lr: 0.0000 (0.0000)  time: 2.126059  data: 0.001841  max mem: 3113
lr 4.927435487896227e-06
I20241204 08:03:13 2488422 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:53:38  loss: 16.6466 (18.4983)  lr: 0.0000 (0.0000)  time: 2.126848  data: 0.001645  max mem: 3113
lr 4.925924983921652e-06
I20241204 08:03:34 2488422 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:53:14  loss: 16.8232 (18.5466)  lr: 0.0000 (0.0000)  time: 2.127993  data: 0.001780  max mem: 3113
lr 4.92439915648641e-06
I20241204 08:03:55 2488422 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:52:50  loss: 16.6466 (18.4968)  lr: 0.0000 (0.0000)  time: 2.126104  data: 0.001545  max mem: 3113
lr 4.922858015228454e-06
I20241204 08:04:16 2488422 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:52:25  loss: 16.1788 (18.4719)  lr: 0.0000 (0.0000)  time: 2.124608  data: 0.001487  max mem: 3113
lr 4.921301569882469e-06
I20241204 08:04:38 2488422 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:52:01  loss: 16.6466 (18.4634)  lr: 0.0000 (0.0000)  time: 2.124159  data: 0.001566  max mem: 3113
lr 4.919729830279811e-06
I20241204 08:04:59 2488422 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:51:36  loss: 16.6466 (18.4552)  lr: 0.0000 (0.0000)  time: 2.124202  data: 0.002062  max mem: 3113
lr 4.918142806348443e-06
I20241204 08:05:20 2488422 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:51:12  loss: 16.1788 (18.4284)  lr: 0.0000 (0.0000)  time: 2.124055  data: 0.002023  max mem: 3113
lr 4.916540508112869e-06
I20241204 08:05:41 2488422 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:48  loss: 16.0023 (18.3703)  lr: 0.0000 (0.0000)  time: 2.123977  data: 0.001552  max mem: 3113
lr 4.914922945694074e-06
I20241204 08:06:03 2488422 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:50:23  loss: 15.9833 (18.3475)  lr: 0.0000 (0.0000)  time: 2.123766  data: 0.001479  max mem: 3113
lr 4.913290129309465e-06
I20241204 08:06:24 2488422 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:59  loss: 15.6870 (18.3086)  lr: 0.0000 (0.0000)  time: 2.123751  data: 0.001409  max mem: 3113
lr 4.911642069272796e-06
I20241204 08:06:45 2488422 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:49:35  loss: 15.6761 (18.2813)  lr: 0.0000 (0.0000)  time: 2.124432  data: 0.001971  max mem: 3113
lr 4.909978775994108e-06
I20241204 08:07:06 2488422 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:49:11  loss: 15.6761 (18.2483)  lr: 0.0000 (0.0000)  time: 2.124128  data: 0.002085  max mem: 3113
lr 4.908300259979668e-06
I20241204 08:07:28 2488422 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:47  loss: 15.6870 (18.2701)  lr: 0.0000 (0.0000)  time: 2.123385  data: 0.002021  max mem: 3113
lr 4.906606531831894e-06
I20241204 08:07:49 2488422 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:48:23  loss: 15.6870 (18.2556)  lr: 0.0000 (0.0000)  time: 2.123274  data: 0.002289  max mem: 3113
lr 4.904897602249294e-06
I20241204 08:08:10 2488422 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:59  loss: 15.6761 (18.2171)  lr: 0.0000 (0.0000)  time: 2.123899  data: 0.002429  max mem: 3113
lr 4.903173482026397e-06
I20241204 08:08:31 2488422 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:47:35  loss: 15.6761 (18.2229)  lr: 0.0000 (0.0000)  time: 2.124267  data: 0.002112  max mem: 3113
lr 4.9014341820536815e-06
I20241204 08:08:53 2488422 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:47:11  loss: 15.6761 (18.1944)  lr: 0.0000 (0.0000)  time: 2.124997  data: 0.001744  max mem: 3113
lr 4.899679713317512e-06
I20241204 08:09:14 2488422 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:48  loss: 15.6870 (18.1905)  lr: 0.0000 (0.0000)  time: 2.125226  data: 0.002116  max mem: 3113
lr 4.897910086900068e-06
I20241204 08:09:35 2488422 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:46:24  loss: 15.6870 (18.2150)  lr: 0.0000 (0.0000)  time: 2.123675  data: 0.003593  max mem: 3113
lr 4.896125313979271e-06
I20241204 08:09:56 2488422 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:46:00  loss: 15.6870 (18.1895)  lr: 0.0000 (0.0000)  time: 2.122740  data: 0.003413  max mem: 3113
lr 4.894325405828717e-06
I20241204 08:10:18 2488422 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:45:36  loss: 15.6870 (18.1641)  lr: 0.0000 (0.0000)  time: 2.122886  data: 0.001923  max mem: 3113
lr 4.8925103738176015e-06
I20241204 08:10:39 2488422 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:45:12  loss: 15.6870 (18.1653)  lr: 0.0000 (0.0000)  time: 2.121962  data: 0.002675  max mem: 3113
lr 4.890680229410655e-06
I20241204 08:11:00 2488422 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:49  loss: 15.6870 (18.1173)  lr: 0.0000 (0.0000)  time: 2.122870  data: 0.002854  max mem: 3113
lr 4.888834984168066e-06
I20241204 08:11:21 2488422 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:44:25  loss: 15.6870 (18.1253)  lr: 0.0000 (0.0000)  time: 2.123422  data: 0.001705  max mem: 3113
lr 4.886974649745406e-06
I20241204 08:11:42 2488422 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:44:02  loss: 15.3930 (18.1002)  lr: 0.0000 (0.0000)  time: 2.122368  data: 0.001177  max mem: 3113
lr 4.885099237893554e-06
I20241204 08:12:04 2488422 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:43:38  loss: 15.3930 (18.1000)  lr: 0.0000 (0.0000)  time: 2.122824  data: 0.001350  max mem: 3113
lr 4.883208760458633e-06
I20241204 08:12:25 2488422 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:43:14  loss: 15.3930 (18.0788)  lr: 0.0000 (0.0000)  time: 2.121798  data: 0.001391  max mem: 3113
lr 4.881303229381928e-06
I20241204 08:12:45 2488422 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:38  loss: 15.4914 (18.0946)  lr: 0.0000 (0.0000)  time: 2.053779  data: 0.002259  max mem: 3113
lr 4.8793826566998085e-06
I20241204 08:12:59 2488422 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:41:12  loss: 15.4914 (18.0996)  lr: 0.0000 (0.0000)  time: 1.705551  data: 0.002176  max mem: 3113
I20241204 08:13:09 2488422 dinov2 linear.py:272] running validation !
submitit ERROR (2024-12-04 08:13:09,226) - Submitted job triggered an exception
E20241204 08:13:09 2488422 submitit submission.py:68] Submitted job triggered an exception
