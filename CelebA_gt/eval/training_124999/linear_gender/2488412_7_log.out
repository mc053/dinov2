submitit INFO (2024-12-04 07:27:40,436) - Starting with JobEnvironment(job_id=2488412, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-04 07:27:40,437) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender/2488412_submitted.pkl
I20241204 07:27:48 2488423 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488423 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488423 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488423 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488423 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:19 2488423 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488423 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:24 2488423 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 07:28:31 2488423 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488423 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:34 2488423 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:34 2488423 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:34 2488423 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:34 2488423 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:34 2488423 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 07:28:36 2488423 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:36 2488423 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:36 2488423 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:36 2488423 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:36 2488423 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:36 2488423 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:36 2488423 dinov2 linear.py:338] Starting training from iteration 0
lr 4.999999921043166e-06
I20241204 07:28:54 2488423 dinov2 helpers.py:102] Training  [    0/12500]  eta: 2 days, 14:25:56  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 17.980488  data: 14.264837  max mem: 2706
I20241204 07:28:54 2488423 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
lr 4.999990446229023e-06
I20241204 07:29:05 2488423 dinov2 helpers.py:102] Training  [   10/12500]  eta: 9:09:04  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 2.637709  data: 1.303654  max mem: 3115
lr 4.999965180116501e-06
I20241204 07:29:18 2488423 dinov2 helpers.py:102] Training  [   20/12500]  eta: 6:50:49  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 1.174842  data: 0.004926  max mem: 3115
lr 4.999924122865191e-06
I20241204 07:29:38 2488423 dinov2 helpers.py:102] Training  [   30/12500]  eta: 6:57:31  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 1.663177  data: 0.003121  max mem: 3115
lr 4.999867274734432e-06
I20241204 07:29:59 2488423 dinov2 helpers.py:102] Training  [   40/12500]  eta: 7:01:35  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.087935  data: 0.004014  max mem: 3115
lr 4.999794636083308e-06
I20241204 07:30:20 2488423 dinov2 helpers.py:102] Training  [   50/12500]  eta: 7:04:29  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.102760  data: 0.003334  max mem: 3115
lr 4.999706207370645e-06
I20241204 07:30:42 2488423 dinov2 helpers.py:102] Training  [   60/12500]  eta: 7:06:25  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.111084  data: 0.002508  max mem: 3115
lr 4.999601989155004e-06
I20241204 07:31:03 2488423 dinov2 helpers.py:102] Training  [   70/12500]  eta: 7:07:56  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.116524  data: 0.002638  max mem: 3115
lr 4.999481982094688e-06
I20241204 07:31:24 2488423 dinov2 helpers.py:102] Training  [   80/12500]  eta: 7:09:05  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.122525  data: 0.002317  max mem: 3115
lr 4.9993461869477276e-06
I20241204 07:31:45 2488423 dinov2 helpers.py:102] Training  [   90/12500]  eta: 7:09:55  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.124367  data: 0.003410  max mem: 3115
lr 4.999194604571874e-06
I20241204 07:32:06 2488423 dinov2 helpers.py:102] Training  [  100/12500]  eta: 7:10:32  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.125124  data: 0.003668  max mem: 3115
lr 4.999027235924608e-06
I20241204 07:32:28 2488423 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:10:58  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125401  data: 0.002027  max mem: 3115
lr 4.998844082063119e-06
I20241204 07:32:49 2488423 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:11:16  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.125165  data: 0.002940  max mem: 3115
lr 4.998645144144304e-06
I20241204 07:33:10 2488423 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:11:28  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.125116  data: 0.002656  max mem: 3115
lr 4.998430423424764e-06
I20241204 07:33:31 2488423 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:11:34  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.124246  data: 0.001325  max mem: 3115
lr 4.9981999212607945e-06
I20241204 07:33:53 2488423 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:11:38  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.124902  data: 0.002182  max mem: 3115
lr 4.997953639108375e-06
I20241204 07:34:14 2488423 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:11:38  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.125142  data: 0.002051  max mem: 3115
lr 4.997691578523149e-06
I20241204 07:34:35 2488423 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:11:38  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.126157  data: 0.001812  max mem: 3115
lr 4.9974137411604395e-06
I20241204 07:34:57 2488423 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:11:37  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.128616  data: 0.002164  max mem: 3115
lr 4.9971201287752166e-06
I20241204 07:35:18 2488423 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:11:30  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.126216  data: 0.002687  max mem: 3115
lr 4.996810743222097e-06
I20241204 07:35:39 2488423 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:11:21  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123117  data: 0.002505  max mem: 3115
lr 4.996485586455328e-06
I20241204 07:36:00 2488423 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:11:11  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.123615  data: 0.001777  max mem: 3115
lr 4.996144660528775e-06
I20241204 07:36:22 2488423 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:11:01  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.124100  data: 0.002106  max mem: 3115
lr 4.99578796759591e-06
I20241204 07:36:43 2488423 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:10:50  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124357  data: 0.001986  max mem: 3115
lr 4.995415509909803e-06
I20241204 07:37:04 2488423 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:10:37  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.124103  data: 0.001870  max mem: 3115
lr 4.995027289823097e-06
I20241204 07:37:25 2488423 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:10:23  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.122772  data: 0.001939  max mem: 3115
lr 4.9946233097880025e-06
I20241204 07:37:46 2488423 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:10:10  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.123943  data: 0.002559  max mem: 3115
lr 4.994203572356276e-06
I20241204 07:38:08 2488423 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:09:56  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.125039  data: 0.002365  max mem: 3115
lr 4.9937680801792065e-06
I20241204 07:38:29 2488423 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:09:43  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.125233  data: 0.002405  max mem: 3115
lr 4.993316836007601e-06
I20241204 07:38:50 2488423 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:09:28  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.125863  data: 0.003499  max mem: 3115
lr 4.992849842691759e-06
I20241204 07:39:11 2488423 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:09:12  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.124588  data: 0.002482  max mem: 3115
lr 4.99236710318147e-06
I20241204 07:39:33 2488423 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:08:56  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.123993  data: 0.001572  max mem: 3115
lr 4.991868620525976e-06
I20241204 07:39:54 2488423 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:08:41  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.125165  data: 0.002651  max mem: 3115
lr 4.991354397873964e-06
I20241204 07:40:15 2488423 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:08:26  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.126684  data: 0.002379  max mem: 3115
lr 4.990824438473544e-06
I20241204 07:40:37 2488423 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:08:09  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.126339  data: 0.001326  max mem: 3115
lr 4.990278745672229e-06
I20241204 07:40:58 2488423 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:07:51  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.123886  data: 0.001264  max mem: 3115
lr 4.98971732291691e-06
I20241204 07:41:19 2488423 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:07:34  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.123323  data: 0.001274  max mem: 3115
lr 4.989140173753839e-06
I20241204 07:41:40 2488423 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:07:17  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.001500  max mem: 3115
lr 4.988547301828603e-06
I20241204 07:42:02 2488423 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:07:00  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125685  data: 0.001829  max mem: 3115
lr 4.987938710886104e-06
I20241204 07:42:23 2488423 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:06:41  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.124411  data: 0.001617  max mem: 3115
lr 4.9873144047705305e-06
I20241204 07:42:44 2488423 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:06:24  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124677  data: 0.001239  max mem: 3115
lr 4.986674387425343e-06
I20241204 07:43:05 2488423 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:06:06  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.125303  data: 0.001503  max mem: 3115
lr 4.9860186628932356e-06
I20241204 07:43:26 2488423 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:05:47  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123829  data: 0.001675  max mem: 3115
lr 4.985347235316124e-06
I20241204 07:43:48 2488423 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:05:28  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.123416  data: 0.001449  max mem: 3115
lr 4.984660108935109e-06
I20241204 07:44:09 2488423 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:05:09  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123023  data: 0.001510  max mem: 3115
lr 4.983957288090453e-06
I20241204 07:44:30 2488423 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:04:51  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123533  data: 0.002236  max mem: 3115
lr 4.9832387772215545e-06
I20241204 07:44:51 2488423 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:04:31  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.123996  data: 0.001993  max mem: 3115
lr 4.982504580866918e-06
I20241204 07:45:13 2488423 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:04:13  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123882  data: 0.001192  max mem: 3115
lr 4.981754703664129e-06
I20241204 07:45:34 2488423 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:03:54  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.125294  data: 0.001181  max mem: 3115
lr 4.980989150349819e-06
I20241204 07:45:55 2488423 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:03:35  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.125326  data: 0.001911  max mem: 3115
lr 4.980207925759636e-06
I20241204 07:46:16 2488423 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:03:15  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123971  data: 0.002060  max mem: 3115
lr 4.979411034828223e-06
I20241204 07:46:38 2488423 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:02:56  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.123709  data: 0.001327  max mem: 3115
lr 4.978598482589174e-06
I20241204 07:46:59 2488423 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:02:37  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123658  data: 0.001215  max mem: 3115
lr 4.977770274175011e-06
I20241204 07:47:20 2488423 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:02:17  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124259  data: 0.001547  max mem: 3115
lr 4.97692641481715e-06
I20241204 07:47:41 2488423 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:01:58  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.124818  data: 0.001623  max mem: 3115
lr 4.976066909845862e-06
I20241204 07:48:03 2488423 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:01:39  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125314  data: 0.001228  max mem: 3115
lr 4.975191764690249e-06
I20241204 07:48:24 2488423 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:01:19  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.124998  data: 0.000969  max mem: 3115
lr 4.974300984878205e-06
I20241204 07:48:45 2488423 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:00:59  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.123785  data: 0.000990  max mem: 3115
lr 4.973394576036379e-06
I20241204 07:49:06 2488423 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:00:39  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124286  data: 0.001396  max mem: 3115
lr 4.97247254389014e-06
I20241204 07:49:28 2488423 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:00:20  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.124524  data: 0.001474  max mem: 3115
lr 4.9715348942635445e-06
I20241204 07:49:49 2488423 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:00:00  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125206  data: 0.001592  max mem: 3115
lr 4.9705816330792985e-06
I20241204 07:50:10 2488423 dinov2 helpers.py:102] Training  [  610/12500]  eta: 6:59:40  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.125498  data: 0.001721  max mem: 3115
lr 4.969612766358717e-06
I20241204 07:50:31 2488423 dinov2 helpers.py:102] Training  [  620/12500]  eta: 6:59:21  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.125387  data: 0.001484  max mem: 3115
lr 4.9686283002216905e-06
I20241204 07:50:53 2488423 dinov2 helpers.py:102] Training  [  630/12500]  eta: 6:59:01  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.125293  data: 0.001721  max mem: 3115
lr 4.967628240886639e-06
I20241204 07:51:14 2488423 dinov2 helpers.py:102] Training  [  640/12500]  eta: 6:58:41  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124411  data: 0.001770  max mem: 3115
lr 4.966612594670483e-06
I20241204 07:51:35 2488423 dinov2 helpers.py:102] Training  [  650/12500]  eta: 6:58:21  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124448  data: 0.002886  max mem: 3115
lr 4.965581367988594e-06
I20241204 07:51:56 2488423 dinov2 helpers.py:102] Training  [  660/12500]  eta: 6:58:01  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.124164  data: 0.002816  max mem: 3115
lr 4.964534567354764e-06
I20241204 07:52:18 2488423 dinov2 helpers.py:102] Training  [  670/12500]  eta: 6:57:40  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.123635  data: 0.001723  max mem: 3115
lr 4.96347219938115e-06
I20241204 07:52:39 2488423 dinov2 helpers.py:102] Training  [  680/12500]  eta: 6:57:20  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124700  data: 0.001600  max mem: 3115
lr 4.96239427077825e-06
I20241204 07:53:00 2488423 dinov2 helpers.py:102] Training  [  690/12500]  eta: 6:57:00  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124767  data: 0.001835  max mem: 3115
lr 4.961300788354844e-06
I20241204 07:53:21 2488423 dinov2 helpers.py:102] Training  [  700/12500]  eta: 6:56:40  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.124339  data: 0.003231  max mem: 3115
lr 4.960191759017962e-06
I20241204 07:53:43 2488423 dinov2 helpers.py:102] Training  [  710/12500]  eta: 6:56:20  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.124805  data: 0.002974  max mem: 3115
lr 4.959067189772836e-06
I20241204 07:54:04 2488423 dinov2 helpers.py:102] Training  [  720/12500]  eta: 6:56:00  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.126276  data: 0.003144  max mem: 3115
lr 4.957927087722856e-06
I20241204 07:54:25 2488423 dinov2 helpers.py:102] Training  [  730/12500]  eta: 6:55:40  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.126573  data: 0.002703  max mem: 3115
lr 4.956771460069526e-06
I20241204 07:54:46 2488423 dinov2 helpers.py:102] Training  [  740/12500]  eta: 6:55:20  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125640  data: 0.001059  max mem: 3115
lr 4.95560031411242e-06
I20241204 07:55:08 2488423 dinov2 helpers.py:102] Training  [  750/12500]  eta: 6:54:59  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124886  data: 0.001051  max mem: 3115
lr 4.9544136572491304e-06
I20241204 07:55:29 2488423 dinov2 helpers.py:102] Training  [  760/12500]  eta: 6:54:39  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.123879  data: 0.001246  max mem: 3115
lr 4.953211496975229e-06
I20241204 07:55:50 2488423 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:54:19  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.124135  data: 0.001439  max mem: 3115
lr 4.951993840884212e-06
I20241204 07:56:11 2488423 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:53:58  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.124500  data: 0.001555  max mem: 3115
lr 4.950760696667457e-06
I20241204 07:56:33 2488423 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:53:38  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.125949  data: 0.002119  max mem: 3115
lr 4.949512072114174e-06
I20241204 07:56:54 2488423 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:53:18  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126383  data: 0.002415  max mem: 3115
lr 4.948247975111351e-06
I20241204 07:57:15 2488423 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:52:58  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.125761  data: 0.001996  max mem: 3115
lr 4.946968413643719e-06
I20241204 07:57:36 2488423 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:52:38  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.126677  data: 0.001512  max mem: 3115
lr 4.945673395793676e-06
I20241204 07:57:58 2488423 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:52:17  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.125551  data: 0.001471  max mem: 3115
lr 4.9443629297412615e-06
I20241204 07:58:19 2488423 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:51:57  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125622  data: 0.001534  max mem: 3115
lr 4.943037023764093e-06
I20241204 07:58:40 2488423 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:51:36  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.125673  data: 0.001265  max mem: 3115
lr 4.941695686237312e-06
I20241204 07:59:01 2488423 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:51:16  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125628  data: 0.001804  max mem: 3115
lr 4.940338925633534e-06
I20241204 07:59:23 2488423 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:50:55  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.126028  data: 0.002160  max mem: 3115
lr 4.938966750522798e-06
I20241204 07:59:44 2488423 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:50:35  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125703  data: 0.001536  max mem: 3115
lr 4.937579169572506e-06
I20241204 08:00:05 2488423 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:50:15  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126744  data: 0.002235  max mem: 3115
lr 4.936176191547377e-06
I20241204 08:00:27 2488423 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:49:54  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126812  data: 0.002353  max mem: 3115
lr 4.934757825309379e-06
I20241204 08:00:48 2488423 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:49:34  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.126379  data: 0.001416  max mem: 3115
lr 4.933324079817689e-06
I20241204 08:01:09 2488423 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:49:14  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.127834  data: 0.002390  max mem: 3115
lr 4.9318749641286164e-06
I20241204 08:01:30 2488423 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:48:54  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.128770  data: 0.002436  max mem: 3115
lr 4.930410487395568e-06
I20241204 08:01:52 2488423 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:48:33  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.127465  data: 0.001552  max mem: 3115
lr 4.928930658868971e-06
I20241204 08:02:13 2488423 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:48:12  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.125388  data: 0.001286  max mem: 3115
lr 4.927435487896227e-06
I20241204 08:02:34 2488423 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:47:52  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.124378  data: 0.000903  max mem: 3115
lr 4.925924983921652e-06
I20241204 08:02:55 2488423 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:47:31  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.126196  data: 0.001247  max mem: 3115
lr 4.92439915648641e-06
I20241204 08:03:17 2488423 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:47:11  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.126946  data: 0.002306  max mem: 3115
lr 4.922858015228454e-06
I20241204 08:03:38 2488423 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:46:50  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.126025  data: 0.002131  max mem: 3115
lr 4.921301569882469e-06
I20241204 08:03:59 2488423 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:46:29  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.124389  data: 0.001440  max mem: 3115
lr 4.919729830279811e-06
I20241204 08:04:20 2488423 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:46:09  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.125331  data: 0.001934  max mem: 3115
lr 4.918142806348443e-06
I20241204 08:04:42 2488423 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:45:48  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.126234  data: 0.001720  max mem: 3115
lr 4.916540508112869e-06
I20241204 08:05:03 2488423 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:45:27  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.124934  data: 0.001418  max mem: 3115
lr 4.914922945694074e-06
I20241204 08:05:24 2488423 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:45:06  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.124628  data: 0.002460  max mem: 3115
lr 4.913290129309465e-06
I20241204 08:05:45 2488423 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:44:46  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124561  data: 0.002325  max mem: 3115
lr 4.911642069272796e-06
I20241204 08:06:07 2488423 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:44:25  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.124042  data: 0.001747  max mem: 3115
lr 4.909978775994108e-06
I20241204 08:06:28 2488423 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:44:04  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123715  data: 0.001914  max mem: 3115
lr 4.908300259979668e-06
I20241204 08:06:49 2488423 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:43:43  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.124311  data: 0.002505  max mem: 3115
lr 4.906606531831894e-06
I20241204 08:07:10 2488423 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:43:22  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.124548  data: 0.002620  max mem: 3115
lr 4.904897602249294e-06
I20241204 08:07:32 2488423 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:43:01  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124574  data: 0.001602  max mem: 3115
lr 4.903173482026397e-06
I20241204 08:07:53 2488423 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:42:40  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123640  data: 0.002219  max mem: 3115
lr 4.9014341820536815e-06
I20241204 08:08:14 2488423 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:42:19  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.123162  data: 0.002398  max mem: 3115
lr 4.899679713317512e-06
I20241204 08:08:35 2488423 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:41:58  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.123830  data: 0.001578  max mem: 3115
lr 4.897910086900068e-06
I20241204 08:08:57 2488423 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:41:37  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124156  data: 0.001575  max mem: 3115
lr 4.896125313979271e-06
I20241204 08:09:18 2488423 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:41:16  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123813  data: 0.001871  max mem: 3115
lr 4.894325405828717e-06
I20241204 08:09:39 2488423 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:40:55  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122984  data: 0.002017  max mem: 3115
lr 4.8925103738176015e-06
I20241204 08:10:00 2488423 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:40:34  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.123439  data: 0.001623  max mem: 3115
lr 4.890680229410655e-06
I20241204 08:10:22 2488423 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:40:13  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123744  data: 0.001774  max mem: 3115
lr 4.888834984168066e-06
I20241204 08:10:43 2488423 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:39:52  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.121428  data: 0.003215  max mem: 3115
lr 4.886974649745406e-06
I20241204 08:11:04 2488423 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:39:31  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.120603  data: 0.002660  max mem: 3115
lr 4.885099237893554e-06
I20241204 08:11:25 2488423 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:39:10  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.122416  data: 0.001214  max mem: 3115
lr 4.883208760458633e-06
I20241204 08:11:46 2488423 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:38:49  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.124067  data: 0.001977  max mem: 3115
lr 4.881303229381928e-06
I20241204 08:12:08 2488423 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:38:28  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.124126  data: 0.002025  max mem: 3115
lr 4.8793826566998085e-06
I20241204 08:12:29 2488423 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:38:07  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 2.123180  data: 0.001632  max mem: 3115
I20241204 08:12:46 2488423 dinov2 linear.py:272] running validation !
submitit ERROR (2024-12-04 08:12:46,491) - Submitted job triggered an exception
E20241204 08:12:46 2488423 submitit submission.py:68] Submitted job triggered an exception
