submitit INFO (2024-12-04 07:27:40,416) - Starting with JobEnvironment(job_id=2488412, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-04 07:27:40,416) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender/2488412_submitted.pkl
I20241204 07:27:48 2488417 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488417 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488417 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488417 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488417 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:19 2488417 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:24 2488417 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:24 2488417 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 07:28:31 2488417 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488417 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:34 2488417 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:34 2488417 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:34 2488417 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:34 2488417 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:34 2488417 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 07:28:37 2488417 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:37 2488417 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:37 2488417 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:37 2488417 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:37 2488417 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:37 2488417 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:37 2488417 dinov2 linear.py:338] Starting training from iteration 0
lr 4.999999921043166e-06
I20241204 07:28:56 2488417 dinov2 helpers.py:102] Training  [    0/12500]  eta: 2 days, 18:52:52  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 19.261826  data: 16.908451  max mem: 2706
I20241204 07:28:56 2488417 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
lr 4.999990446229023e-06
I20241204 07:29:02 2488417 dinov2 helpers.py:102] Training  [   10/12500]  eta: 8:03:54  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 2.324629  data: 1.538470  max mem: 3115
lr 4.999965180116501e-06
I20241204 07:29:08 2488417 dinov2 helpers.py:102] Training  [   20/12500]  eta: 5:14:26  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 0.624261  data: 0.004060  max mem: 3115
lr 4.999924122865191e-06
I20241204 07:29:26 2488417 dinov2 helpers.py:102] Training  [   30/12500]  eta: 5:31:32  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 1.194084  data: 0.003906  max mem: 3115
lr 4.999867274734432e-06
I20241204 07:29:47 2488417 dinov2 helpers.py:102] Training  [   40/12500]  eta: 5:56:02  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 1.927335  data: 0.001300  max mem: 3115
lr 4.999794636083308e-06
I20241204 07:30:08 2488417 dinov2 helpers.py:102] Training  [   50/12500]  eta: 6:11:28  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.092546  data: 0.001366  max mem: 3115
lr 4.999706207370645e-06
I20241204 07:30:29 2488417 dinov2 helpers.py:102] Training  [   60/12500]  eta: 6:22:08  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.106771  data: 0.001601  max mem: 3115
lr 4.999601989155004e-06
I20241204 07:30:50 2488417 dinov2 helpers.py:102] Training  [   70/12500]  eta: 6:29:40  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.112281  data: 0.001805  max mem: 3115
lr 4.999481982094688e-06
I20241204 07:31:11 2488417 dinov2 helpers.py:102] Training  [   80/12500]  eta: 6:35:17  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.112456  data: 0.001656  max mem: 3115
lr 4.9993461869477276e-06
I20241204 07:31:32 2488417 dinov2 helpers.py:102] Training  [   90/12500]  eta: 6:39:47  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.117492  data: 0.002653  max mem: 3115
lr 4.999194604571874e-06
I20241204 07:31:54 2488417 dinov2 helpers.py:102] Training  [  100/12500]  eta: 6:43:26  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.124456  data: 0.003288  max mem: 3115
lr 4.999027235924608e-06
I20241204 07:32:15 2488417 dinov2 helpers.py:102] Training  [  110/12500]  eta: 6:46:18  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125348  data: 0.002332  max mem: 3115
lr 4.998844082063119e-06
I20241204 07:32:36 2488417 dinov2 helpers.py:102] Training  [  120/12500]  eta: 6:48:38  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.123531  data: 0.001784  max mem: 3115
lr 4.998645144144304e-06
I20241204 07:32:57 2488417 dinov2 helpers.py:102] Training  [  130/12500]  eta: 6:50:33  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.123182  data: 0.001408  max mem: 3115
lr 4.998430423424764e-06
I20241204 07:33:19 2488417 dinov2 helpers.py:102] Training  [  140/12500]  eta: 6:52:09  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.123462  data: 0.001186  max mem: 3115
lr 4.9981999212607945e-06
I20241204 07:33:40 2488417 dinov2 helpers.py:102] Training  [  150/12500]  eta: 6:53:30  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.124335  data: 0.001389  max mem: 3115
lr 4.997953639108375e-06
I20241204 07:34:01 2488417 dinov2 helpers.py:102] Training  [  160/12500]  eta: 6:54:39  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.124721  data: 0.001356  max mem: 3115
lr 4.997691578523149e-06
I20241204 07:34:22 2488417 dinov2 helpers.py:102] Training  [  170/12500]  eta: 6:55:38  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.124993  data: 0.001291  max mem: 3115
lr 4.9974137411604395e-06
I20241204 07:34:44 2488417 dinov2 helpers.py:102] Training  [  180/12500]  eta: 6:56:25  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.123888  data: 0.001537  max mem: 3115
lr 4.9971201287752166e-06
I20241204 07:35:05 2488417 dinov2 helpers.py:102] Training  [  190/12500]  eta: 6:57:07  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.123439  data: 0.002300  max mem: 3115
lr 4.996810743222097e-06
I20241204 07:35:26 2488417 dinov2 helpers.py:102] Training  [  200/12500]  eta: 6:57:42  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.124204  data: 0.002658  max mem: 3115
lr 4.996485586455328e-06
I20241204 07:35:47 2488417 dinov2 helpers.py:102] Training  [  210/12500]  eta: 6:58:12  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.123655  data: 0.001809  max mem: 3115
lr 4.996144660528775e-06
I20241204 07:36:09 2488417 dinov2 helpers.py:102] Training  [  220/12500]  eta: 6:58:37  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.123270  data: 0.001010  max mem: 3115
lr 4.99578796759591e-06
I20241204 07:36:30 2488417 dinov2 helpers.py:102] Training  [  230/12500]  eta: 6:58:59  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124152  data: 0.002374  max mem: 3115
lr 4.995415509909803e-06
I20241204 07:36:51 2488417 dinov2 helpers.py:102] Training  [  240/12500]  eta: 6:59:16  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.124321  data: 0.002924  max mem: 3115
lr 4.995027289823097e-06
I20241204 07:37:12 2488417 dinov2 helpers.py:102] Training  [  250/12500]  eta: 6:59:30  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123065  data: 0.001689  max mem: 3115
lr 4.9946233097880025e-06
I20241204 07:37:34 2488417 dinov2 helpers.py:102] Training  [  260/12500]  eta: 6:59:41  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.122375  data: 0.001342  max mem: 3115
lr 4.994203572356276e-06
I20241204 07:37:55 2488417 dinov2 helpers.py:102] Training  [  270/12500]  eta: 6:59:51  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.123312  data: 0.002050  max mem: 3115
lr 4.9937680801792065e-06
I20241204 07:38:16 2488417 dinov2 helpers.py:102] Training  [  280/12500]  eta: 6:59:58  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123727  data: 0.002876  max mem: 3115
lr 4.993316836007601e-06
I20241204 07:38:37 2488417 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:00:04  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124147  data: 0.003255  max mem: 3115
lr 4.992849842691759e-06
I20241204 07:38:59 2488417 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:00:07  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.123678  data: 0.002982  max mem: 3115
lr 4.99236710318147e-06
I20241204 07:39:20 2488417 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:00:08  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.122537  data: 0.003286  max mem: 3115
lr 4.991868620525976e-06
I20241204 07:39:41 2488417 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:00:10  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.124705  data: 0.003130  max mem: 3115
lr 4.991354397873964e-06
I20241204 07:40:02 2488417 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:00:09  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.124953  data: 0.002052  max mem: 3115
lr 4.990824438473544e-06
I20241204 07:40:24 2488417 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:00:06  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.123421  data: 0.001468  max mem: 3115
lr 4.990278745672229e-06
I20241204 07:40:45 2488417 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:00:04  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.124109  data: 0.001132  max mem: 3115
lr 4.98971732291691e-06
I20241204 07:41:06 2488417 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:00:00  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.124307  data: 0.001550  max mem: 3115
lr 4.989140173753839e-06
I20241204 07:41:27 2488417 dinov2 helpers.py:102] Training  [  370/12500]  eta: 6:59:55  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.124757  data: 0.001893  max mem: 3115
lr 4.988547301828603e-06
I20241204 07:41:49 2488417 dinov2 helpers.py:102] Training  [  380/12500]  eta: 6:59:50  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125762  data: 0.001697  max mem: 3115
lr 4.987938710886104e-06
I20241204 07:42:10 2488417 dinov2 helpers.py:102] Training  [  390/12500]  eta: 6:59:43  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.124364  data: 0.001147  max mem: 3115
lr 4.9873144047705305e-06
I20241204 07:42:31 2488417 dinov2 helpers.py:102] Training  [  400/12500]  eta: 6:59:35  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.122878  data: 0.001356  max mem: 3115
lr 4.986674387425343e-06
I20241204 07:42:52 2488417 dinov2 helpers.py:102] Training  [  410/12500]  eta: 6:59:27  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.123460  data: 0.002130  max mem: 3115
lr 4.9860186628932356e-06
I20241204 07:43:13 2488417 dinov2 helpers.py:102] Training  [  420/12500]  eta: 6:59:19  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.124784  data: 0.002210  max mem: 3115
lr 4.985347235316124e-06
I20241204 07:43:35 2488417 dinov2 helpers.py:102] Training  [  430/12500]  eta: 6:59:09  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.124403  data: 0.001547  max mem: 3115
lr 4.984660108935109e-06
I20241204 07:43:56 2488417 dinov2 helpers.py:102] Training  [  440/12500]  eta: 6:59:00  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123572  data: 0.001414  max mem: 3115
lr 4.983957288090453e-06
I20241204 07:44:17 2488417 dinov2 helpers.py:102] Training  [  450/12500]  eta: 6:58:49  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123174  data: 0.001481  max mem: 3115
lr 4.9832387772215545e-06
I20241204 07:44:38 2488417 dinov2 helpers.py:102] Training  [  460/12500]  eta: 6:58:39  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.125219  data: 0.001756  max mem: 3115
lr 4.982504580866918e-06
I20241204 07:45:00 2488417 dinov2 helpers.py:102] Training  [  470/12500]  eta: 6:58:27  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.124996  data: 0.001772  max mem: 3115
lr 4.981754703664129e-06
I20241204 07:45:21 2488417 dinov2 helpers.py:102] Training  [  480/12500]  eta: 6:58:15  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.122190  data: 0.001556  max mem: 3115
lr 4.980989150349819e-06
I20241204 07:45:42 2488417 dinov2 helpers.py:102] Training  [  490/12500]  eta: 6:58:03  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.123086  data: 0.001701  max mem: 3115
lr 4.980207925759636e-06
I20241204 07:46:03 2488417 dinov2 helpers.py:102] Training  [  500/12500]  eta: 6:57:51  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123917  data: 0.001710  max mem: 3115
lr 4.979411034828223e-06
I20241204 07:46:25 2488417 dinov2 helpers.py:102] Training  [  510/12500]  eta: 6:57:38  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.123659  data: 0.003395  max mem: 3115
lr 4.978598482589174e-06
I20241204 07:46:46 2488417 dinov2 helpers.py:102] Training  [  520/12500]  eta: 6:57:25  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.124367  data: 0.003124  max mem: 3115
lr 4.977770274175011e-06
I20241204 07:47:07 2488417 dinov2 helpers.py:102] Training  [  530/12500]  eta: 6:57:12  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124386  data: 0.001488  max mem: 3115
lr 4.97692641481715e-06
I20241204 07:47:28 2488417 dinov2 helpers.py:102] Training  [  540/12500]  eta: 6:56:59  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.125549  data: 0.002379  max mem: 3115
lr 4.976066909845862e-06
I20241204 07:47:50 2488417 dinov2 helpers.py:102] Training  [  550/12500]  eta: 6:56:45  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.126447  data: 0.002295  max mem: 3115
lr 4.975191764690249e-06
I20241204 07:48:11 2488417 dinov2 helpers.py:102] Training  [  560/12500]  eta: 6:56:31  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125099  data: 0.001434  max mem: 3115
lr 4.974300984878205e-06
I20241204 07:48:32 2488417 dinov2 helpers.py:102] Training  [  570/12500]  eta: 6:56:17  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.124703  data: 0.001365  max mem: 3115
lr 4.973394576036379e-06
I20241204 07:48:53 2488417 dinov2 helpers.py:102] Training  [  580/12500]  eta: 6:56:03  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.125714  data: 0.001741  max mem: 3115
lr 4.97247254389014e-06
I20241204 07:49:15 2488417 dinov2 helpers.py:102] Training  [  590/12500]  eta: 6:55:48  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.126150  data: 0.001695  max mem: 3115
lr 4.9715348942635445e-06
I20241204 07:49:36 2488417 dinov2 helpers.py:102] Training  [  600/12500]  eta: 6:55:33  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125483  data: 0.001573  max mem: 3115
lr 4.9705816330792985e-06
I20241204 07:49:57 2488417 dinov2 helpers.py:102] Training  [  610/12500]  eta: 6:55:18  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.126323  data: 0.001486  max mem: 3115
lr 4.969612766358717e-06
I20241204 07:50:18 2488417 dinov2 helpers.py:102] Training  [  620/12500]  eta: 6:55:03  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.126021  data: 0.001436  max mem: 3115
lr 4.9686283002216905e-06
I20241204 07:50:40 2488417 dinov2 helpers.py:102] Training  [  630/12500]  eta: 6:54:47  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124237  data: 0.001785  max mem: 3115
lr 4.967628240886639e-06
I20241204 07:51:01 2488417 dinov2 helpers.py:102] Training  [  640/12500]  eta: 6:54:32  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124773  data: 0.002055  max mem: 3115
lr 4.966612594670483e-06
I20241204 07:51:22 2488417 dinov2 helpers.py:102] Training  [  650/12500]  eta: 6:54:16  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.126092  data: 0.001898  max mem: 3115
lr 4.965581367988594e-06
I20241204 07:51:43 2488417 dinov2 helpers.py:102] Training  [  660/12500]  eta: 6:54:00  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.126367  data: 0.001460  max mem: 3115
lr 4.964534567354764e-06
I20241204 07:52:05 2488417 dinov2 helpers.py:102] Training  [  670/12500]  eta: 6:53:44  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.125471  data: 0.001359  max mem: 3115
lr 4.96347219938115e-06
I20241204 07:52:26 2488417 dinov2 helpers.py:102] Training  [  680/12500]  eta: 6:53:27  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124703  data: 0.002182  max mem: 3115
lr 4.96239427077825e-06
I20241204 07:52:47 2488417 dinov2 helpers.py:102] Training  [  690/12500]  eta: 6:53:11  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124706  data: 0.002315  max mem: 3115
lr 4.961300788354844e-06
I20241204 07:53:08 2488417 dinov2 helpers.py:102] Training  [  700/12500]  eta: 6:52:54  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.123124  data: 0.001619  max mem: 3115
lr 4.960191759017962e-06
I20241204 07:53:30 2488417 dinov2 helpers.py:102] Training  [  710/12500]  eta: 6:52:37  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.122656  data: 0.001675  max mem: 3115
lr 4.959067189772836e-06
I20241204 07:53:51 2488417 dinov2 helpers.py:102] Training  [  720/12500]  eta: 6:52:20  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.125839  data: 0.001578  max mem: 3115
lr 4.957927087722856e-06
I20241204 07:54:12 2488417 dinov2 helpers.py:102] Training  [  730/12500]  eta: 6:52:03  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.126984  data: 0.001613  max mem: 3115
lr 4.956771460069526e-06
I20241204 07:54:33 2488417 dinov2 helpers.py:102] Training  [  740/12500]  eta: 6:51:46  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125181  data: 0.001930  max mem: 3115
lr 4.95560031411242e-06
I20241204 07:54:55 2488417 dinov2 helpers.py:102] Training  [  750/12500]  eta: 6:51:29  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.125229  data: 0.001781  max mem: 3115
lr 4.9544136572491304e-06
I20241204 07:55:16 2488417 dinov2 helpers.py:102] Training  [  760/12500]  eta: 6:51:12  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.127672  data: 0.001832  max mem: 3115
lr 4.953211496975229e-06
I20241204 07:55:37 2488417 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:50:55  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.127400  data: 0.002975  max mem: 3115
lr 4.951993840884212e-06
I20241204 07:55:59 2488417 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:50:38  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125737  data: 0.002724  max mem: 3115
lr 4.950760696667457e-06
I20241204 07:56:20 2488417 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:50:20  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.126678  data: 0.001766  max mem: 3115
lr 4.949512072114174e-06
I20241204 07:56:41 2488417 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:50:02  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.125706  data: 0.001578  max mem: 3115
lr 4.948247975111351e-06
I20241204 07:57:02 2488417 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:49:45  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126352  data: 0.002257  max mem: 3115
lr 4.946968413643719e-06
I20241204 07:57:24 2488417 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:49:28  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.128073  data: 0.003150  max mem: 3115
lr 4.945673395793676e-06
I20241204 07:57:45 2488417 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:49:10  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.126507  data: 0.002494  max mem: 3115
lr 4.9443629297412615e-06
I20241204 07:58:06 2488417 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:48:52  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125535  data: 0.001653  max mem: 3115
lr 4.943037023764093e-06
I20241204 07:58:27 2488417 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:48:34  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.126578  data: 0.001266  max mem: 3115
lr 4.941695686237312e-06
I20241204 07:58:49 2488417 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:48:16  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.126572  data: 0.001344  max mem: 3115
lr 4.940338925633534e-06
I20241204 07:59:10 2488417 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:47:57  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.124756  data: 0.001383  max mem: 3115
lr 4.938966750522798e-06
I20241204 07:59:31 2488417 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:47:39  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.124126  data: 0.001196  max mem: 3115
lr 4.937579169572506e-06
I20241204 07:59:52 2488417 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:47:21  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126091  data: 0.001733  max mem: 3115
lr 4.936176191547377e-06
I20241204 08:00:14 2488417 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:47:02  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.127127  data: 0.001642  max mem: 3115
lr 4.934757825309379e-06
I20241204 08:00:35 2488417 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:46:44  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.125989  data: 0.001183  max mem: 3115
lr 4.933324079817689e-06
I20241204 08:00:56 2488417 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:46:25  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.125840  data: 0.001142  max mem: 3115
lr 4.9318749641286164e-06
I20241204 08:01:17 2488417 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:46:07  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.127119  data: 0.001521  max mem: 3115
lr 4.930410487395568e-06
I20241204 08:01:39 2488417 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:45:48  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.127155  data: 0.001581  max mem: 3115
lr 4.928930658868971e-06
I20241204 08:02:00 2488417 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:45:30  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126652  data: 0.001082  max mem: 3115
lr 4.927435487896227e-06
I20241204 08:02:21 2488417 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:45:11  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.126595  data: 0.001384  max mem: 3115
lr 4.925924983921652e-06
I20241204 08:02:43 2488417 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:44:53  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.127160  data: 0.003110  max mem: 3115
lr 4.92439915648641e-06
I20241204 08:03:04 2488417 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:44:34  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.127085  data: 0.002970  max mem: 3115
lr 4.922858015228454e-06
I20241204 08:03:25 2488417 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:44:15  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.125552  data: 0.001612  max mem: 3115
lr 4.921301569882469e-06
I20241204 08:03:46 2488417 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:43:56  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.125229  data: 0.001674  max mem: 3115
lr 4.919729830279811e-06
I20241204 08:04:08 2488417 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:43:37  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.125238  data: 0.001252  max mem: 3115
lr 4.918142806348443e-06
I20241204 08:04:29 2488417 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:43:18  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125777  data: 0.001172  max mem: 3115
lr 4.916540508112869e-06
I20241204 08:04:50 2488417 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:42:59  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.125810  data: 0.001239  max mem: 3115
lr 4.914922945694074e-06
I20241204 08:05:11 2488417 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:42:40  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.125896  data: 0.001533  max mem: 3115
lr 4.913290129309465e-06
I20241204 08:05:33 2488417 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:42:20  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124552  data: 0.001773  max mem: 3115
lr 4.911642069272796e-06
I20241204 08:05:54 2488417 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:42:01  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.123580  data: 0.001339  max mem: 3115
lr 4.909978775994108e-06
I20241204 08:06:15 2488417 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:41:41  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.124261  data: 0.001266  max mem: 3115
lr 4.908300259979668e-06
I20241204 08:06:36 2488417 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:41:22  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.123593  data: 0.001403  max mem: 3115
lr 4.906606531831894e-06
I20241204 08:06:58 2488417 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:41:02  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123436  data: 0.001584  max mem: 3115
lr 4.904897602249294e-06
I20241204 08:07:19 2488417 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:40:43  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.123857  data: 0.002749  max mem: 3115
lr 4.903173482026397e-06
I20241204 08:07:40 2488417 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:40:23  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.124005  data: 0.002540  max mem: 3115
lr 4.9014341820536815e-06
I20241204 08:08:01 2488417 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:40:04  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.124088  data: 0.001464  max mem: 3115
lr 4.899679713317512e-06
I20241204 08:08:23 2488417 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:39:44  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.125094  data: 0.001610  max mem: 3115
lr 4.897910086900068e-06
I20241204 08:08:44 2488417 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:39:25  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124770  data: 0.001508  max mem: 3115
lr 4.896125313979271e-06
I20241204 08:09:05 2488417 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:39:05  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123983  data: 0.001801  max mem: 3115
lr 4.894325405828717e-06
I20241204 08:09:26 2488417 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:38:45  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.124438  data: 0.001969  max mem: 3115
lr 4.8925103738176015e-06
I20241204 08:09:48 2488417 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:38:26  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.124336  data: 0.001695  max mem: 3115
lr 4.890680229410655e-06
I20241204 08:10:09 2488417 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:38:06  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123721  data: 0.001586  max mem: 3115
lr 4.888834984168066e-06
I20241204 08:10:30 2488417 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:37:46  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.123196  data: 0.002149  max mem: 3115
lr 4.886974649745406e-06
I20241204 08:10:51 2488417 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:37:26  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121296  data: 0.002065  max mem: 3115
lr 4.885099237893554e-06
I20241204 08:11:12 2488417 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:37:06  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.121033  data: 0.001158  max mem: 3115
lr 4.883208760458633e-06
I20241204 08:11:34 2488417 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:36:46  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.122381  data: 0.001022  max mem: 3115
lr 4.881303229381928e-06
I20241204 08:11:55 2488417 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:36:26  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.121997  data: 0.001166  max mem: 3115
lr 4.8793826566998085e-06
I20241204 08:12:16 2488417 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:36:06  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 2.122141  data: 0.001632  max mem: 3115
I20241204 08:12:35 2488417 dinov2 linear.py:272] running validation !
submitit ERROR (2024-12-04 08:12:35,192) - Submitted job triggered an exception
E20241204 08:12:35 2488417 submitit submission.py:68] Submitted job triggered an exception
