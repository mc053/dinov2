I20241203 14:09:27 2107383 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:27 2107383 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:27 2107383 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:27 2107383 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:27 2107383 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:28 2107384 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:28 2107384 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:28 2107384 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:28 2107384 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:28 2107384 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:28 2107380 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:28 2107380 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:28 2107380 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:28 2107380 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:28 2107385 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:28 2107385 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:28 2107385 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:28 2107385 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:28 2107382 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:28 2107382 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:28 2107382 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:28 2107382 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:28 2107380 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:28 2107385 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:29 2107381 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:29 2107381 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:29 2107381 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:29 2107382 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:29 2107381 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:29 2107387 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:29 2107387 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:29 2107387 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:29 2107387 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:29 2107381 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:29 2107387 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:29 2107386 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 14:09:29 2107386 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241203 14:09:29 2107386 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 14:09:29 2107386 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 14:09:29 2107386 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 14:09:58 2107383 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:00 2107384 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:01 2107380 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:01 2107382 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:01 2107385 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:02 2107386 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:02 2107387 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:02 2107383 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:03 2107381 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 14:10:03 2107383 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:05 2107384 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:05 2107382 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:06 2107380 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:06 2107384 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:06 2107382 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:06 2107385 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:06 2107386 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:06 2107380 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:06 2107385 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:07 2107386 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:07 2107387 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:07 2107387 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:07 2107381 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 14:10:07 2107381 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241203 14:10:11 2107383 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:11 2107383 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:12 2107384 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:12 2107384 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:12 2107380 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:12 2107380 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:12 2107382 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:12 2107382 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:13 2107386 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:13 2107386 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:13 2107385 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:13 2107385 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:13 2107387 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:13 2107387 submitit submission.py:68] Submitted job triggered an exception
I20241203 14:10:13 2107381 dinov2 loaders.py:93] # of dataset samples: 162,127
E20241203 14:10:13 2107381 submitit submission.py:68] Submitted job triggered an exception
I20241204 07:27:48 2488416 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488416 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488416 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488416 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488416 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:48 2488423 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488423 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488423 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488423 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488423 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:48 2488417 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488417 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488417 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488417 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488422 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488422 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488422 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488422 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488417 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:49 2488422 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:49 2488419 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:49 2488419 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:49 2488419 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:49 2488419 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488421 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:49 2488421 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:49 2488421 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:49 2488421 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488418 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:49 2488418 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:49 2488418 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:49 2488418 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488420 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:49 2488420 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:49 2488420 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:49 2488420 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488419 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:49 2488421 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:49 2488418 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:27:49 2488420 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:19 2488423 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:19 2488417 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:20 2488419 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:22 2488416 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488421 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488422 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488418 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488420 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:23 2488423 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:24 2488423 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:24 2488417 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:24 2488417 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:24 2488419 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:24 2488419 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:27 2488416 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:27 2488416 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:28 2488421 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488422 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488421 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:28 2488420 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488418 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488420 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:28 2488422 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:28 2488418 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
I20241204 07:28:31 2488423 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:31 2488419 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:31 2488417 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488416 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488421 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488423 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:34 2488423 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:34 2488423 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:34 2488423 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:34 2488423 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:34 2488423 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:34 2488419 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:34 2488419 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:34 2488419 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:34 2488419 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:34 2488419 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:34 2488419 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:34 2488417 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:34 2488417 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:34 2488417 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:34 2488417 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:34 2488417 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:34 2488417 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:34 2488422 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488420 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:34 2488418 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:36 2488423 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:36 2488423 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:36 2488423 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:36 2488423 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:36 2488423 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:36 2488423 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:36 2488423 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:37 2488419 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:37 2488419 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:37 2488419 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:37 2488419 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:37 2488419 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:37 2488419 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:37 2488419 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:37 2488417 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:37 2488417 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:37 2488417 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:37 2488417 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:37 2488417 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:37 2488417 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:37 2488417 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:39 2488416 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:39 2488416 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:39 2488416 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:39 2488416 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:39 2488416 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:39 2488416 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:40 2488421 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:40 2488421 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:40 2488421 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:40 2488421 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:40 2488421 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:40 2488421 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:41 2488422 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488422 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:41 2488422 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488422 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488422 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:41 2488422 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:41 2488420 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488420 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:41 2488420 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488420 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488420 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:41 2488420 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:41 2488418 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488418 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:41 2488418 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488418 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488418 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:41 2488418 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
I20241204 07:28:41 2488416 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:41 2488416 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:41 2488416 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488416 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488416 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:41 2488416 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488416 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:45 2488421 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:45 2488421 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:45 2488421 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:45 2488421 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:45 2488421 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:45 2488421 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:45 2488421 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:46 2488422 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:46 2488422 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:46 2488422 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:46 2488422 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:46 2488422 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:46 2488422 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:46 2488422 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:48 2488418 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:48 2488418 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:48 2488418 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:48 2488418 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:48 2488418 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:48 2488418 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:48 2488418 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:48 2488420 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:48 2488420 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:48 2488420 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:48 2488420 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:48 2488420 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:48 2488420 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:48 2488420 dinov2 linear.py:338] Starting training from iteration 0
I20241204 07:28:54 2488423 dinov2 helpers.py:102] Training  [    0/12500]  eta: 2 days, 14:25:56  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 17.980488  data: 14.264837  max mem: 2706
I20241204 07:28:54 2488423 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:28:55 2488419 dinov2 helpers.py:102] Training  [    0/12500]  eta: 2 days, 14:44:13  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 18.068283  data: 15.609655  max mem: 2706
I20241204 07:28:55 2488419 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:28:56 2488417 dinov2 helpers.py:102] Training  [    0/12500]  eta: 2 days, 18:52:52  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 19.261826  data: 16.908451  max mem: 2706
I20241204 07:28:56 2488417 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:02 2488417 dinov2 helpers.py:102] Training  [   10/12500]  eta: 8:03:54  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 2.324629  data: 1.538470  max mem: 3115
I20241204 07:29:05 2488416 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 9:00:32  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 23.330566  data: 16.807346  max mem: 2706
I20241204 07:29:05 2488419 dinov2 helpers.py:102] Training  [   10/12500]  eta: 8:52:53  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 2.559930  data: 1.435729  max mem: 3115
I20241204 07:29:05 2488423 dinov2 helpers.py:102] Training  [   10/12500]  eta: 9:09:04  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 2.637709  data: 1.303654  max mem: 3115
I20241204 07:29:06 2488416 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:08 2488417 dinov2 helpers.py:102] Training  [   20/12500]  eta: 5:14:26  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 0.624261  data: 0.004060  max mem: 3115
I20241204 07:29:10 2488421 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 12:19:30  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 24.285614  data: 16.912766  max mem: 2706
I20241204 07:29:12 2488420 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 11:39:11  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 24.092113  data: 18.209854  max mem: 2706
I20241204 07:29:13 2488421 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:13 2488422 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 23:09:18  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 27.404659  data: 20.905233  max mem: 2709
I20241204 07:29:15 2488418 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 19:44:32  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 26.421762  data: 21.496368  max mem: 2706
I20241204 07:29:15 2488420 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:15 2488422 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:17 2488418 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241204 07:29:18 2488423 dinov2 helpers.py:102] Training  [   20/12500]  eta: 6:50:49  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 1.174842  data: 0.004926  max mem: 3115
I20241204 07:29:22 2488419 dinov2 helpers.py:102] Training  [   20/12500]  eta: 7:25:43  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 1.346641  data: 0.056884  max mem: 3115
I20241204 07:29:23 2488416 dinov2 helpers.py:102] Training  [   10/12500]  eta: 13:11:16  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 3.801128  data: 1.528547  max mem: 3115
I20241204 07:29:26 2488417 dinov2 helpers.py:102] Training  [   30/12500]  eta: 5:31:32  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 1.194084  data: 0.003906  max mem: 3115
I20241204 07:29:31 2488421 dinov2 helpers.py:102] Training  [   10/12500]  eta: 14:30:20  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 4.180958  data: 1.635488  max mem: 3115
I20241204 07:29:33 2488420 dinov2 helpers.py:102] Training  [   10/12500]  eta: 14:09:46  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 4.082217  data: 1.656328  max mem: 3115
I20241204 07:29:34 2488422 dinov2 helpers.py:102] Training  [   10/12500]  eta: 15:15:47  loss: 28.3534 (31.6803)  lr: 0.0000 (0.0000)  time: 4.399310  data: 1.938053  max mem: 3113
I20241204 07:29:36 2488418 dinov2 helpers.py:102] Training  [   10/12500]  eta: 14:59:41  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 4.322003  data: 1.956850  max mem: 3115
I20241204 07:29:38 2488423 dinov2 helpers.py:102] Training  [   30/12500]  eta: 6:57:31  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 1.663177  data: 0.003121  max mem: 3115
I20241204 07:29:42 2488419 dinov2 helpers.py:102] Training  [   30/12500]  eta: 7:21:14  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 1.882759  data: 0.048570  max mem: 3115
I20241204 07:29:44 2488416 dinov2 helpers.py:102] Training  [   20/12500]  eta: 10:20:17  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 1.964764  data: 0.001369  max mem: 3115
I20241204 07:29:47 2488417 dinov2 helpers.py:102] Training  [   40/12500]  eta: 5:56:02  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 1.927335  data: 0.001300  max mem: 3115
I20241204 07:29:52 2488421 dinov2 helpers.py:102] Training  [   20/12500]  eta: 11:02:27  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 2.129822  data: 0.054648  max mem: 3115
I20241204 07:29:54 2488420 dinov2 helpers.py:102] Training  [   20/12500]  eta: 10:52:03  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 2.087053  data: 0.001224  max mem: 3115
I20241204 07:29:55 2488422 dinov2 helpers.py:102] Training  [   20/12500]  eta: 11:27:17  loss: 28.3534 (28.9273)  lr: 0.0000 (0.0000)  time: 2.099283  data: 0.021459  max mem: 3113
I20241204 07:29:57 2488418 dinov2 helpers.py:102] Training  [   20/12500]  eta: 11:18:49  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 2.105633  data: 0.003629  max mem: 3115
I20241204 07:29:59 2488423 dinov2 helpers.py:102] Training  [   40/12500]  eta: 7:01:35  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.087935  data: 0.004014  max mem: 3115
I20241204 07:30:03 2488419 dinov2 helpers.py:102] Training  [   40/12500]  eta: 7:19:29  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.088406  data: 0.001701  max mem: 3115
I20241204 07:30:05 2488416 dinov2 helpers.py:102] Training  [   30/12500]  eta: 9:20:34  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.090111  data: 0.001831  max mem: 3115
I20241204 07:30:08 2488417 dinov2 helpers.py:102] Training  [   50/12500]  eta: 6:11:28  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.092546  data: 0.001366  max mem: 3115
I20241204 07:30:13 2488421 dinov2 helpers.py:102] Training  [   30/12500]  eta: 9:49:31  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.097103  data: 0.001710  max mem: 3115
I20241204 07:30:15 2488420 dinov2 helpers.py:102] Training  [   30/12500]  eta: 9:42:37  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.099927  data: 0.001529  max mem: 3115
I20241204 07:30:16 2488422 dinov2 helpers.py:102] Training  [   30/12500]  eta: 10:06:26  loss: 23.4213 (26.6113)  lr: 0.0000 (0.0000)  time: 2.103218  data: 0.001592  max mem: 3113
I20241204 07:30:18 2488418 dinov2 helpers.py:102] Training  [   30/12500]  eta: 10:01:01  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.105314  data: 0.003126  max mem: 3115
I20241204 07:30:20 2488423 dinov2 helpers.py:102] Training  [   50/12500]  eta: 7:04:29  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.102760  data: 0.003334  max mem: 3115
I20241204 07:30:24 2488419 dinov2 helpers.py:102] Training  [   50/12500]  eta: 7:19:02  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.104657  data: 0.003030  max mem: 3115
I20241204 07:30:26 2488416 dinov2 helpers.py:102] Training  [   40/12500]  eta: 8:50:22  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.104424  data: 0.001357  max mem: 3115
I20241204 07:30:29 2488417 dinov2 helpers.py:102] Training  [   60/12500]  eta: 6:22:08  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.106771  data: 0.001601  max mem: 3115
I20241204 07:30:34 2488421 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:12:18  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.108013  data: 0.001902  max mem: 3115
I20241204 07:30:36 2488420 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:07:08  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.109464  data: 0.001458  max mem: 3115
I20241204 07:30:37 2488422 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:25:42  loss: 23.4213 (25.7552)  lr: 0.0000 (0.0000)  time: 2.114882  data: 0.001414  max mem: 3113
I20241204 07:30:39 2488418 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:21:05  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.112099  data: 0.002369  max mem: 3115
I20241204 07:30:42 2488423 dinov2 helpers.py:102] Training  [   60/12500]  eta: 7:06:25  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.111084  data: 0.002508  max mem: 3115
I20241204 07:30:46 2488419 dinov2 helpers.py:102] Training  [   60/12500]  eta: 7:18:34  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.113306  data: 0.003030  max mem: 3115
I20241204 07:30:47 2488416 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:32:07  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.112813  data: 0.001383  max mem: 3115
I20241204 07:30:50 2488417 dinov2 helpers.py:102] Training  [   70/12500]  eta: 6:29:40  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.112281  data: 0.001805  max mem: 3115
I20241204 07:30:56 2488421 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:49:38  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.112233  data: 0.001622  max mem: 3115
I20241204 07:30:58 2488420 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:45:27  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.112218  data: 0.003804  max mem: 3115
I20241204 07:30:59 2488422 dinov2 helpers.py:102] Training  [   50/12500]  eta: 9:01:07  loss: 23.4213 (25.5184)  lr: 0.0000 (0.0000)  time: 2.127129  data: 0.001427  max mem: 3113
I20241204 07:31:00 2488418 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:56:59  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.116841  data: 0.003018  max mem: 3115
I20241204 07:31:03 2488423 dinov2 helpers.py:102] Training  [   70/12500]  eta: 7:07:56  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.116524  data: 0.002638  max mem: 3115
I20241204 07:31:07 2488419 dinov2 helpers.py:102] Training  [   70/12500]  eta: 7:18:18  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.115321  data: 0.001359  max mem: 3115
I20241204 07:31:08 2488416 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:19:46  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.116310  data: 0.001792  max mem: 3115
I20241204 07:31:11 2488417 dinov2 helpers.py:102] Training  [   80/12500]  eta: 6:35:17  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.112456  data: 0.001656  max mem: 3115
I20241204 07:31:17 2488421 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:34:27  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.115782  data: 0.001259  max mem: 3115
I20241204 07:31:19 2488420 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:30:56  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.115030  data: 0.004089  max mem: 3115
I20241204 07:31:20 2488422 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:44:11  loss: 23.4213 (24.9988)  lr: 0.0000 (0.0000)  time: 2.126867  data: 0.001802  max mem: 3113
I20241204 07:31:21 2488418 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:40:49  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.122713  data: 0.002835  max mem: 3115
I20241204 07:31:24 2488423 dinov2 helpers.py:102] Training  [   80/12500]  eta: 7:09:05  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.122525  data: 0.002317  max mem: 3115
I20241204 07:31:28 2488419 dinov2 helpers.py:102] Training  [   80/12500]  eta: 7:18:10  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.121352  data: 0.001431  max mem: 3115
I20241204 07:31:30 2488416 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:11:00  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.120419  data: 0.001753  max mem: 3115
I20241204 07:31:32 2488417 dinov2 helpers.py:102] Training  [   90/12500]  eta: 6:39:47  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.117492  data: 0.002653  max mem: 3115
I20241204 07:31:38 2488421 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:23:38  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.121639  data: 0.001627  max mem: 3115
I20241204 07:31:40 2488420 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:20:35  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.120673  data: 0.001482  max mem: 3115
I20241204 07:31:41 2488422 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:31:58  loss: 22.4366 (24.6785)  lr: 0.0000 (0.0000)  time: 2.123212  data: 0.001879  max mem: 3113
I20241204 07:31:43 2488418 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:29:05  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.124578  data: 0.001931  max mem: 3115
I20241204 07:31:45 2488423 dinov2 helpers.py:102] Training  [   90/12500]  eta: 7:09:55  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.124367  data: 0.003410  max mem: 3115
I20241204 07:31:49 2488419 dinov2 helpers.py:102] Training  [   90/12500]  eta: 7:18:01  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.125124  data: 0.001525  max mem: 3115
I20241204 07:31:51 2488416 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:04:21  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.124487  data: 0.001374  max mem: 3115
I20241204 07:31:54 2488417 dinov2 helpers.py:102] Training  [  100/12500]  eta: 6:43:26  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.124456  data: 0.003288  max mem: 3115
I20241204 07:31:59 2488421 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:15:30  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.126759  data: 0.004259  max mem: 3115
I20241204 07:32:01 2488420 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:12:46  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.125256  data: 0.001357  max mem: 3115
I20241204 07:32:03 2488422 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:22:44  loss: 22.7961 (24.4693)  lr: 0.0000 (0.0000)  time: 2.125093  data: 0.001780  max mem: 3113
I20241204 07:32:04 2488418 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:20:13  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.125509  data: 0.001641  max mem: 3115
I20241204 07:32:06 2488423 dinov2 helpers.py:102] Training  [  100/12500]  eta: 7:10:32  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.125124  data: 0.003668  max mem: 3115
I20241204 07:32:11 2488419 dinov2 helpers.py:102] Training  [  100/12500]  eta: 7:17:47  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.124623  data: 0.001222  max mem: 3115
I20241204 07:32:12 2488416 dinov2 helpers.py:102] Training  [   90/12500]  eta: 7:59:04  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.124832  data: 0.001486  max mem: 3115
I20241204 07:32:15 2488417 dinov2 helpers.py:102] Training  [  110/12500]  eta: 6:46:18  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125348  data: 0.002332  max mem: 3115
I20241204 07:32:21 2488421 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:08:59  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.126544  data: 0.004199  max mem: 3115
I20241204 07:32:22 2488420 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:06:34  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.125749  data: 0.001443  max mem: 3115
I20241204 07:32:24 2488422 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:15:26  loss: 22.4366 (24.0474)  lr: 0.0000 (0.0000)  time: 2.125919  data: 0.001653  max mem: 3113
I20241204 07:32:25 2488418 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:13:14  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.126520  data: 0.002193  max mem: 3115
I20241204 07:32:28 2488423 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:10:58  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125401  data: 0.002027  max mem: 3115
I20241204 07:32:32 2488419 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:17:36  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125517  data: 0.001889  max mem: 3115
I20241204 07:32:33 2488416 dinov2 helpers.py:102] Training  [  100/12500]  eta: 7:54:44  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.123949  data: 0.002100  max mem: 3115
I20241204 07:32:36 2488417 dinov2 helpers.py:102] Training  [  120/12500]  eta: 6:48:38  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.123531  data: 0.001784  max mem: 3115
I20241204 07:32:42 2488421 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:03:39  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.123962  data: 0.001594  max mem: 3115
I20241204 07:32:44 2488420 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:01:29  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.123884  data: 0.001522  max mem: 3115
I20241204 07:32:45 2488422 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:09:28  loss: 22.4366 (23.2664)  lr: 0.0000 (0.0000)  time: 2.124123  data: 0.001478  max mem: 3113
I20241204 07:32:46 2488418 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:07:31  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.125710  data: 0.002517  max mem: 3115
I20241204 07:32:49 2488423 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:11:16  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.125165  data: 0.002940  max mem: 3115
I20241204 07:32:53 2488419 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:17:20  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.125997  data: 0.002185  max mem: 3115
I20241204 07:32:55 2488416 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:51:09  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.124450  data: 0.002220  max mem: 3115
I20241204 07:32:57 2488417 dinov2 helpers.py:102] Training  [  130/12500]  eta: 6:50:33  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.123182  data: 0.001408  max mem: 3115
I20241204 07:33:03 2488421 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:59:16  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.124264  data: 0.001777  max mem: 3115
I20241204 07:33:05 2488420 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:57:17  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.124074  data: 0.001482  max mem: 3115
I20241204 07:33:06 2488422 dinov2 helpers.py:102] Training  [  110/12500]  eta: 8:04:31  loss: 22.4366 (23.4682)  lr: 0.0000 (0.0000)  time: 2.123392  data: 0.001374  max mem: 3113
I20241204 07:33:08 2488418 dinov2 helpers.py:102] Training  [  110/12500]  eta: 8:02:48  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.125671  data: 0.002956  max mem: 3115
I20241204 07:33:10 2488423 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:11:28  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.125116  data: 0.002656  max mem: 3115
I20241204 07:33:14 2488419 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:17:03  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.124097  data: 0.001617  max mem: 3115
I20241204 07:33:16 2488416 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:48:05  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.124618  data: 0.001924  max mem: 3115
I20241204 07:33:19 2488417 dinov2 helpers.py:102] Training  [  140/12500]  eta: 6:52:09  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.123462  data: 0.001186  max mem: 3115
I20241204 07:33:24 2488421 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:55:32  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.124753  data: 0.002602  max mem: 3115
I20241204 07:33:26 2488420 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:53:42  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.124424  data: 0.000997  max mem: 3115
I20241204 07:33:27 2488422 dinov2 helpers.py:102] Training  [  120/12500]  eta: 8:00:22  loss: 22.4366 (23.0133)  lr: 0.0000 (0.0000)  time: 2.124752  data: 0.001438  max mem: 3113
I20241204 07:33:29 2488418 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:58:47  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.125993  data: 0.002278  max mem: 3115
I20241204 07:33:31 2488423 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:11:34  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.124246  data: 0.001325  max mem: 3115
I20241204 07:33:36 2488419 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:16:46  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.123927  data: 0.001258  max mem: 3115
I20241204 07:33:37 2488416 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:45:28  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.125040  data: 0.001962  max mem: 3115
I20241204 07:33:40 2488417 dinov2 helpers.py:102] Training  [  150/12500]  eta: 6:53:30  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.124335  data: 0.001389  max mem: 3115
I20241204 07:33:45 2488421 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:52:16  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.123279  data: 0.003786  max mem: 3115
I20241204 07:33:47 2488420 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:50:37  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.123988  data: 0.001137  max mem: 3115
I20241204 07:33:49 2488422 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:56:47  loss: 22.3304 (22.9422)  lr: 0.0000 (0.0000)  time: 2.125485  data: 0.001984  max mem: 3113
I20241204 07:33:50 2488418 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:55:19  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.125264  data: 0.001362  max mem: 3115
I20241204 07:33:53 2488423 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:11:38  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.124902  data: 0.002182  max mem: 3115
I20241204 07:33:57 2488419 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:16:29  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.125380  data: 0.001295  max mem: 3115
I20241204 07:33:58 2488416 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:43:08  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.125180  data: 0.002157  max mem: 3115
I20241204 07:34:01 2488417 dinov2 helpers.py:102] Training  [  160/12500]  eta: 6:54:39  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.124721  data: 0.001356  max mem: 3115
I20241204 07:34:07 2488421 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:49:29  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.123900  data: 0.002970  max mem: 3115
I20241204 07:34:09 2488420 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:47:55  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.124494  data: 0.001532  max mem: 3115
I20241204 07:34:10 2488422 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:53:40  loss: 22.4366 (22.9390)  lr: 0.0000 (0.0000)  time: 2.125583  data: 0.002659  max mem: 3113
I20241204 07:34:11 2488418 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:52:19  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.125547  data: 0.001940  max mem: 3115
I20241204 07:34:14 2488423 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:11:38  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.125142  data: 0.002051  max mem: 3115
I20241204 07:34:18 2488419 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:16:11  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.125341  data: 0.001332  max mem: 3115
I20241204 07:34:20 2488416 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:41:07  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.126029  data: 0.004408  max mem: 3115
I20241204 07:34:22 2488417 dinov2 helpers.py:102] Training  [  170/12500]  eta: 6:55:38  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.124993  data: 0.001291  max mem: 3115
I20241204 07:34:28 2488421 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:47:00  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.125499  data: 0.001524  max mem: 3115
I20241204 07:34:30 2488420 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:45:32  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.124485  data: 0.001626  max mem: 3115
I20241204 07:34:31 2488422 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:50:55  loss: 22.3304 (22.6978)  lr: 0.0000 (0.0000)  time: 2.125558  data: 0.002031  max mem: 3113
I20241204 07:34:33 2488418 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:49:39  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.126024  data: 0.001819  max mem: 3115
I20241204 07:34:35 2488423 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:11:38  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.126157  data: 0.001812  max mem: 3115
I20241204 07:34:39 2488419 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:15:51  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.123702  data: 0.001409  max mem: 3115
I20241204 07:34:41 2488416 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:39:16  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.126029  data: 0.003917  max mem: 3115
I20241204 07:34:44 2488417 dinov2 helpers.py:102] Training  [  180/12500]  eta: 6:56:25  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.123888  data: 0.001537  max mem: 3115
I20241204 07:34:49 2488421 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:44:45  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.124039  data: 0.001419  max mem: 3115
I20241204 07:34:51 2488420 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:43:23  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.123434  data: 0.001403  max mem: 3115
I20241204 07:34:53 2488422 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:48:27  loss: 22.3304 (22.4806)  lr: 0.0000 (0.0000)  time: 2.125210  data: 0.001926  max mem: 3113
I20241204 07:34:54 2488418 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:47:14  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.124176  data: 0.001435  max mem: 3115
I20241204 07:34:57 2488423 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:11:37  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.128616  data: 0.002164  max mem: 3115
I20241204 07:35:01 2488419 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:15:32  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.123672  data: 0.001664  max mem: 3115
I20241204 07:35:02 2488416 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:37:35  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.124289  data: 0.001564  max mem: 3115
I20241204 07:35:05 2488417 dinov2 helpers.py:102] Training  [  190/12500]  eta: 6:57:07  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.123439  data: 0.002300  max mem: 3115
I20241204 07:35:10 2488421 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:42:45  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.123337  data: 0.001064  max mem: 3115
I20241204 07:35:12 2488420 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:41:28  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.123561  data: 0.002188  max mem: 3115
I20241204 07:35:14 2488422 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:46:13  loss: 22.0177 (22.2354)  lr: 0.0000 (0.0000)  time: 2.123846  data: 0.002007  max mem: 3113
I20241204 07:35:15 2488418 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:45:04  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.122657  data: 0.001893  max mem: 3115
I20241204 07:35:18 2488423 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:11:30  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.126216  data: 0.002687  max mem: 3115
I20241204 07:35:22 2488419 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:15:14  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.124942  data: 0.001683  max mem: 3115
I20241204 07:35:23 2488416 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:36:05  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.125840  data: 0.002989  max mem: 3115
I20241204 07:35:26 2488417 dinov2 helpers.py:102] Training  [  200/12500]  eta: 6:57:42  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.124204  data: 0.002658  max mem: 3115
I20241204 07:35:32 2488421 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:40:57  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.125506  data: 0.001491  max mem: 3115
I20241204 07:35:34 2488420 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:39:42  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.124020  data: 0.002482  max mem: 3115
I20241204 07:35:35 2488422 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:44:12  loss: 22.0177 (22.0306)  lr: 0.0000 (0.0000)  time: 2.124058  data: 0.002198  max mem: 3113
I20241204 07:35:36 2488418 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:43:09  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.124933  data: 0.002778  max mem: 3115
I20241204 07:35:39 2488423 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:11:21  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123117  data: 0.002505  max mem: 3115
I20241204 07:35:43 2488419 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:14:54  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.124599  data: 0.001544  max mem: 3115
I20241204 07:35:45 2488416 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:34:40  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.125123  data: 0.002675  max mem: 3115
I20241204 07:35:47 2488417 dinov2 helpers.py:102] Training  [  210/12500]  eta: 6:58:12  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.123655  data: 0.001809  max mem: 3115
I20241204 07:35:53 2488421 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:39:17  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.125621  data: 0.001791  max mem: 3115
I20241204 07:35:55 2488420 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:38:06  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.123541  data: 0.001468  max mem: 3115
I20241204 07:35:56 2488422 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:42:22  loss: 21.8810 (21.8644)  lr: 0.0000 (0.0000)  time: 2.124728  data: 0.003168  max mem: 3113
I20241204 07:35:58 2488418 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:41:21  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.125046  data: 0.002363  max mem: 3115
I20241204 07:36:00 2488423 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:11:11  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.123615  data: 0.001777  max mem: 3115
I20241204 07:36:04 2488419 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:14:36  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124742  data: 0.002702  max mem: 3115
I20241204 07:36:06 2488416 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:33:21  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123784  data: 0.001124  max mem: 3115
I20241204 07:36:09 2488417 dinov2 helpers.py:102] Training  [  220/12500]  eta: 6:58:37  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.123270  data: 0.001010  max mem: 3115
I20241204 07:36:14 2488421 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:37:44  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123576  data: 0.002409  max mem: 3115
I20241204 07:36:16 2488420 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:36:36  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123671  data: 0.001424  max mem: 3115
I20241204 07:36:17 2488422 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:40:39  loss: 20.2497 (21.6677)  lr: 0.0000 (0.0000)  time: 2.123378  data: 0.002540  max mem: 3113
I20241204 07:36:19 2488418 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:39:43  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.124369  data: 0.002492  max mem: 3115
I20241204 07:36:22 2488423 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:11:01  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.124100  data: 0.002106  max mem: 3115
I20241204 07:36:26 2488419 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:14:16  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.125237  data: 0.003121  max mem: 3115
I20241204 07:36:27 2488416 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:32:08  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124102  data: 0.001245  max mem: 3115
I20241204 07:36:30 2488417 dinov2 helpers.py:102] Training  [  230/12500]  eta: 6:58:59  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124152  data: 0.002374  max mem: 3115
I20241204 07:36:35 2488421 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:36:20  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124739  data: 0.002682  max mem: 3115
I20241204 07:36:37 2488420 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:35:15  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124704  data: 0.001674  max mem: 3115
I20241204 07:36:39 2488422 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:39:05  loss: 19.6635 (21.3674)  lr: 0.0000 (0.0000)  time: 2.123581  data: 0.001896  max mem: 3113
I20241204 07:36:40 2488418 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:38:12  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.125485  data: 0.002554  max mem: 3115
I20241204 07:36:43 2488423 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:10:50  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124357  data: 0.001986  max mem: 3115
I20241204 07:36:47 2488419 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:13:57  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.125083  data: 0.001804  max mem: 3115
I20241204 07:36:48 2488416 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:31:00  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.124183  data: 0.001337  max mem: 3115
I20241204 07:36:51 2488417 dinov2 helpers.py:102] Training  [  240/12500]  eta: 6:59:16  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.124321  data: 0.002924  max mem: 3115
I20241204 07:36:57 2488421 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:35:00  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.125366  data: 0.002312  max mem: 3115
I20241204 07:36:59 2488420 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:33:58  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.125326  data: 0.002186  max mem: 3115
I20241204 07:37:00 2488422 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:37:37  loss: 19.4132 (21.2824)  lr: 0.0000 (0.0000)  time: 2.123911  data: 0.001987  max mem: 3113
I20241204 07:37:01 2488418 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:36:49  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.126256  data: 0.002258  max mem: 3115
I20241204 07:37:04 2488423 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:10:37  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.124103  data: 0.001870  max mem: 3115
I20241204 07:37:08 2488419 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:13:36  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.124120  data: 0.001699  max mem: 3115
I20241204 07:37:09 2488416 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:29:56  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124241  data: 0.001059  max mem: 3115
I20241204 07:37:12 2488417 dinov2 helpers.py:102] Training  [  250/12500]  eta: 6:59:30  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123065  data: 0.001689  max mem: 3115
I20241204 07:37:18 2488421 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:33:46  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124612  data: 0.002184  max mem: 3115
I20241204 07:37:20 2488420 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:32:47  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124920  data: 0.002416  max mem: 3115
I20241204 07:37:21 2488422 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:36:15  loss: 19.0806 (21.1466)  lr: 0.0000 (0.0000)  time: 2.123063  data: 0.001767  max mem: 3113
I20241204 07:37:23 2488418 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:35:29  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.125533  data: 0.002548  max mem: 3115
I20241204 07:37:25 2488423 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:10:23  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.122772  data: 0.001939  max mem: 3115
I20241204 07:37:29 2488419 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:13:16  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123603  data: 0.001758  max mem: 3115
I20241204 07:37:31 2488416 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:28:54  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.123193  data: 0.001038  max mem: 3115
I20241204 07:37:34 2488417 dinov2 helpers.py:102] Training  [  260/12500]  eta: 6:59:41  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.122375  data: 0.001342  max mem: 3115
I20241204 07:37:39 2488421 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:32:37  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.126455  data: 0.002669  max mem: 3115
I20241204 07:37:41 2488420 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:31:38  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.123493  data: 0.001620  max mem: 3115
I20241204 07:37:42 2488422 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:34:59  loss: 19.0806 (21.2802)  lr: 0.0000 (0.0000)  time: 2.124565  data: 0.003635  max mem: 3113
I20241204 07:37:44 2488418 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:34:14  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.123869  data: 0.001840  max mem: 3115
I20241204 07:37:46 2488423 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:10:10  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.123943  data: 0.002559  max mem: 3115
I20241204 07:37:51 2488419 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:12:56  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.125036  data: 0.001376  max mem: 3115
I20241204 07:37:52 2488416 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:27:56  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123132  data: 0.001234  max mem: 3115
I20241204 07:37:55 2488417 dinov2 helpers.py:102] Training  [  270/12500]  eta: 6:59:51  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.123312  data: 0.002050  max mem: 3115
I20241204 07:38:01 2488421 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:31:30  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.125917  data: 0.002279  max mem: 3115
I20241204 07:38:02 2488420 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:30:34  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123138  data: 0.001584  max mem: 3115
I20241204 07:38:04 2488422 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:33:47  loss: 19.0053 (21.1557)  lr: 0.0000 (0.0000)  time: 2.125604  data: 0.003456  max mem: 3113
I20241204 07:38:05 2488418 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:33:04  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.124363  data: 0.001412  max mem: 3115
I20241204 07:38:08 2488423 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:09:56  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.125039  data: 0.002365  max mem: 3115
I20241204 07:38:12 2488419 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:12:36  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.124674  data: 0.001933  max mem: 3115
I20241204 07:38:13 2488416 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:27:02  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.124740  data: 0.001875  max mem: 3115
I20241204 07:38:16 2488417 dinov2 helpers.py:102] Training  [  280/12500]  eta: 6:59:58  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123727  data: 0.002876  max mem: 3115
I20241204 07:38:22 2488421 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:30:28  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.125217  data: 0.001135  max mem: 3115
I20241204 07:38:24 2488420 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:29:32  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.123775  data: 0.003065  max mem: 3115
I20241204 07:38:25 2488422 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:32:39  loss: 18.7059 (21.0447)  lr: 0.0000 (0.0000)  time: 2.124530  data: 0.002434  max mem: 3113
I20241204 07:38:26 2488418 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:31:57  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.123782  data: 0.001223  max mem: 3115
I20241204 07:38:29 2488423 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:09:43  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.125233  data: 0.002405  max mem: 3115
I20241204 07:38:33 2488419 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:12:17  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.125731  data: 0.003338  max mem: 3115
I20241204 07:38:34 2488416 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:26:10  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.125073  data: 0.002279  max mem: 3115
I20241204 07:38:37 2488417 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:00:04  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124147  data: 0.003255  max mem: 3115
I20241204 07:38:43 2488421 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:29:28  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.124447  data: 0.001319  max mem: 3115
I20241204 07:38:45 2488420 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:28:35  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.124533  data: 0.002925  max mem: 3115
I20241204 07:38:46 2488422 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:31:34  loss: 18.3436 (20.9234)  lr: 0.0000 (0.0000)  time: 2.124020  data: 0.002979  max mem: 3113
I20241204 07:38:48 2488418 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:30:53  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.122646  data: 0.001801  max mem: 3115
I20241204 07:38:50 2488423 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:09:28  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.125863  data: 0.003499  max mem: 3115
I20241204 07:38:54 2488419 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:11:55  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.125057  data: 0.003257  max mem: 3115
I20241204 07:38:56 2488416 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:25:20  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123539  data: 0.001653  max mem: 3115
I20241204 07:38:59 2488417 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:00:07  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.123678  data: 0.002982  max mem: 3115
I20241204 07:39:04 2488421 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:28:30  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123510  data: 0.001500  max mem: 3115
I20241204 07:39:06 2488420 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:27:40  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.124514  data: 0.001550  max mem: 3115
I20241204 07:39:07 2488422 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:30:31  loss: 18.3436 (20.8691)  lr: 0.0000 (0.0000)  time: 2.123629  data: 0.002053  max mem: 3113
I20241204 07:39:09 2488418 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:29:53  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123701  data: 0.001888  max mem: 3115
I20241204 07:39:11 2488423 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:09:12  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.124588  data: 0.002482  max mem: 3115
I20241204 07:39:15 2488419 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:11:34  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.122638  data: 0.002383  max mem: 3115
I20241204 07:39:17 2488416 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:24:31  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.123024  data: 0.001488  max mem: 3115
I20241204 07:39:20 2488417 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:00:08  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.122537  data: 0.003286  max mem: 3115
I20241204 07:39:26 2488421 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:27:36  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124561  data: 0.001356  max mem: 3115
I20241204 07:39:27 2488420 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:26:48  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124987  data: 0.001649  max mem: 3115
I20241204 07:39:29 2488422 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:29:33  loss: 18.1600 (20.7686)  lr: 0.0000 (0.0000)  time: 2.124066  data: 0.002213  max mem: 3113
I20241204 07:39:30 2488418 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:28:55  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124585  data: 0.001398  max mem: 3115
I20241204 07:39:33 2488423 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:08:56  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.123993  data: 0.001572  max mem: 3115
I20241204 07:39:37 2488419 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:11:13  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.123098  data: 0.002346  max mem: 3115
I20241204 07:39:38 2488416 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:23:45  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.123422  data: 0.001448  max mem: 3115
I20241204 07:39:41 2488417 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:00:10  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.124705  data: 0.003130  max mem: 3115
I20241204 07:39:47 2488421 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:26:44  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.124964  data: 0.001301  max mem: 3115
I20241204 07:39:49 2488420 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:25:57  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.125453  data: 0.001545  max mem: 3115
I20241204 07:39:50 2488422 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:28:37  loss: 18.1600 (20.6192)  lr: 0.0000 (0.0000)  time: 2.124971  data: 0.002243  max mem: 3113
I20241204 07:39:51 2488418 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:27:59  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.123775  data: 0.002251  max mem: 3115
I20241204 07:39:54 2488423 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:08:41  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.125165  data: 0.002651  max mem: 3115
I20241204 07:39:58 2488419 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:10:53  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.123428  data: 0.001773  max mem: 3115
I20241204 07:39:59 2488416 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:22:59  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.122797  data: 0.001324  max mem: 3115
I20241204 07:40:02 2488417 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:00:09  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.124953  data: 0.002052  max mem: 3115
I20241204 07:40:08 2488421 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:25:53  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.124828  data: 0.002214  max mem: 3115
I20241204 07:40:10 2488420 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:25:07  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.123389  data: 0.001272  max mem: 3115
I20241204 07:40:11 2488422 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:27:43  loss: 18.1600 (20.6042)  lr: 0.0000 (0.0000)  time: 2.126120  data: 0.002234  max mem: 3113
I20241204 07:40:13 2488418 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:27:07  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.123984  data: 0.002752  max mem: 3115
I20241204 07:40:15 2488423 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:08:26  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.126684  data: 0.002379  max mem: 3115
I20241204 07:40:19 2488419 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:10:32  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123758  data: 0.001474  max mem: 3115
I20241204 07:40:21 2488416 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:22:16  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.122729  data: 0.001340  max mem: 3115
I20241204 07:40:24 2488417 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:00:06  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.123421  data: 0.001468  max mem: 3115
I20241204 07:40:29 2488421 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:25:04  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.123712  data: 0.002109  max mem: 3115
I20241204 07:40:31 2488420 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:24:20  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.123448  data: 0.001955  max mem: 3115
I20241204 07:40:32 2488422 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:26:51  loss: 18.1600 (20.4089)  lr: 0.0000 (0.0000)  time: 2.125617  data: 0.001985  max mem: 3113
I20241204 07:40:34 2488418 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:26:16  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.125455  data: 0.001932  max mem: 3115
I20241204 07:40:37 2488423 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:08:09  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.126339  data: 0.001326  max mem: 3115
I20241204 07:40:40 2488419 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:10:12  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.124822  data: 0.002260  max mem: 3115
I20241204 07:40:42 2488416 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:21:34  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123884  data: 0.001423  max mem: 3115
I20241204 07:40:45 2488417 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:00:04  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.124109  data: 0.001132  max mem: 3115
I20241204 07:40:50 2488421 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:24:17  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123839  data: 0.001047  max mem: 3115
I20241204 07:40:52 2488420 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:23:34  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123849  data: 0.002049  max mem: 3115
I20241204 07:40:54 2488422 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:26:00  loss: 18.0669 (20.2835)  lr: 0.0000 (0.0000)  time: 2.123755  data: 0.001308  max mem: 3113
I20241204 07:40:55 2488418 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:25:28  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.125960  data: 0.002506  max mem: 3115
I20241204 07:40:58 2488423 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:07:51  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.123886  data: 0.001264  max mem: 3115
I20241204 07:41:02 2488419 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:09:51  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.125548  data: 0.002360  max mem: 3115
I20241204 07:41:03 2488416 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:20:53  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.123903  data: 0.001424  max mem: 3115
I20241204 07:41:06 2488417 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:00:00  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.124307  data: 0.001550  max mem: 3115
I20241204 07:41:12 2488421 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:23:33  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.125661  data: 0.002050  max mem: 3115
I20241204 07:41:14 2488420 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:22:49  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.123185  data: 0.001365  max mem: 3115
I20241204 07:41:15 2488422 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:25:12  loss: 18.0419 (20.1256)  lr: 0.0000 (0.0000)  time: 2.124204  data: 0.001657  max mem: 3113
I20241204 07:41:16 2488418 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:24:40  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.125671  data: 0.002363  max mem: 3115
I20241204 07:41:19 2488423 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:07:34  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.123323  data: 0.001274  max mem: 3115
I20241204 07:41:23 2488419 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:09:31  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.125573  data: 0.001785  max mem: 3115
I20241204 07:41:24 2488416 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:20:13  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.123368  data: 0.002141  max mem: 3115
I20241204 07:41:27 2488417 dinov2 helpers.py:102] Training  [  370/12500]  eta: 6:59:55  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.124757  data: 0.001893  max mem: 3115
I20241204 07:41:33 2488421 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:22:48  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.125355  data: 0.002370  max mem: 3115
I20241204 07:41:35 2488420 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:22:07  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.124414  data: 0.001285  max mem: 3115
I20241204 07:41:36 2488422 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:24:25  loss: 18.0243 (20.0453)  lr: 0.0000 (0.0000)  time: 2.124448  data: 0.001770  max mem: 3113
I20241204 07:41:38 2488418 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:23:54  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.125094  data: 0.001368  max mem: 3115
I20241204 07:41:40 2488423 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:07:17  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.001500  max mem: 3115
I20241204 07:41:44 2488419 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:09:11  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.126140  data: 0.001326  max mem: 3115
I20241204 07:41:46 2488416 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:19:35  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.124412  data: 0.002660  max mem: 3115
I20241204 07:41:49 2488417 dinov2 helpers.py:102] Training  [  380/12500]  eta: 6:59:50  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125762  data: 0.001697  max mem: 3115
I20241204 07:41:54 2488421 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:22:06  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.125007  data: 0.001841  max mem: 3115
I20241204 07:41:56 2488420 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:21:25  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.124688  data: 0.001949  max mem: 3115
I20241204 07:41:57 2488422 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:23:39  loss: 17.8542 (19.9730)  lr: 0.0000 (0.0000)  time: 2.124184  data: 0.001489  max mem: 3113
I20241204 07:41:59 2488418 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:23:10  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.125744  data: 0.001989  max mem: 3115
I20241204 07:42:02 2488423 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:07:00  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125685  data: 0.001829  max mem: 3115
I20241204 07:42:05 2488419 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:08:50  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125394  data: 0.001449  max mem: 3115
I20241204 07:42:07 2488416 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:18:59  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.126093  data: 0.002281  max mem: 3115
I20241204 07:42:10 2488417 dinov2 helpers.py:102] Training  [  390/12500]  eta: 6:59:43  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.124364  data: 0.001147  max mem: 3115
I20241204 07:42:16 2488421 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:21:24  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.125219  data: 0.001964  max mem: 3115
I20241204 07:42:17 2488420 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:20:44  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.123187  data: 0.003451  max mem: 3115
I20241204 07:42:19 2488422 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:22:55  loss: 17.7355 (19.8749)  lr: 0.0000 (0.0000)  time: 2.124307  data: 0.001882  max mem: 3113
I20241204 07:42:20 2488418 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:22:27  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.124820  data: 0.003138  max mem: 3115
I20241204 07:42:23 2488423 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:06:41  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.124411  data: 0.001617  max mem: 3115
I20241204 07:42:27 2488419 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:08:30  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.124300  data: 0.002038  max mem: 3115
I20241204 07:42:28 2488416 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:18:22  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125057  data: 0.001841  max mem: 3115
I20241204 07:42:31 2488417 dinov2 helpers.py:102] Training  [  400/12500]  eta: 6:59:35  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.122878  data: 0.001356  max mem: 3115
I20241204 07:42:37 2488421 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:20:44  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.123957  data: 0.001546  max mem: 3115
I20241204 07:42:39 2488420 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:20:05  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.124250  data: 0.003089  max mem: 3115
I20241204 07:42:40 2488422 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:22:12  loss: 17.6483 (19.7530)  lr: 0.0000 (0.0000)  time: 2.124377  data: 0.002012  max mem: 3113
I20241204 07:42:41 2488418 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:21:45  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.124703  data: 0.003260  max mem: 3115
I20241204 07:42:44 2488423 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:06:24  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124677  data: 0.001239  max mem: 3115
I20241204 07:42:48 2488419 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:08:09  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124554  data: 0.001790  max mem: 3115
I20241204 07:42:49 2488416 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:17:46  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.123758  data: 0.001120  max mem: 3115
I20241204 07:42:52 2488417 dinov2 helpers.py:102] Training  [  410/12500]  eta: 6:59:27  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.123460  data: 0.002130  max mem: 3115
I20241204 07:42:58 2488421 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:20:05  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.125063  data: 0.001493  max mem: 3115
I20241204 07:43:00 2488420 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:19:27  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.126075  data: 0.002103  max mem: 3115
I20241204 07:43:01 2488422 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:21:30  loss: 17.6483 (19.7513)  lr: 0.0000 (0.0000)  time: 2.125093  data: 0.001941  max mem: 3113
I20241204 07:43:03 2488418 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:21:04  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.125408  data: 0.002039  max mem: 3115
I20241204 07:43:05 2488423 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:06:06  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.125303  data: 0.001503  max mem: 3115
I20241204 07:43:09 2488419 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:07:48  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.124982  data: 0.001298  max mem: 3115
I20241204 07:43:11 2488416 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:17:11  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124601  data: 0.001050  max mem: 3115
I20241204 07:43:13 2488417 dinov2 helpers.py:102] Training  [  420/12500]  eta: 6:59:19  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.124784  data: 0.002210  max mem: 3115
I20241204 07:43:19 2488421 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:19:26  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.125751  data: 0.002489  max mem: 3115
I20241204 07:43:21 2488420 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:18:50  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.126279  data: 0.002980  max mem: 3115
I20241204 07:43:22 2488422 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:20:49  loss: 17.6483 (19.7418)  lr: 0.0000 (0.0000)  time: 2.124808  data: 0.001712  max mem: 3113
I20241204 07:43:24 2488418 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:20:23  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124042  data: 0.001531  max mem: 3115
I20241204 07:43:26 2488423 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:05:47  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123829  data: 0.001675  max mem: 3115
I20241204 07:43:30 2488419 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:07:28  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.125261  data: 0.002295  max mem: 3115
I20241204 07:43:32 2488416 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:16:37  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.125633  data: 0.001217  max mem: 3115
I20241204 07:43:35 2488417 dinov2 helpers.py:102] Training  [  430/12500]  eta: 6:59:09  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.124403  data: 0.001547  max mem: 3115
I20241204 07:43:40 2488421 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:18:48  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.124029  data: 0.002340  max mem: 3115
I20241204 07:43:42 2488420 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:18:12  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.124710  data: 0.002761  max mem: 3115
I20241204 07:43:44 2488422 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:20:10  loss: 17.6483 (19.6050)  lr: 0.0000 (0.0000)  time: 2.125081  data: 0.001970  max mem: 3113
I20241204 07:43:45 2488418 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:19:44  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.123702  data: 0.001534  max mem: 3115
I20241204 07:43:48 2488423 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:05:28  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.123416  data: 0.001449  max mem: 3115
I20241204 07:43:52 2488419 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:07:07  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.125157  data: 0.002443  max mem: 3115
I20241204 07:43:53 2488416 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:16:03  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.125011  data: 0.001551  max mem: 3115
I20241204 07:43:56 2488417 dinov2 helpers.py:102] Training  [  440/12500]  eta: 6:59:00  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123572  data: 0.001414  max mem: 3115
I20241204 07:44:02 2488421 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:18:10  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123154  data: 0.001399  max mem: 3115
I20241204 07:44:04 2488420 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:17:36  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123207  data: 0.001699  max mem: 3115
I20241204 07:44:05 2488422 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:19:30  loss: 17.3709 (19.4991)  lr: 0.0000 (0.0000)  time: 2.124314  data: 0.002697  max mem: 3113
I20241204 07:44:06 2488418 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:19:05  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123820  data: 0.001441  max mem: 3115
I20241204 07:44:09 2488423 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:05:09  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123023  data: 0.001510  max mem: 3115
I20241204 07:44:13 2488419 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:06:46  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.124194  data: 0.001751  max mem: 3115
I20241204 07:44:14 2488416 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:15:30  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.124787  data: 0.002997  max mem: 3115
I20241204 07:44:17 2488417 dinov2 helpers.py:102] Training  [  450/12500]  eta: 6:58:49  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123174  data: 0.001481  max mem: 3115
I20241204 07:44:23 2488421 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:17:34  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.122757  data: 0.001433  max mem: 3115
I20241204 07:44:25 2488420 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:17:01  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.124149  data: 0.001680  max mem: 3115
I20241204 07:44:26 2488422 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:18:52  loss: 17.3709 (19.4702)  lr: 0.0000 (0.0000)  time: 2.123162  data: 0.003621  max mem: 3113
I20241204 07:44:28 2488418 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:18:27  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.123388  data: 0.001392  max mem: 3115
I20241204 07:44:30 2488423 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:04:51  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123533  data: 0.002236  max mem: 3115
I20241204 07:44:34 2488419 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:06:25  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123466  data: 0.001903  max mem: 3115
I20241204 07:44:36 2488416 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:14:57  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.125692  data: 0.003542  max mem: 3115
I20241204 07:44:38 2488417 dinov2 helpers.py:102] Training  [  460/12500]  eta: 6:58:39  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.125219  data: 0.001756  max mem: 3115
I20241204 07:44:44 2488421 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:16:58  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.124174  data: 0.001525  max mem: 3115
I20241204 07:44:46 2488420 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:16:25  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123859  data: 0.001599  max mem: 3115
I20241204 07:44:47 2488422 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:18:15  loss: 17.3709 (19.4725)  lr: 0.0000 (0.0000)  time: 2.124297  data: 0.003288  max mem: 3113
I20241204 07:44:49 2488418 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:17:50  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.123680  data: 0.001433  max mem: 3115
I20241204 07:44:51 2488423 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:04:31  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.123996  data: 0.001993  max mem: 3115
I20241204 07:44:55 2488419 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:06:04  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124324  data: 0.001626  max mem: 3115
I20241204 07:44:57 2488416 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:14:25  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.124669  data: 0.001950  max mem: 3115
I20241204 07:45:00 2488417 dinov2 helpers.py:102] Training  [  470/12500]  eta: 6:58:27  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.124996  data: 0.001772  max mem: 3115
I20241204 07:45:05 2488421 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:16:23  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.124947  data: 0.002613  max mem: 3115
I20241204 07:45:07 2488420 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:15:51  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123738  data: 0.001730  max mem: 3115
I20241204 07:45:09 2488422 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:17:38  loss: 17.3709 (19.4830)  lr: 0.0000 (0.0000)  time: 2.124568  data: 0.001695  max mem: 3113
I20241204 07:45:10 2488418 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:17:14  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.123658  data: 0.001505  max mem: 3115
I20241204 07:45:13 2488423 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:04:13  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123882  data: 0.001192  max mem: 3115
I20241204 07:45:17 2488419 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:05:43  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.125174  data: 0.001901  max mem: 3115
I20241204 07:45:18 2488416 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:13:53  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124232  data: 0.001681  max mem: 3115
I20241204 07:45:21 2488417 dinov2 helpers.py:102] Training  [  480/12500]  eta: 6:58:15  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.122190  data: 0.001556  max mem: 3115
I20241204 07:45:27 2488421 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:15:49  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124156  data: 0.002710  max mem: 3115
I20241204 07:45:28 2488420 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:15:17  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.123957  data: 0.002190  max mem: 3115
I20241204 07:45:30 2488422 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:17:02  loss: 17.3709 (19.4623)  lr: 0.0000 (0.0000)  time: 2.124040  data: 0.001255  max mem: 3113
I20241204 07:45:31 2488418 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:16:38  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124090  data: 0.002240  max mem: 3115
I20241204 07:45:34 2488423 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:03:54  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.125294  data: 0.001181  max mem: 3115
I20241204 07:45:38 2488419 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:05:23  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.125832  data: 0.002723  max mem: 3115
I20241204 07:45:39 2488416 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:13:22  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123637  data: 0.001757  max mem: 3115
I20241204 07:45:42 2488417 dinov2 helpers.py:102] Training  [  490/12500]  eta: 6:58:03  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.123086  data: 0.001701  max mem: 3115
I20241204 07:45:48 2488421 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:15:15  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123903  data: 0.001430  max mem: 3115
I20241204 07:45:50 2488420 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:14:44  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.124384  data: 0.002101  max mem: 3115
I20241204 07:45:51 2488422 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:16:26  loss: 17.2319 (19.3538)  lr: 0.0000 (0.0000)  time: 2.123861  data: 0.001219  max mem: 3113
I20241204 07:45:53 2488418 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:16:04  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.125271  data: 0.003232  max mem: 3115
I20241204 07:45:55 2488423 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:03:35  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.125326  data: 0.001911  max mem: 3115
I20241204 07:45:59 2488419 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:05:02  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.125903  data: 0.002849  max mem: 3115
I20241204 07:46:01 2488416 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:12:51  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.123150  data: 0.001159  max mem: 3115
I20241204 07:46:03 2488417 dinov2 helpers.py:102] Training  [  500/12500]  eta: 6:57:51  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123917  data: 0.001710  max mem: 3115
I20241204 07:46:09 2488421 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:14:41  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.122913  data: 0.001207  max mem: 3115
I20241204 07:46:11 2488420 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:14:11  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.124295  data: 0.001728  max mem: 3115
I20241204 07:46:12 2488422 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:15:52  loss: 16.2446 (19.2759)  lr: 0.0000 (0.0000)  time: 2.125996  data: 0.002997  max mem: 3113
I20241204 07:46:14 2488418 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:15:29  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.124559  data: 0.002471  max mem: 3115
I20241204 07:46:16 2488423 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:03:15  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123971  data: 0.002060  max mem: 3115
I20241204 07:46:20 2488419 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:04:41  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.125524  data: 0.004632  max mem: 3115
I20241204 07:46:22 2488416 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:12:20  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.122850  data: 0.001191  max mem: 3115
I20241204 07:46:25 2488417 dinov2 helpers.py:102] Training  [  510/12500]  eta: 6:57:38  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.123659  data: 0.003395  max mem: 3115
I20241204 07:46:30 2488421 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:14:08  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.124242  data: 0.002330  max mem: 3115
I20241204 07:46:32 2488420 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:13:39  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.122855  data: 0.001559  max mem: 3115
I20241204 07:46:34 2488422 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:15:18  loss: 16.2446 (19.2218)  lr: 0.0000 (0.0000)  time: 2.126049  data: 0.003200  max mem: 3113
I20241204 07:46:35 2488418 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:14:55  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.124203  data: 0.001353  max mem: 3115
I20241204 07:46:38 2488423 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:02:56  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.123709  data: 0.001327  max mem: 3115
I20241204 07:46:42 2488419 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:04:20  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.124315  data: 0.004223  max mem: 3115
I20241204 07:46:43 2488416 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:11:50  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.124516  data: 0.001923  max mem: 3115
I20241204 07:46:46 2488417 dinov2 helpers.py:102] Training  [  520/12500]  eta: 6:57:25  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.124367  data: 0.003124  max mem: 3115
I20241204 07:46:52 2488421 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:13:36  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.125318  data: 0.002310  max mem: 3115
I20241204 07:46:53 2488420 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:13:07  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123770  data: 0.001809  max mem: 3115
I20241204 07:46:55 2488422 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:14:44  loss: 16.5730 (19.2194)  lr: 0.0000 (0.0000)  time: 2.124650  data: 0.002102  max mem: 3113
I20241204 07:46:56 2488418 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:14:21  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.123315  data: 0.001501  max mem: 3115
I20241204 07:46:59 2488423 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:02:37  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123658  data: 0.001215  max mem: 3115
I20241204 07:47:03 2488419 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:03:58  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.122948  data: 0.001575  max mem: 3115
I20241204 07:47:04 2488416 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:11:20  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.125314  data: 0.002188  max mem: 3115
I20241204 07:47:07 2488417 dinov2 helpers.py:102] Training  [  530/12500]  eta: 6:57:12  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124386  data: 0.001488  max mem: 3115
I20241204 07:47:13 2488421 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:13:04  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.124074  data: 0.001541  max mem: 3115
I20241204 07:47:15 2488420 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:12:35  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.124214  data: 0.002305  max mem: 3115
I20241204 07:47:16 2488422 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:14:11  loss: 16.5730 (19.2320)  lr: 0.0000 (0.0000)  time: 2.125265  data: 0.002804  max mem: 3113
I20241204 07:47:18 2488418 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:13:48  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.122835  data: 0.002119  max mem: 3115
I20241204 07:47:20 2488423 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:02:17  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124259  data: 0.001547  max mem: 3115
I20241204 07:47:24 2488419 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:03:38  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124364  data: 0.001195  max mem: 3115
I20241204 07:47:26 2488416 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:10:51  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.124662  data: 0.001912  max mem: 3115
I20241204 07:47:28 2488417 dinov2 helpers.py:102] Training  [  540/12500]  eta: 6:56:59  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.125549  data: 0.002379  max mem: 3115
I20241204 07:47:34 2488421 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:12:32  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123551  data: 0.001935  max mem: 3115
I20241204 07:47:36 2488420 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:12:04  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123112  data: 0.002878  max mem: 3115
I20241204 07:47:37 2488422 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:13:38  loss: 17.2319 (19.2441)  lr: 0.0000 (0.0000)  time: 2.125145  data: 0.002719  max mem: 3113
I20241204 07:47:39 2488418 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:13:16  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123247  data: 0.002480  max mem: 3115
I20241204 07:47:41 2488423 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:01:58  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.124818  data: 0.001623  max mem: 3115
I20241204 07:47:45 2488419 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:03:16  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.124530  data: 0.001332  max mem: 3115
I20241204 07:47:47 2488416 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:10:22  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124525  data: 0.001800  max mem: 3115
I20241204 07:47:50 2488417 dinov2 helpers.py:102] Training  [  550/12500]  eta: 6:56:45  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.126447  data: 0.002295  max mem: 3115
I20241204 07:47:55 2488421 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:12:01  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.123844  data: 0.002117  max mem: 3115
I20241204 07:47:57 2488420 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:11:33  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.122542  data: 0.002416  max mem: 3115
I20241204 07:47:59 2488422 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:13:06  loss: 17.2348 (19.2069)  lr: 0.0000 (0.0000)  time: 2.125132  data: 0.002170  max mem: 3113
I20241204 07:48:00 2488418 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:12:44  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124650  data: 0.003772  max mem: 3115
I20241204 07:48:03 2488423 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:01:39  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125314  data: 0.001228  max mem: 3115
I20241204 07:48:07 2488419 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:02:56  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.124662  data: 0.001633  max mem: 3115
I20241204 07:48:08 2488416 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:09:53  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.124214  data: 0.001588  max mem: 3115
I20241204 07:48:11 2488417 dinov2 helpers.py:102] Training  [  560/12500]  eta: 6:56:31  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125099  data: 0.001434  max mem: 3115
I20241204 07:48:17 2488421 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:11:31  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.125046  data: 0.001851  max mem: 3115
I20241204 07:48:18 2488420 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:11:03  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.123307  data: 0.003261  max mem: 3115
I20241204 07:48:20 2488422 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:12:34  loss: 17.3709 (19.2336)  lr: 0.0000 (0.0000)  time: 2.125774  data: 0.001792  max mem: 3113
I20241204 07:48:21 2488418 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:12:13  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.126335  data: 0.003794  max mem: 3115
I20241204 07:48:24 2488423 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:01:19  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.124998  data: 0.000969  max mem: 3115
I20241204 07:48:28 2488419 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:02:35  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125820  data: 0.002188  max mem: 3115
I20241204 07:48:29 2488416 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:09:25  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125094  data: 0.001474  max mem: 3115
I20241204 07:48:32 2488417 dinov2 helpers.py:102] Training  [  570/12500]  eta: 6:56:17  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.124703  data: 0.001365  max mem: 3115
I20241204 07:48:38 2488421 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:11:00  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125286  data: 0.001431  max mem: 3115
I20241204 07:48:40 2488420 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:10:33  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.123964  data: 0.003278  max mem: 3115
I20241204 07:48:41 2488422 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:12:03  loss: 17.3709 (19.1505)  lr: 0.0000 (0.0000)  time: 2.125552  data: 0.001487  max mem: 3113
I20241204 07:48:43 2488418 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:11:42  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125325  data: 0.003331  max mem: 3115
I20241204 07:48:45 2488423 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:00:59  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.123785  data: 0.000990  max mem: 3115
I20241204 07:48:49 2488419 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:02:14  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125380  data: 0.002336  max mem: 3115
I20241204 07:48:51 2488416 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:08:57  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125842  data: 0.001322  max mem: 3115
I20241204 07:48:53 2488417 dinov2 helpers.py:102] Training  [  580/12500]  eta: 6:56:03  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.125714  data: 0.001741  max mem: 3115
I20241204 07:48:59 2488421 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:10:31  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125504  data: 0.002371  max mem: 3115
I20241204 07:49:01 2488420 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:10:04  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125061  data: 0.002170  max mem: 3115
I20241204 07:49:02 2488422 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:11:32  loss: 18.2288 (19.1649)  lr: 0.0000 (0.0000)  time: 2.124692  data: 0.001694  max mem: 3113
I20241204 07:49:04 2488418 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:11:11  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.124974  data: 0.002729  max mem: 3115
I20241204 07:49:06 2488423 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:00:39  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124286  data: 0.001396  max mem: 3115
I20241204 07:49:10 2488419 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:01:53  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.125578  data: 0.002146  max mem: 3115
I20241204 07:49:12 2488416 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:08:29  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125718  data: 0.001332  max mem: 3115
I20241204 07:49:15 2488417 dinov2 helpers.py:102] Training  [  590/12500]  eta: 6:55:48  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.126150  data: 0.001695  max mem: 3115
I20241204 07:49:20 2488421 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:10:01  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125280  data: 0.002286  max mem: 3115
I20241204 07:49:22 2488420 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:09:34  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125586  data: 0.001926  max mem: 3115
I20241204 07:49:24 2488422 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:11:01  loss: 18.5130 (19.2856)  lr: 0.0000 (0.0000)  time: 2.124168  data: 0.002229  max mem: 3113
I20241204 07:49:25 2488418 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:10:41  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.124951  data: 0.001396  max mem: 3115
I20241204 07:49:28 2488423 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:00:20  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.124524  data: 0.001474  max mem: 3115
I20241204 07:49:32 2488419 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:01:32  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125024  data: 0.002287  max mem: 3115
I20241204 07:49:33 2488416 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:08:01  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124620  data: 0.001500  max mem: 3115
I20241204 07:49:36 2488417 dinov2 helpers.py:102] Training  [  600/12500]  eta: 6:55:33  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125483  data: 0.001573  max mem: 3115
I20241204 07:49:42 2488421 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:09:32  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124745  data: 0.001952  max mem: 3115
I20241204 07:49:43 2488420 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:09:07  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.128078  data: 0.003697  max mem: 3115
I20241204 07:49:45 2488422 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:10:31  loss: 18.5130 (19.2699)  lr: 0.0000 (0.0000)  time: 2.125081  data: 0.001999  max mem: 3113
I20241204 07:49:46 2488418 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:10:11  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.125412  data: 0.001422  max mem: 3115
I20241204 07:49:49 2488423 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:00:00  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125206  data: 0.001592  max mem: 3115
I20241204 07:49:53 2488419 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:01:11  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125095  data: 0.001838  max mem: 3115
I20241204 07:49:54 2488416 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:07:34  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125233  data: 0.002020  max mem: 3115
I20241204 07:49:57 2488417 dinov2 helpers.py:102] Training  [  610/12500]  eta: 6:55:18  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.126323  data: 0.001486  max mem: 3115
I20241204 07:50:03 2488421 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:09:03  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125709  data: 0.001832  max mem: 3115
I20241204 07:50:05 2488420 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:08:38  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.128143  data: 0.003887  max mem: 3115
I20241204 07:50:06 2488422 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:10:00  loss: 18.3581 (19.2352)  lr: 0.0000 (0.0000)  time: 2.125732  data: 0.001559  max mem: 3113
I20241204 07:50:08 2488418 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:09:41  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125544  data: 0.002236  max mem: 3115
I20241204 07:50:10 2488423 dinov2 helpers.py:102] Training  [  610/12500]  eta: 6:59:40  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.125498  data: 0.001721  max mem: 3115
I20241204 07:50:14 2488419 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:00:51  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.126192  data: 0.002109  max mem: 3115
I20241204 07:50:16 2488416 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:07:07  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125940  data: 0.001813  max mem: 3115
I20241204 07:50:18 2488417 dinov2 helpers.py:102] Training  [  620/12500]  eta: 6:55:03  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.126021  data: 0.001436  max mem: 3115
I20241204 07:50:24 2488421 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:08:34  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125494  data: 0.001241  max mem: 3115
I20241204 07:50:26 2488420 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:08:10  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125572  data: 0.001562  max mem: 3115
I20241204 07:50:27 2488422 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:09:31  loss: 18.3581 (19.2963)  lr: 0.0000 (0.0000)  time: 2.125588  data: 0.002690  max mem: 3113
I20241204 07:50:29 2488418 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:09:12  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125191  data: 0.002085  max mem: 3115
I20241204 07:50:31 2488423 dinov2 helpers.py:102] Training  [  620/12500]  eta: 6:59:21  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.125387  data: 0.001484  max mem: 3115
I20241204 07:50:36 2488419 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:00:30  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.127273  data: 0.002990  max mem: 3115
I20241204 07:50:37 2488416 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:06:40  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.125717  data: 0.001978  max mem: 3115
I20241204 07:50:40 2488417 dinov2 helpers.py:102] Training  [  630/12500]  eta: 6:54:47  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124237  data: 0.001785  max mem: 3115
I20241204 07:50:45 2488421 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:08:05  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.124801  data: 0.001584  max mem: 3115
I20241204 07:50:47 2488420 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:07:42  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.125757  data: 0.001391  max mem: 3115
I20241204 07:50:49 2488422 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:09:01  loss: 18.3581 (19.2325)  lr: 0.0000 (0.0000)  time: 2.126100  data: 0.002559  max mem: 3113
I20241204 07:50:50 2488418 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:08:42  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.124897  data: 0.001557  max mem: 3115
I20241204 07:50:53 2488423 dinov2 helpers.py:102] Training  [  630/12500]  eta: 6:59:01  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.125293  data: 0.001721  max mem: 3115
I20241204 07:50:57 2488419 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:00:09  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.126635  data: 0.003100  max mem: 3115
I20241204 07:50:58 2488416 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:06:13  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.125055  data: 0.002727  max mem: 3115
I20241204 07:51:01 2488417 dinov2 helpers.py:102] Training  [  640/12500]  eta: 6:54:32  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124773  data: 0.002055  max mem: 3115
I20241204 07:51:07 2488421 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:07:37  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.124824  data: 0.001588  max mem: 3115
I20241204 07:51:08 2488420 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:07:14  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.126170  data: 0.001304  max mem: 3115
I20241204 07:51:10 2488422 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:08:32  loss: 18.5130 (19.2533)  lr: 0.0000 (0.0000)  time: 2.125848  data: 0.001184  max mem: 3113
I20241204 07:51:11 2488418 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:08:13  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.124643  data: 0.002578  max mem: 3115
I20241204 07:51:14 2488423 dinov2 helpers.py:102] Training  [  640/12500]  eta: 6:58:41  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124411  data: 0.001770  max mem: 3115
I20241204 07:51:18 2488419 dinov2 helpers.py:102] Training  [  640/12500]  eta: 6:59:48  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124040  data: 0.001929  max mem: 3115
I20241204 07:51:19 2488416 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:05:46  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124091  data: 0.001926  max mem: 3115
I20241204 07:51:22 2488417 dinov2 helpers.py:102] Training  [  650/12500]  eta: 6:54:16  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.126092  data: 0.001898  max mem: 3115
I20241204 07:51:28 2488421 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:07:09  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124902  data: 0.001447  max mem: 3115
I20241204 07:51:30 2488420 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:06:47  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.126313  data: 0.001338  max mem: 3115
I20241204 07:51:31 2488422 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:08:03  loss: 19.0997 (19.2902)  lr: 0.0000 (0.0000)  time: 2.124421  data: 0.002033  max mem: 3113
I20241204 07:51:33 2488418 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:07:45  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.126029  data: 0.003514  max mem: 3115
I20241204 07:51:35 2488423 dinov2 helpers.py:102] Training  [  650/12500]  eta: 6:58:21  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124448  data: 0.002886  max mem: 3115
I20241204 07:51:39 2488419 dinov2 helpers.py:102] Training  [  650/12500]  eta: 6:59:27  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124762  data: 0.000961  max mem: 3115
I20241204 07:51:41 2488416 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:05:19  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124650  data: 0.001213  max mem: 3115
I20241204 07:51:43 2488417 dinov2 helpers.py:102] Training  [  660/12500]  eta: 6:54:00  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.126367  data: 0.001460  max mem: 3115
I20241204 07:51:49 2488421 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:06:41  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.125100  data: 0.001380  max mem: 3115
I20241204 07:51:51 2488420 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:06:19  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.125192  data: 0.001344  max mem: 3115
I20241204 07:51:52 2488422 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:07:34  loss: 18.8884 (19.2840)  lr: 0.0000 (0.0000)  time: 2.124512  data: 0.002642  max mem: 3113
I20241204 07:51:54 2488418 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:07:17  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.127014  data: 0.003130  max mem: 3115
I20241204 07:51:56 2488423 dinov2 helpers.py:102] Training  [  660/12500]  eta: 6:58:01  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.124164  data: 0.002816  max mem: 3115
I20241204 07:52:01 2488419 dinov2 helpers.py:102] Training  [  660/12500]  eta: 6:59:06  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.125597  data: 0.001204  max mem: 3115
I20241204 07:52:02 2488416 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:04:53  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124877  data: 0.001151  max mem: 3115
I20241204 07:52:05 2488417 dinov2 helpers.py:102] Training  [  670/12500]  eta: 6:53:44  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.125471  data: 0.001359  max mem: 3115
I20241204 07:52:10 2488421 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:06:13  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124467  data: 0.001865  max mem: 3115
I20241204 07:52:12 2488420 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:05:52  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.125409  data: 0.001433  max mem: 3115
I20241204 07:52:14 2488422 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:07:05  loss: 18.8884 (19.3079)  lr: 0.0000 (0.0000)  time: 2.124857  data: 0.001684  max mem: 3113
I20241204 07:52:15 2488418 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:06:48  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.125918  data: 0.002126  max mem: 3115
I20241204 07:52:18 2488423 dinov2 helpers.py:102] Training  [  670/12500]  eta: 6:57:40  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.123635  data: 0.001723  max mem: 3115
I20241204 07:52:22 2488419 dinov2 helpers.py:102] Training  [  670/12500]  eta: 6:58:45  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.124154  data: 0.001756  max mem: 3115
I20241204 07:52:23 2488416 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:04:27  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.125142  data: 0.001329  max mem: 3115
I20241204 07:52:26 2488417 dinov2 helpers.py:102] Training  [  680/12500]  eta: 6:53:27  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124703  data: 0.002182  max mem: 3115
I20241204 07:52:32 2488421 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:05:45  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.123690  data: 0.002164  max mem: 3115
I20241204 07:52:33 2488420 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:05:24  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.124494  data: 0.001437  max mem: 3115
I20241204 07:52:35 2488422 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:06:37  loss: 18.8884 (19.2943)  lr: 0.0000 (0.0000)  time: 2.124582  data: 0.001089  max mem: 3113
I20241204 07:52:36 2488418 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:06:20  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.123806  data: 0.001624  max mem: 3115
I20241204 07:52:39 2488423 dinov2 helpers.py:102] Training  [  680/12500]  eta: 6:57:20  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124700  data: 0.001600  max mem: 3115
I20241204 07:52:43 2488419 dinov2 helpers.py:102] Training  [  680/12500]  eta: 6:58:24  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124480  data: 0.002126  max mem: 3115
I20241204 07:52:44 2488416 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:04:00  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.125548  data: 0.002069  max mem: 3115
I20241204 07:52:47 2488417 dinov2 helpers.py:102] Training  [  690/12500]  eta: 6:53:11  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124706  data: 0.002315  max mem: 3115
I20241204 07:52:53 2488421 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:05:18  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.124077  data: 0.002934  max mem: 3115
I20241204 07:52:55 2488420 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:04:57  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.123805  data: 0.001462  max mem: 3115
I20241204 07:52:56 2488422 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:06:09  loss: 19.0997 (19.3012)  lr: 0.0000 (0.0000)  time: 2.126217  data: 0.001450  max mem: 3113
I20241204 07:52:58 2488418 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:05:52  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.124682  data: 0.001413  max mem: 3115
I20241204 07:53:00 2488423 dinov2 helpers.py:102] Training  [  690/12500]  eta: 6:57:00  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124767  data: 0.001835  max mem: 3115
I20241204 07:53:04 2488419 dinov2 helpers.py:102] Training  [  690/12500]  eta: 6:58:03  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124837  data: 0.001759  max mem: 3115
I20241204 07:53:06 2488416 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:03:35  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124955  data: 0.001979  max mem: 3115
I20241204 07:53:08 2488417 dinov2 helpers.py:102] Training  [  700/12500]  eta: 6:52:54  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.123124  data: 0.001619  max mem: 3115
I20241204 07:53:14 2488421 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:04:51  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124834  data: 0.002594  max mem: 3115
I20241204 07:53:16 2488420 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:04:30  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124526  data: 0.001444  max mem: 3115
I20241204 07:53:17 2488422 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:05:41  loss: 19.0997 (19.2080)  lr: 0.0000 (0.0000)  time: 2.125657  data: 0.001576  max mem: 3113
I20241204 07:53:19 2488418 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:05:24  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124834  data: 0.001441  max mem: 3115
I20241204 07:53:21 2488423 dinov2 helpers.py:102] Training  [  700/12500]  eta: 6:56:40  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.124339  data: 0.003231  max mem: 3115
I20241204 07:53:26 2488419 dinov2 helpers.py:102] Training  [  700/12500]  eta: 6:57:42  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.124939  data: 0.002482  max mem: 3115
I20241204 07:53:27 2488416 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:03:09  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.002233  max mem: 3115
I20241204 07:53:30 2488417 dinov2 helpers.py:102] Training  [  710/12500]  eta: 6:52:37  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.122656  data: 0.001675  max mem: 3115
I20241204 07:53:35 2488421 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:04:24  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.125173  data: 0.001400  max mem: 3115
I20241204 07:53:37 2488420 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:04:04  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124314  data: 0.001121  max mem: 3115
I20241204 07:53:39 2488422 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:05:14  loss: 19.0997 (19.1619)  lr: 0.0000 (0.0000)  time: 2.124978  data: 0.002058  max mem: 3113
I20241204 07:53:40 2488418 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:04:57  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.124570  data: 0.001734  max mem: 3115
I20241204 07:53:43 2488423 dinov2 helpers.py:102] Training  [  710/12500]  eta: 6:56:20  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.124805  data: 0.002974  max mem: 3115
I20241204 07:53:47 2488419 dinov2 helpers.py:102] Training  [  710/12500]  eta: 6:57:21  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125943  data: 0.002594  max mem: 3115
I20241204 07:53:48 2488416 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:02:43  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.126149  data: 0.002426  max mem: 3115
I20241204 07:53:51 2488417 dinov2 helpers.py:102] Training  [  720/12500]  eta: 6:52:20  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.125839  data: 0.001578  max mem: 3115
I20241204 07:53:57 2488421 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:03:57  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.125791  data: 0.001459  max mem: 3115
I20241204 07:53:58 2488420 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:03:37  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.124390  data: 0.001997  max mem: 3115
I20241204 07:54:00 2488422 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:04:46  loss: 18.8884 (19.0967)  lr: 0.0000 (0.0000)  time: 2.125606  data: 0.002036  max mem: 3113
I20241204 07:54:01 2488418 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:04:29  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.124610  data: 0.001671  max mem: 3115
I20241204 07:54:04 2488423 dinov2 helpers.py:102] Training  [  720/12500]  eta: 6:56:00  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.126276  data: 0.003144  max mem: 3115
I20241204 07:54:08 2488419 dinov2 helpers.py:102] Training  [  720/12500]  eta: 6:57:00  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.125886  data: 0.001547  max mem: 3115
I20241204 07:54:09 2488416 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:02:18  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125513  data: 0.001547  max mem: 3115
I20241204 07:54:12 2488417 dinov2 helpers.py:102] Training  [  730/12500]  eta: 6:52:03  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.126984  data: 0.001613  max mem: 3115
I20241204 07:54:18 2488421 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:03:30  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125489  data: 0.001360  max mem: 3115
I20241204 07:54:20 2488420 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:03:11  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.124617  data: 0.002544  max mem: 3115
I20241204 07:54:21 2488422 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:04:19  loss: 18.3949 (19.0409)  lr: 0.0000 (0.0000)  time: 2.125468  data: 0.001722  max mem: 3113
I20241204 07:54:23 2488418 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:04:03  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125278  data: 0.001949  max mem: 3115
I20241204 07:54:25 2488423 dinov2 helpers.py:102] Training  [  730/12500]  eta: 6:55:40  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.126573  data: 0.002703  max mem: 3115
I20241204 07:54:29 2488419 dinov2 helpers.py:102] Training  [  730/12500]  eta: 6:56:39  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125994  data: 0.001394  max mem: 3115
I20241204 07:54:31 2488416 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:01:52  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.124794  data: 0.001520  max mem: 3115
I20241204 07:54:33 2488417 dinov2 helpers.py:102] Training  [  740/12500]  eta: 6:51:46  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125181  data: 0.001930  max mem: 3115
I20241204 07:54:39 2488421 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:03:04  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.124372  data: 0.001155  max mem: 3115
I20241204 07:54:41 2488420 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:02:44  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.124929  data: 0.001836  max mem: 3115
I20241204 07:54:42 2488422 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:03:52  loss: 18.3949 (19.0378)  lr: 0.0000 (0.0000)  time: 2.125230  data: 0.002040  max mem: 3113
I20241204 07:54:44 2488418 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:03:36  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.125620  data: 0.002006  max mem: 3115
I20241204 07:54:46 2488423 dinov2 helpers.py:102] Training  [  740/12500]  eta: 6:55:20  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125640  data: 0.001059  max mem: 3115
I20241204 07:54:51 2488419 dinov2 helpers.py:102] Training  [  740/12500]  eta: 6:56:18  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125624  data: 0.001333  max mem: 3115
I20241204 07:54:52 2488416 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:01:27  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125748  data: 0.002344  max mem: 3115
I20241204 07:54:55 2488417 dinov2 helpers.py:102] Training  [  750/12500]  eta: 6:51:29  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.125229  data: 0.001781  max mem: 3115
I20241204 07:55:00 2488421 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:02:38  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125240  data: 0.001926  max mem: 3115
I20241204 07:55:02 2488420 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:02:18  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.124314  data: 0.001420  max mem: 3115
I20241204 07:55:04 2488422 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:03:25  loss: 18.3949 (18.9731)  lr: 0.0000 (0.0000)  time: 2.124940  data: 0.001649  max mem: 3113
I20241204 07:55:05 2488418 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:03:09  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125371  data: 0.001599  max mem: 3115
I20241204 07:55:08 2488423 dinov2 helpers.py:102] Training  [  750/12500]  eta: 6:54:59  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124886  data: 0.001051  max mem: 3115
I20241204 07:55:12 2488419 dinov2 helpers.py:102] Training  [  750/12500]  eta: 6:55:57  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124668  data: 0.001163  max mem: 3115
I20241204 07:55:13 2488416 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:01:02  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125624  data: 0.002148  max mem: 3115
I20241204 07:55:16 2488417 dinov2 helpers.py:102] Training  [  760/12500]  eta: 6:51:12  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.127672  data: 0.001832  max mem: 3115
I20241204 07:55:22 2488421 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:02:11  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125164  data: 0.003182  max mem: 3115
I20241204 07:55:23 2488420 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:01:52  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.123600  data: 0.001397  max mem: 3115
I20241204 07:55:25 2488422 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:02:57  loss: 18.3581 (18.9568)  lr: 0.0000 (0.0000)  time: 2.123901  data: 0.001241  max mem: 3113
I20241204 07:55:26 2488418 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:02:42  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.124683  data: 0.001547  max mem: 3115
I20241204 07:55:29 2488423 dinov2 helpers.py:102] Training  [  760/12500]  eta: 6:54:39  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.123879  data: 0.001246  max mem: 3115
I20241204 07:55:33 2488419 dinov2 helpers.py:102] Training  [  760/12500]  eta: 6:55:35  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.123956  data: 0.001970  max mem: 3115
I20241204 07:55:34 2488416 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:00:37  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124903  data: 0.001262  max mem: 3115
I20241204 07:55:37 2488417 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:50:55  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.127400  data: 0.002975  max mem: 3115
I20241204 07:55:43 2488421 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:01:45  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124697  data: 0.002929  max mem: 3115
I20241204 07:55:45 2488420 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:01:26  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124772  data: 0.002015  max mem: 3115
I20241204 07:55:46 2488422 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:02:31  loss: 18.3581 (18.8799)  lr: 0.0000 (0.0000)  time: 2.124921  data: 0.001479  max mem: 3113
I20241204 07:55:48 2488418 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:02:16  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124482  data: 0.001612  max mem: 3115
I20241204 07:55:50 2488423 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:54:19  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.124135  data: 0.001439  max mem: 3115
I20241204 07:55:54 2488419 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:55:15  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.125136  data: 0.002139  max mem: 3115
I20241204 07:55:56 2488416 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:00:12  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.125560  data: 0.001586  max mem: 3115
I20241204 07:55:59 2488417 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:50:38  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125737  data: 0.002724  max mem: 3115
I20241204 07:56:04 2488421 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:01:20  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.126362  data: 0.002192  max mem: 3115
I20241204 07:56:06 2488420 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:01:01  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.125122  data: 0.002190  max mem: 3115
I20241204 07:56:07 2488422 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:02:04  loss: 17.9727 (18.8682)  lr: 0.0000 (0.0000)  time: 2.126230  data: 0.001830  max mem: 3113
I20241204 07:56:09 2488418 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:01:50  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.126939  data: 0.001994  max mem: 3115
I20241204 07:56:11 2488423 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:53:58  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.124500  data: 0.001555  max mem: 3115
I20241204 07:56:16 2488419 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:54:54  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125996  data: 0.001335  max mem: 3115
I20241204 07:56:17 2488416 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:59:47  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.126182  data: 0.002332  max mem: 3115
I20241204 07:56:20 2488417 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:50:20  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.126678  data: 0.001766  max mem: 3115
I20241204 07:56:25 2488421 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:00:54  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.127151  data: 0.001666  max mem: 3115
I20241204 07:56:27 2488420 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:00:35  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.124854  data: 0.002803  max mem: 3115
I20241204 07:56:29 2488422 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:01:38  loss: 17.7523 (18.8063)  lr: 0.0000 (0.0000)  time: 2.125436  data: 0.001935  max mem: 3113
I20241204 07:56:30 2488418 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:01:23  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.126600  data: 0.001777  max mem: 3115
I20241204 07:56:33 2488423 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:53:38  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.125949  data: 0.002119  max mem: 3115
I20241204 07:56:37 2488419 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:54:33  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.126106  data: 0.002313  max mem: 3115
I20241204 07:56:38 2488416 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:59:22  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.001834  max mem: 3115
I20241204 07:56:41 2488417 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:50:02  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.125706  data: 0.001578  max mem: 3115
I20241204 07:56:47 2488421 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:00:28  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.126560  data: 0.001897  max mem: 3115
I20241204 07:56:48 2488420 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:00:10  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125136  data: 0.002729  max mem: 3115
I20241204 07:56:50 2488422 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:01:12  loss: 17.7523 (18.8491)  lr: 0.0000 (0.0000)  time: 2.125435  data: 0.002116  max mem: 3113
I20241204 07:56:51 2488418 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:00:57  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.124842  data: 0.001272  max mem: 3115
I20241204 07:56:54 2488423 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:53:18  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126383  data: 0.002415  max mem: 3115
I20241204 07:56:58 2488419 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:54:12  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126396  data: 0.002606  max mem: 3115
I20241204 07:56:59 2488416 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:58:58  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.124550  data: 0.001173  max mem: 3115
I20241204 07:57:02 2488417 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:49:45  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126352  data: 0.002257  max mem: 3115
I20241204 07:57:08 2488421 dinov2 helpers.py:102] Training  [  790/12500]  eta: 7:00:03  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.126959  data: 0.002038  max mem: 3115
I20241204 07:57:10 2488420 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:59:44  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.124987  data: 0.001776  max mem: 3115
I20241204 07:57:11 2488422 dinov2 helpers.py:102] Training  [  790/12500]  eta: 7:00:45  loss: 17.9727 (18.8945)  lr: 0.0000 (0.0000)  time: 2.124557  data: 0.001681  max mem: 3113
I20241204 07:57:13 2488418 dinov2 helpers.py:102] Training  [  790/12500]  eta: 7:00:31  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.125060  data: 0.001189  max mem: 3115
I20241204 07:57:15 2488423 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:52:58  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.125761  data: 0.001996  max mem: 3115
I20241204 07:57:19 2488419 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:53:51  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126152  data: 0.001877  max mem: 3115
I20241204 07:57:21 2488416 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:58:33  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.125658  data: 0.001388  max mem: 3115
I20241204 07:57:24 2488417 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:49:28  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.128073  data: 0.003150  max mem: 3115
I20241204 07:57:29 2488421 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:59:38  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126649  data: 0.001407  max mem: 3115
I20241204 07:57:31 2488420 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:59:19  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.125018  data: 0.002038  max mem: 3115
I20241204 07:57:32 2488422 dinov2 helpers.py:102] Training  [  800/12500]  eta: 7:00:20  loss: 17.7523 (18.8601)  lr: 0.0000 (0.0000)  time: 2.125189  data: 0.001277  max mem: 3113
I20241204 07:57:34 2488418 dinov2 helpers.py:102] Training  [  800/12500]  eta: 7:00:06  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126199  data: 0.001376  max mem: 3115
I20241204 07:57:36 2488423 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:52:38  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.126677  data: 0.001512  max mem: 3115
I20241204 07:57:41 2488419 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:53:30  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.125696  data: 0.001689  max mem: 3115
I20241204 07:57:42 2488416 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:58:09  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126716  data: 0.001440  max mem: 3115
I20241204 07:57:45 2488417 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:49:10  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.126507  data: 0.002494  max mem: 3115
I20241204 07:57:51 2488421 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:59:13  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126929  data: 0.001545  max mem: 3115
I20241204 07:57:52 2488420 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:58:54  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.124746  data: 0.001562  max mem: 3115
I20241204 07:57:54 2488422 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:59:54  loss: 17.9727 (18.8552)  lr: 0.0000 (0.0000)  time: 2.126242  data: 0.001582  max mem: 3113
I20241204 07:57:55 2488418 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:59:40  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126561  data: 0.001632  max mem: 3115
I20241204 07:57:58 2488423 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:52:17  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.125551  data: 0.001471  max mem: 3115
I20241204 07:58:02 2488419 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:53:09  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.124678  data: 0.002504  max mem: 3115
I20241204 07:58:03 2488416 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:57:45  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.127187  data: 0.001147  max mem: 3115
I20241204 07:58:06 2488417 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:48:52  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125535  data: 0.001653  max mem: 3115
I20241204 07:58:12 2488421 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:58:48  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.127851  data: 0.001695  max mem: 3115
I20241204 07:58:13 2488420 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:58:29  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.125415  data: 0.001082  max mem: 3115
I20241204 07:58:15 2488422 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:59:28  loss: 17.7523 (18.8286)  lr: 0.0000 (0.0000)  time: 2.126453  data: 0.001949  max mem: 3113
I20241204 07:58:16 2488418 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:59:14  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.125690  data: 0.001597  max mem: 3115
I20241204 07:58:19 2488423 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:51:57  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125622  data: 0.001534  max mem: 3115
I20241204 07:58:23 2488419 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:52:48  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125516  data: 0.002746  max mem: 3115
I20241204 07:58:24 2488416 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:57:21  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.127015  data: 0.001522  max mem: 3115
I20241204 07:58:27 2488417 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:48:34  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.126578  data: 0.001266  max mem: 3115
I20241204 07:58:33 2488421 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:58:23  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.126573  data: 0.002384  max mem: 3115
I20241204 07:58:35 2488420 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:58:04  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.126290  data: 0.001523  max mem: 3115
I20241204 07:58:36 2488422 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:59:03  loss: 17.7523 (18.8413)  lr: 0.0000 (0.0000)  time: 2.128327  data: 0.002406  max mem: 3113
I20241204 07:58:38 2488418 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:58:49  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.125374  data: 0.001273  max mem: 3115
I20241204 07:58:40 2488423 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:51:36  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.125673  data: 0.001265  max mem: 3115
I20241204 07:58:44 2488419 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:52:27  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.126318  data: 0.001682  max mem: 3115
I20241204 07:58:46 2488416 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:56:56  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125133  data: 0.001611  max mem: 3115
I20241204 07:58:49 2488417 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:48:16  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.126572  data: 0.001344  max mem: 3115
I20241204 07:58:54 2488421 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:57:58  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125950  data: 0.002576  max mem: 3115
I20241204 07:58:56 2488420 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:57:39  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125526  data: 0.001985  max mem: 3115
I20241204 07:58:58 2488422 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:58:37  loss: 17.7523 (18.8601)  lr: 0.0000 (0.0000)  time: 2.127198  data: 0.001924  max mem: 3113
I20241204 07:58:59 2488418 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:58:24  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.126536  data: 0.001562  max mem: 3115
I20241204 07:59:01 2488423 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:51:16  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125628  data: 0.001804  max mem: 3115
I20241204 07:59:06 2488419 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:52:06  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.127408  data: 0.002599  max mem: 3115
I20241204 07:59:07 2488416 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:56:32  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.124855  data: 0.001345  max mem: 3115
I20241204 07:59:10 2488417 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:47:57  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.124756  data: 0.001383  max mem: 3115
I20241204 07:59:16 2488421 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:57:33  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.125518  data: 0.001585  max mem: 3115
I20241204 07:59:17 2488420 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:57:14  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.124387  data: 0.001935  max mem: 3115
I20241204 07:59:19 2488422 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:58:12  loss: 16.6466 (18.8289)  lr: 0.0000 (0.0000)  time: 2.125506  data: 0.001295  max mem: 3113
I20241204 07:59:20 2488418 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:57:58  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.125950  data: 0.001631  max mem: 3115
I20241204 07:59:23 2488423 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:50:55  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.126028  data: 0.002160  max mem: 3115
I20241204 07:59:27 2488419 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:51:45  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.126703  data: 0.002697  max mem: 3115
I20241204 07:59:28 2488416 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:56:08  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125691  data: 0.001535  max mem: 3115
I20241204 07:59:31 2488417 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:47:39  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.124126  data: 0.001196  max mem: 3115
I20241204 07:59:37 2488421 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:57:08  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125083  data: 0.001172  max mem: 3115
I20241204 07:59:38 2488420 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:56:49  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.124192  data: 0.001684  max mem: 3115
I20241204 07:59:40 2488422 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:57:47  loss: 16.6466 (18.8059)  lr: 0.0000 (0.0000)  time: 2.125586  data: 0.001392  max mem: 3113
I20241204 07:59:41 2488418 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:57:33  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.124926  data: 0.001612  max mem: 3115
I20241204 07:59:44 2488423 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:50:35  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125703  data: 0.001536  max mem: 3115
I20241204 07:59:48 2488419 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:51:24  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125029  data: 0.001464  max mem: 3115
I20241204 07:59:49 2488416 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:55:44  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.125742  data: 0.001550  max mem: 3115
I20241204 07:59:52 2488417 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:47:21  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126091  data: 0.001733  max mem: 3115
I20241204 07:59:58 2488421 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:56:43  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.125263  data: 0.001837  max mem: 3115
I20241204 08:00:00 2488420 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:56:25  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.124426  data: 0.001515  max mem: 3115
I20241204 08:00:01 2488422 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:57:21  loss: 16.1788 (18.7375)  lr: 0.0000 (0.0000)  time: 2.125877  data: 0.001356  max mem: 3113
I20241204 08:00:03 2488418 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:57:08  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.126139  data: 0.001964  max mem: 3115
I20241204 08:00:05 2488423 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:50:15  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126744  data: 0.002235  max mem: 3115
I20241204 08:00:09 2488419 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:51:03  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.125885  data: 0.001603  max mem: 3115
I20241204 08:00:11 2488416 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:55:20  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.127882  data: 0.003304  max mem: 3115
I20241204 08:00:14 2488417 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:47:02  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.127127  data: 0.001642  max mem: 3115
I20241204 08:00:19 2488421 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:18  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125932  data: 0.002097  max mem: 3115
I20241204 08:00:21 2488420 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:00  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125031  data: 0.001537  max mem: 3115
I20241204 08:00:23 2488422 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:56  loss: 16.1788 (18.6667)  lr: 0.0000 (0.0000)  time: 2.126137  data: 0.001335  max mem: 3113
I20241204 08:00:24 2488418 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:43  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.126022  data: 0.001837  max mem: 3115
I20241204 08:00:27 2488423 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:49:54  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126812  data: 0.002353  max mem: 3115
I20241204 08:00:31 2488419 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:50:42  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126331  data: 0.001802  max mem: 3115
I20241204 08:00:32 2488416 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:54:57  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.128030  data: 0.004065  max mem: 3115
I20241204 08:00:35 2488417 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:46:44  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.125989  data: 0.001183  max mem: 3115
I20241204 08:00:41 2488421 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:55:54  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126568  data: 0.001996  max mem: 3115
I20241204 08:00:42 2488420 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:55:36  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.125909  data: 0.001646  max mem: 3115
I20241204 08:00:44 2488422 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:56:31  loss: 16.6466 (18.6853)  lr: 0.0000 (0.0000)  time: 2.125532  data: 0.001358  max mem: 3113
I20241204 08:00:45 2488418 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:56:18  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.125507  data: 0.001371  max mem: 3115
I20241204 08:00:48 2488423 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:49:34  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.126379  data: 0.001416  max mem: 3115
I20241204 08:00:52 2488419 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:50:21  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.126049  data: 0.001704  max mem: 3115
I20241204 08:00:53 2488416 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:54:33  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.127131  data: 0.003192  max mem: 3115
I20241204 08:00:56 2488417 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:46:25  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.125840  data: 0.001142  max mem: 3115
I20241204 08:01:02 2488421 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:55:30  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126462  data: 0.001804  max mem: 3115
I20241204 08:01:03 2488420 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:55:12  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126068  data: 0.001672  max mem: 3115
I20241204 08:01:05 2488422 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:56:06  loss: 16.8232 (18.6990)  lr: 0.0000 (0.0000)  time: 2.125636  data: 0.001345  max mem: 3113
I20241204 08:01:07 2488418 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:55:54  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126562  data: 0.001545  max mem: 3115
I20241204 08:01:09 2488423 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:49:14  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.127834  data: 0.002390  max mem: 3115
I20241204 08:01:13 2488419 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:50:00  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.126148  data: 0.001449  max mem: 3115
I20241204 08:01:15 2488416 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:54:09  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.127130  data: 0.002670  max mem: 3115
I20241204 08:01:17 2488417 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:46:07  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.127119  data: 0.001521  max mem: 3115
I20241204 08:01:23 2488421 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:55:05  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.126426  data: 0.001297  max mem: 3115
I20241204 08:01:25 2488420 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:54:47  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.125994  data: 0.001484  max mem: 3115
I20241204 08:01:26 2488422 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:55:41  loss: 17.7523 (18.6943)  lr: 0.0000 (0.0000)  time: 2.125890  data: 0.001412  max mem: 3113
I20241204 08:01:28 2488418 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:55:29  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.127370  data: 0.002154  max mem: 3115
I20241204 08:01:30 2488423 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:48:54  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.128770  data: 0.002436  max mem: 3115
I20241204 08:01:35 2488419 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:49:39  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.125918  data: 0.001345  max mem: 3115
I20241204 08:01:36 2488416 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:53:46  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.127039  data: 0.002778  max mem: 3115
I20241204 08:01:39 2488417 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:45:48  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.127155  data: 0.001581  max mem: 3115
I20241204 08:01:44 2488421 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:54:41  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.126757  data: 0.001675  max mem: 3115
I20241204 08:01:46 2488420 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:54:23  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.126765  data: 0.001700  max mem: 3115
I20241204 08:01:48 2488422 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:55:17  loss: 16.8232 (18.6182)  lr: 0.0000 (0.0000)  time: 2.126326  data: 0.001623  max mem: 3113
I20241204 08:01:49 2488418 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:55:05  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.127801  data: 0.001910  max mem: 3115
I20241204 08:01:52 2488423 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:48:33  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.127465  data: 0.001552  max mem: 3115
I20241204 08:01:56 2488419 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:49:18  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.125847  data: 0.001856  max mem: 3115
I20241204 08:01:57 2488416 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:53:22  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.126924  data: 0.002415  max mem: 3115
I20241204 08:02:00 2488417 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:45:30  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126652  data: 0.001082  max mem: 3115
I20241204 08:02:06 2488421 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:54:17  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.126166  data: 0.001869  max mem: 3115
I20241204 08:02:07 2488420 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:53:59  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.126436  data: 0.001962  max mem: 3115
I20241204 08:02:09 2488422 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:54:52  loss: 16.8232 (18.5817)  lr: 0.0000 (0.0000)  time: 2.125487  data: 0.001799  max mem: 3113
I20241204 08:02:10 2488418 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:54:40  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.127287  data: 0.001460  max mem: 3115
I20241204 08:02:13 2488423 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:48:12  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.125388  data: 0.001286  max mem: 3115
I20241204 08:02:17 2488419 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:48:57  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126691  data: 0.002414  max mem: 3115
I20241204 08:02:18 2488416 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:52:58  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.125026  data: 0.001348  max mem: 3115
I20241204 08:02:21 2488417 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:45:11  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.126595  data: 0.001384  max mem: 3115
I20241204 08:02:27 2488421 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:53:52  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.125739  data: 0.001698  max mem: 3115
I20241204 08:02:29 2488420 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:53:35  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.124801  data: 0.001664  max mem: 3115
I20241204 08:02:30 2488422 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:54:27  loss: 16.8232 (18.5670)  lr: 0.0000 (0.0000)  time: 2.125721  data: 0.002080  max mem: 3113
I20241204 08:02:32 2488418 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:54:16  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.126341  data: 0.001300  max mem: 3115
I20241204 08:02:34 2488423 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:47:52  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.124378  data: 0.000903  max mem: 3115
I20241204 08:02:38 2488419 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:48:36  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.127109  data: 0.001926  max mem: 3115
I20241204 08:02:40 2488416 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:52:35  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126028  data: 0.001689  max mem: 3115
I20241204 08:02:43 2488417 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:44:53  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.127160  data: 0.003110  max mem: 3115
I20241204 08:02:48 2488421 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:53:28  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.125434  data: 0.001499  max mem: 3115
I20241204 08:02:50 2488420 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:53:11  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.125794  data: 0.002057  max mem: 3115
I20241204 08:02:51 2488422 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:54:02  loss: 16.8232 (18.5277)  lr: 0.0000 (0.0000)  time: 2.126059  data: 0.001841  max mem: 3113
I20241204 08:02:53 2488418 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:53:51  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126226  data: 0.001650  max mem: 3115
I20241204 08:02:55 2488423 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:47:31  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.126196  data: 0.001247  max mem: 3115
I20241204 08:03:00 2488419 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:48:15  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.126493  data: 0.002020  max mem: 3115
I20241204 08:03:01 2488416 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:52:11  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.127217  data: 0.001684  max mem: 3115
I20241204 08:03:04 2488417 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:44:34  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.127085  data: 0.002970  max mem: 3115
I20241204 08:03:09 2488421 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:53:04  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.124517  data: 0.001186  max mem: 3115
I20241204 08:03:11 2488420 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:52:47  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.126015  data: 0.002157  max mem: 3115
I20241204 08:03:13 2488422 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:53:38  loss: 16.6466 (18.4983)  lr: 0.0000 (0.0000)  time: 2.126848  data: 0.001645  max mem: 3113
I20241204 08:03:14 2488418 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:53:27  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.126343  data: 0.001828  max mem: 3115
I20241204 08:03:17 2488423 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:47:11  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.126946  data: 0.002306  max mem: 3115
I20241204 08:03:21 2488419 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:47:54  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.126331  data: 0.002137  max mem: 3115
I20241204 08:03:22 2488416 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:51:48  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.125635  data: 0.001326  max mem: 3115
I20241204 08:03:25 2488417 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:44:15  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.125552  data: 0.001612  max mem: 3115
I20241204 08:03:31 2488421 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:52:39  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.124629  data: 0.001503  max mem: 3115
I20241204 08:03:32 2488420 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:52:23  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.125036  data: 0.001678  max mem: 3115
I20241204 08:03:34 2488422 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:53:14  loss: 16.8232 (18.5466)  lr: 0.0000 (0.0000)  time: 2.127993  data: 0.001780  max mem: 3113
I20241204 08:03:35 2488418 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:53:02  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.124974  data: 0.001374  max mem: 3115
I20241204 08:03:38 2488423 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:46:50  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.126025  data: 0.002131  max mem: 3115
I20241204 08:03:42 2488419 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:47:33  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.125854  data: 0.002112  max mem: 3115
I20241204 08:03:43 2488416 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:51:24  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.125000  data: 0.001495  max mem: 3115
I20241204 08:03:46 2488417 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:43:56  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.125229  data: 0.001674  max mem: 3115
I20241204 08:03:52 2488421 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:52:15  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.124385  data: 0.001591  max mem: 3115
I20241204 08:03:54 2488420 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:51:59  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.124380  data: 0.001395  max mem: 3115
I20241204 08:03:55 2488422 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:52:50  loss: 16.6466 (18.4968)  lr: 0.0000 (0.0000)  time: 2.126104  data: 0.001545  max mem: 3113
I20241204 08:03:57 2488418 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:52:38  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.123832  data: 0.001330  max mem: 3115
I20241204 08:03:59 2488423 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:46:29  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.124389  data: 0.001440  max mem: 3115
I20241204 08:04:03 2488419 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:47:11  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.123930  data: 0.002224  max mem: 3115
I20241204 08:04:05 2488416 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:51:01  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.123971  data: 0.001459  max mem: 3115
I20241204 08:04:08 2488417 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:43:37  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.125238  data: 0.001252  max mem: 3115
I20241204 08:04:13 2488421 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:51:51  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.124221  data: 0.001876  max mem: 3115
I20241204 08:04:15 2488420 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:51:35  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.125257  data: 0.001931  max mem: 3115
I20241204 08:04:16 2488422 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:52:25  loss: 16.1788 (18.4719)  lr: 0.0000 (0.0000)  time: 2.124608  data: 0.001487  max mem: 3113
I20241204 08:04:18 2488418 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:52:14  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.126559  data: 0.002977  max mem: 3115
I20241204 08:04:20 2488423 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:46:09  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.125331  data: 0.001934  max mem: 3115
I20241204 08:04:25 2488419 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:46:50  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.123844  data: 0.001452  max mem: 3115
I20241204 08:04:26 2488416 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:50:37  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.123626  data: 0.001895  max mem: 3115
I20241204 08:04:29 2488417 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:43:18  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125777  data: 0.001172  max mem: 3115
I20241204 08:04:34 2488421 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:51:27  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.124373  data: 0.002112  max mem: 3115
I20241204 08:04:36 2488420 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:51:11  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.125959  data: 0.002040  max mem: 3115
I20241204 08:04:38 2488422 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:52:01  loss: 16.6466 (18.4634)  lr: 0.0000 (0.0000)  time: 2.124159  data: 0.001566  max mem: 3113
I20241204 08:04:39 2488418 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:51:50  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.127447  data: 0.002839  max mem: 3115
I20241204 08:04:42 2488423 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:45:48  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.126234  data: 0.001720  max mem: 3115
I20241204 08:04:46 2488419 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:46:29  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125522  data: 0.001280  max mem: 3115
I20241204 08:04:47 2488416 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:50:14  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.124681  data: 0.001811  max mem: 3115
I20241204 08:04:50 2488417 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:42:59  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.125810  data: 0.001239  max mem: 3115
I20241204 08:04:56 2488421 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:51:03  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.124887  data: 0.001938  max mem: 3115
I20241204 08:04:57 2488420 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:50:48  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.124948  data: 0.001244  max mem: 3115
I20241204 08:04:59 2488422 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:51:36  loss: 16.6466 (18.4552)  lr: 0.0000 (0.0000)  time: 2.124202  data: 0.002062  max mem: 3113
I20241204 08:05:00 2488418 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:51:26  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.125765  data: 0.002659  max mem: 3115
I20241204 08:05:03 2488423 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:45:27  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.124934  data: 0.001418  max mem: 3115
I20241204 08:05:07 2488419 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:46:07  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.124810  data: 0.001344  max mem: 3115
I20241204 08:05:08 2488416 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:49:50  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.124000  data: 0.001235  max mem: 3115
I20241204 08:05:11 2488417 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:42:40  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.125896  data: 0.001533  max mem: 3115
I20241204 08:05:17 2488421 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:50:39  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125925  data: 0.002592  max mem: 3115
I20241204 08:05:19 2488420 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:50:24  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125099  data: 0.001110  max mem: 3115
I20241204 08:05:20 2488422 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:51:12  loss: 16.1788 (18.4284)  lr: 0.0000 (0.0000)  time: 2.124055  data: 0.002023  max mem: 3113
I20241204 08:05:22 2488418 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:51:02  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125499  data: 0.002741  max mem: 3115
I20241204 08:05:24 2488423 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:45:06  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.124628  data: 0.002460  max mem: 3115
I20241204 08:05:28 2488419 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:45:46  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.125249  data: 0.002463  max mem: 3115
I20241204 08:05:30 2488416 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:49:27  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.123172  data: 0.001301  max mem: 3115
I20241204 08:05:33 2488417 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:42:20  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124552  data: 0.001773  max mem: 3115
I20241204 08:05:38 2488421 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:16  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.125579  data: 0.002741  max mem: 3115
I20241204 08:05:40 2488420 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:00  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.124680  data: 0.001174  max mem: 3115
I20241204 08:05:41 2488422 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:48  loss: 16.0023 (18.3703)  lr: 0.0000 (0.0000)  time: 2.123977  data: 0.001552  max mem: 3113
I20241204 08:05:43 2488418 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:38  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.124519  data: 0.001289  max mem: 3115
I20241204 08:05:45 2488423 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:44:46  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124561  data: 0.002325  max mem: 3115
I20241204 08:05:50 2488419 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:45:25  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.125772  data: 0.002482  max mem: 3115
I20241204 08:05:51 2488416 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:49:03  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.123295  data: 0.001207  max mem: 3115
I20241204 08:05:54 2488417 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:42:01  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.123580  data: 0.001339  max mem: 3115
I20241204 08:05:59 2488421 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:49:52  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.124410  data: 0.002066  max mem: 3115
I20241204 08:06:01 2488420 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:49:36  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.123629  data: 0.001161  max mem: 3115
I20241204 08:06:03 2488422 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:50:23  loss: 15.9833 (18.3475)  lr: 0.0000 (0.0000)  time: 2.123766  data: 0.001479  max mem: 3113
I20241204 08:06:04 2488418 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:50:13  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.124102  data: 0.001215  max mem: 3115
I20241204 08:06:07 2488423 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:44:25  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.124042  data: 0.001747  max mem: 3115
I20241204 08:06:11 2488419 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:45:04  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.123448  data: 0.001360  max mem: 3115
I20241204 08:06:12 2488416 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:48:40  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124233  data: 0.002279  max mem: 3115
I20241204 08:06:15 2488417 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:41:41  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.124261  data: 0.001266  max mem: 3115
I20241204 08:06:21 2488421 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:28  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124510  data: 0.001659  max mem: 3115
I20241204 08:06:22 2488420 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:13  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.122598  data: 0.001200  max mem: 3115
I20241204 08:06:24 2488422 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:59  loss: 15.6870 (18.3086)  lr: 0.0000 (0.0000)  time: 2.123751  data: 0.001409  max mem: 3113
I20241204 08:06:25 2488418 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:50  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.125306  data: 0.001103  max mem: 3115
I20241204 08:06:28 2488423 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:44:04  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123715  data: 0.001914  max mem: 3115
I20241204 08:06:32 2488419 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:44:43  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.124569  data: 0.001327  max mem: 3115
I20241204 08:06:33 2488416 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:48:17  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.124705  data: 0.002352  max mem: 3115
I20241204 08:06:36 2488417 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:41:22  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.123593  data: 0.001403  max mem: 3115
I20241204 08:06:42 2488421 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:49:04  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.123768  data: 0.001571  max mem: 3115
I20241204 08:06:44 2488420 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:48:49  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.122618  data: 0.001241  max mem: 3115
I20241204 08:06:45 2488422 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:49:35  loss: 15.6761 (18.2813)  lr: 0.0000 (0.0000)  time: 2.124432  data: 0.001971  max mem: 3113
I20241204 08:06:47 2488418 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:49:26  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.124853  data: 0.001677  max mem: 3115
I20241204 08:06:49 2488423 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:43:43  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.124311  data: 0.002505  max mem: 3115
I20241204 08:06:53 2488419 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:44:21  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.125414  data: 0.001182  max mem: 3115
I20241204 08:06:55 2488416 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:47:53  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123809  data: 0.001313  max mem: 3115
I20241204 08:06:58 2488417 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:41:02  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123436  data: 0.001584  max mem: 3115
I20241204 08:07:03 2488421 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:48:40  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123431  data: 0.001375  max mem: 3115
I20241204 08:07:05 2488420 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:48:25  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123709  data: 0.001096  max mem: 3115
I20241204 08:07:06 2488422 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:49:11  loss: 15.6761 (18.2483)  lr: 0.0000 (0.0000)  time: 2.124128  data: 0.002085  max mem: 3113
I20241204 08:07:08 2488418 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:49:02  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.124044  data: 0.001739  max mem: 3115
I20241204 08:07:10 2488423 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:43:22  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.124548  data: 0.002620  max mem: 3115
I20241204 08:07:15 2488419 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:44:00  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123813  data: 0.001237  max mem: 3115
I20241204 08:07:16 2488416 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:47:30  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.124628  data: 0.001589  max mem: 3115
I20241204 08:07:19 2488417 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:40:43  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.123857  data: 0.002749  max mem: 3115
I20241204 08:07:24 2488421 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:16  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.123255  data: 0.001176  max mem: 3115
I20241204 08:07:26 2488420 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:02  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.123476  data: 0.001097  max mem: 3115
I20241204 08:07:28 2488422 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:47  loss: 15.6870 (18.2701)  lr: 0.0000 (0.0000)  time: 2.123385  data: 0.002021  max mem: 3113
I20241204 08:07:29 2488418 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:38  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.124392  data: 0.001567  max mem: 3115
I20241204 08:07:32 2488423 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:43:01  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124574  data: 0.001602  max mem: 3115
I20241204 08:07:36 2488419 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:43:39  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124103  data: 0.001326  max mem: 3115
I20241204 08:07:37 2488416 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:47:07  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.125288  data: 0.001673  max mem: 3115
I20241204 08:07:40 2488417 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:40:23  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.124005  data: 0.002540  max mem: 3115
I20241204 08:07:46 2488421 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:47:53  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123130  data: 0.001287  max mem: 3115
I20241204 08:07:47 2488420 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:47:38  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123006  data: 0.001347  max mem: 3115
I20241204 08:07:49 2488422 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:48:23  loss: 15.6870 (18.2556)  lr: 0.0000 (0.0000)  time: 2.123274  data: 0.002289  max mem: 3113
I20241204 08:07:50 2488418 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:48:14  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123831  data: 0.001815  max mem: 3115
I20241204 08:07:53 2488423 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:42:40  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123640  data: 0.002219  max mem: 3115
I20241204 08:07:57 2488419 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:43:18  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.124847  data: 0.002077  max mem: 3115
I20241204 08:07:58 2488416 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:46:44  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124583  data: 0.001366  max mem: 3115
I20241204 08:08:01 2488417 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:40:04  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.124088  data: 0.001464  max mem: 3115
I20241204 08:08:07 2488421 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:29  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.123507  data: 0.001318  max mem: 3115
I20241204 08:08:08 2488420 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:15  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124474  data: 0.001773  max mem: 3115
I20241204 08:08:10 2488422 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:59  loss: 15.6761 (18.2171)  lr: 0.0000 (0.0000)  time: 2.123899  data: 0.002429  max mem: 3113
I20241204 08:08:12 2488418 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:50  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.123743  data: 0.001510  max mem: 3115
I20241204 08:08:14 2488423 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:42:19  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.123162  data: 0.002398  max mem: 3115
I20241204 08:08:18 2488419 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:42:57  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.125300  data: 0.002842  max mem: 3115
I20241204 08:08:20 2488416 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:46:21  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123936  data: 0.001211  max mem: 3115
I20241204 08:08:23 2488417 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:39:44  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.125094  data: 0.001610  max mem: 3115
I20241204 08:08:28 2488421 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:47:05  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123003  data: 0.001103  max mem: 3115
I20241204 08:08:30 2488420 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:46:51  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.125503  data: 0.001733  max mem: 3115
I20241204 08:08:31 2488422 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:47:35  loss: 15.6761 (18.2229)  lr: 0.0000 (0.0000)  time: 2.124267  data: 0.002112  max mem: 3113
I20241204 08:08:33 2488418 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:47:26  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123734  data: 0.001929  max mem: 3115
I20241204 08:08:35 2488423 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:41:58  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.123830  data: 0.001578  max mem: 3115
I20241204 08:08:40 2488419 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:42:35  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.124149  data: 0.002251  max mem: 3115
I20241204 08:08:41 2488416 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:45:58  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.124100  data: 0.002015  max mem: 3115
I20241204 08:08:44 2488417 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:39:25  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124770  data: 0.001508  max mem: 3115
I20241204 08:08:49 2488421 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:46:42  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.123699  data: 0.001035  max mem: 3115
I20241204 08:08:51 2488420 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:46:28  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.124207  data: 0.001397  max mem: 3115
I20241204 08:08:53 2488422 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:47:11  loss: 15.6761 (18.1944)  lr: 0.0000 (0.0000)  time: 2.124997  data: 0.001744  max mem: 3113
I20241204 08:08:54 2488418 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:47:02  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.123930  data: 0.001866  max mem: 3115
I20241204 08:08:57 2488423 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:41:37  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124156  data: 0.001575  max mem: 3115
I20241204 08:09:01 2488419 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:42:14  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124128  data: 0.001738  max mem: 3115
I20241204 08:09:02 2488416 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:45:35  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.124687  data: 0.002192  max mem: 3115
I20241204 08:09:05 2488417 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:39:05  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123983  data: 0.001801  max mem: 3115
I20241204 08:09:11 2488421 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:18  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.123685  data: 0.002675  max mem: 3115
I20241204 08:09:12 2488420 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:04  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.123156  data: 0.001742  max mem: 3115
I20241204 08:09:14 2488422 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:48  loss: 15.6870 (18.1905)  lr: 0.0000 (0.0000)  time: 2.125226  data: 0.002116  max mem: 3113
I20241204 08:09:15 2488418 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:39  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.124623  data: 0.001707  max mem: 3115
I20241204 08:09:18 2488423 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:41:16  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123813  data: 0.001871  max mem: 3115
I20241204 08:09:22 2488419 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:41:53  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.124568  data: 0.002075  max mem: 3115
I20241204 08:09:23 2488416 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:45:12  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124651  data: 0.001361  max mem: 3115
I20241204 08:09:26 2488417 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:38:45  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.124438  data: 0.001969  max mem: 3115
I20241204 08:09:32 2488421 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:45:55  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124273  data: 0.004432  max mem: 3115
I20241204 08:09:33 2488420 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:45:41  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.123562  data: 0.002422  max mem: 3115
I20241204 08:09:35 2488422 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:46:24  loss: 15.6870 (18.2150)  lr: 0.0000 (0.0000)  time: 2.123675  data: 0.003593  max mem: 3113
I20241204 08:09:37 2488418 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:46:15  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.123691  data: 0.001711  max mem: 3115
I20241204 08:09:39 2488423 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:40:55  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122984  data: 0.002017  max mem: 3115
I20241204 08:09:43 2488419 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:41:31  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.121177  data: 0.001808  max mem: 3115
I20241204 08:09:45 2488416 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:44:49  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123862  data: 0.001150  max mem: 3115
I20241204 08:09:48 2488417 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:38:26  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.124336  data: 0.001695  max mem: 3115
I20241204 08:09:53 2488421 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:45:32  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.125020  data: 0.003885  max mem: 3115
I20241204 08:09:55 2488420 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:45:18  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123110  data: 0.002111  max mem: 3115
I20241204 08:09:56 2488422 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:46:00  loss: 15.6870 (18.1895)  lr: 0.0000 (0.0000)  time: 2.122740  data: 0.003413  max mem: 3113
I20241204 08:09:58 2488418 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:45:51  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.122381  data: 0.001387  max mem: 3115
I20241204 08:10:00 2488423 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:40:34  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.123439  data: 0.001623  max mem: 3115
I20241204 08:10:04 2488419 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:41:09  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.120349  data: 0.001880  max mem: 3115
I20241204 08:10:06 2488416 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:44:26  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122900  data: 0.001813  max mem: 3115
I20241204 08:10:09 2488417 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:38:06  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123721  data: 0.001586  max mem: 3115
I20241204 08:10:14 2488421 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:45:08  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.123545  data: 0.002393  max mem: 3115
I20241204 08:10:16 2488420 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:44:54  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122637  data: 0.001417  max mem: 3115
I20241204 08:10:18 2488422 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:45:36  loss: 15.6870 (18.1641)  lr: 0.0000 (0.0000)  time: 2.122886  data: 0.001923  max mem: 3113
I20241204 08:10:19 2488418 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:45:28  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122881  data: 0.002139  max mem: 3115
I20241204 08:10:22 2488423 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:40:13  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123744  data: 0.001774  max mem: 3115
I20241204 08:10:26 2488419 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:40:48  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.122714  data: 0.001988  max mem: 3115
I20241204 08:10:27 2488416 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:44:03  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.124458  data: 0.002206  max mem: 3115
I20241204 08:10:30 2488417 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:37:46  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.123196  data: 0.002149  max mem: 3115
I20241204 08:10:36 2488421 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:44:45  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.122584  data: 0.001381  max mem: 3115
I20241204 08:10:37 2488420 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:44:32  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.125384  data: 0.001353  max mem: 3115
I20241204 08:10:39 2488422 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:45:12  loss: 15.6870 (18.1653)  lr: 0.0000 (0.0000)  time: 2.121962  data: 0.002675  max mem: 3113
I20241204 08:10:40 2488418 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:45:04  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.121188  data: 0.003899  max mem: 3115
I20241204 08:10:43 2488423 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:39:52  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.121428  data: 0.003215  max mem: 3115
I20241204 08:10:47 2488419 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:40:26  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.121850  data: 0.001550  max mem: 3115
I20241204 08:10:48 2488416 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:43:40  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.124146  data: 0.001833  max mem: 3115
I20241204 08:10:51 2488417 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:37:26  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121296  data: 0.002065  max mem: 3115
I20241204 08:10:57 2488421 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:21  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123291  data: 0.002150  max mem: 3115
I20241204 08:10:58 2488420 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:08  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.122792  data: 0.001286  max mem: 3115
I20241204 08:11:00 2488422 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:49  loss: 15.6870 (18.1173)  lr: 0.0000 (0.0000)  time: 2.122870  data: 0.002854  max mem: 3113
I20241204 08:11:01 2488418 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:40  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.121855  data: 0.003495  max mem: 3115
I20241204 08:11:04 2488423 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:39:31  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.120603  data: 0.002660  max mem: 3115
I20241204 08:11:08 2488419 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:40:05  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.120786  data: 0.001515  max mem: 3115
I20241204 08:11:09 2488416 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:43:17  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.121975  data: 0.003006  max mem: 3115
I20241204 08:11:12 2488417 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:37:06  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.121033  data: 0.001158  max mem: 3115
I20241204 08:11:18 2488421 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:43:58  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.123227  data: 0.002360  max mem: 3115
I20241204 08:11:20 2488420 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:43:44  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.119493  data: 0.001890  max mem: 3115
I20241204 08:11:21 2488422 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:44:25  loss: 15.6870 (18.1253)  lr: 0.0000 (0.0000)  time: 2.123422  data: 0.001705  max mem: 3113
I20241204 08:11:23 2488418 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:44:17  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.123004  data: 0.002051  max mem: 3115
I20241204 08:11:25 2488423 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:39:10  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.122416  data: 0.001214  max mem: 3115
I20241204 08:11:29 2488419 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:39:43  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.121941  data: 0.001662  max mem: 3115
I20241204 08:11:31 2488416 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:42:54  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121676  data: 0.002789  max mem: 3115
I20241204 08:11:34 2488417 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:36:46  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.122381  data: 0.001022  max mem: 3115
I20241204 08:11:39 2488421 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:43:34  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.122235  data: 0.001689  max mem: 3115
I20241204 08:11:41 2488420 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:43:21  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121686  data: 0.002228  max mem: 3115
I20241204 08:11:42 2488422 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:44:02  loss: 15.3930 (18.1002)  lr: 0.0000 (0.0000)  time: 2.122368  data: 0.001177  max mem: 3113
I20241204 08:11:44 2488418 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:43:53  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121549  data: 0.001652  max mem: 3115
I20241204 08:11:46 2488423 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:38:49  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.124067  data: 0.001977  max mem: 3115
I20241204 08:11:51 2488419 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:39:22  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.121882  data: 0.001554  max mem: 3115
I20241204 08:11:52 2488416 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:42:31  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.121569  data: 0.001847  max mem: 3115
I20241204 08:11:55 2488417 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:36:26  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.121997  data: 0.001166  max mem: 3115
I20241204 08:12:00 2488421 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:43:11  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.122975  data: 0.002398  max mem: 3115
I20241204 08:12:02 2488420 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:42:57  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.119756  data: 0.001546  max mem: 3115
I20241204 08:12:04 2488422 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:43:38  loss: 15.3930 (18.1000)  lr: 0.0000 (0.0000)  time: 2.122824  data: 0.001350  max mem: 3113
I20241204 08:12:05 2488418 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:43:29  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.122193  data: 0.001184  max mem: 3115
I20241204 08:12:08 2488423 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:38:28  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.124126  data: 0.002025  max mem: 3115
I20241204 08:12:12 2488419 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:39:00  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.121494  data: 0.001298  max mem: 3115
I20241204 08:12:13 2488416 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:42:08  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.122008  data: 0.002140  max mem: 3115
I20241204 08:12:16 2488417 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:36:06  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 2.122141  data: 0.001632  max mem: 3115
I20241204 08:12:22 2488421 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:42:48  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.123092  data: 0.002611  max mem: 3115
I20241204 08:12:23 2488420 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:42:34  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.118448  data: 0.001295  max mem: 3115
I20241204 08:12:25 2488422 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:43:14  loss: 15.3930 (18.0788)  lr: 0.0000 (0.0000)  time: 2.121798  data: 0.001391  max mem: 3113
I20241204 08:12:26 2488418 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:43:06  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.124045  data: 0.002603  max mem: 3115
I20241204 08:12:29 2488423 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:38:07  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 2.123180  data: 0.001632  max mem: 3115
I20241204 08:12:33 2488419 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:38:39  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 2.122307  data: 0.001556  max mem: 3115
I20241204 08:12:34 2488416 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:41:45  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.121747  data: 0.001610  max mem: 3115
I20241204 08:12:35 2488417 dinov2 linear.py:272] running validation !
E20241204 08:12:35 2488417 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:12:42 2488421 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:16  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.073720  data: 0.001663  max mem: 3115
I20241204 08:12:43 2488420 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:00  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.062836  data: 0.001338  max mem: 3115
I20241204 08:12:45 2488422 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:38  loss: 15.4914 (18.0946)  lr: 0.0000 (0.0000)  time: 2.053779  data: 0.002259  max mem: 3113
I20241204 08:12:46 2488423 dinov2 linear.py:272] running validation !
E20241204 08:12:46 2488423 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:12:46 2488418 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:28  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.045110  data: 0.003030  max mem: 3115
I20241204 08:12:49 2488419 dinov2 linear.py:272] running validation !
E20241204 08:12:49 2488419 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:12:52 2488416 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:46  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.925996  data: 0.001376  max mem: 3115
I20241204 08:12:57 2488421 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:57  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.766263  data: 0.001855  max mem: 3115
I20241204 08:12:58 2488420 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:38  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.737253  data: 0.001541  max mem: 3115
I20241204 08:12:59 2488422 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:41:12  loss: 15.4914 (18.0996)  lr: 0.0000 (0.0000)  time: 1.705551  data: 0.002176  max mem: 3113
I20241204 08:13:00 2488418 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:58  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.676154  data: 0.001676  max mem: 3115
I20241204 08:13:03 2488416 dinov2 linear.py:272] running validation !
E20241204 08:13:03 2488416 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:13:08 2488421 dinov2 linear.py:272] running validation !
E20241204 08:13:08 2488421 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:13:08 2488420 dinov2 linear.py:272] running validation !
E20241204 08:13:08 2488420 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:13:09 2488422 dinov2 linear.py:272] running validation !
E20241204 08:13:09 2488422 submitit submission.py:68] Submitted job triggered an exception
I20241204 08:13:09 2488418 dinov2 linear.py:272] running validation !
E20241204 08:13:09 2488418 submitit submission.py:68] Submitted job triggered an exception
