submitit INFO (2024-12-04 07:27:40,480) - Starting with JobEnvironment(job_id=2488412, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-04 07:27:40,481) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender/2488412_submitted.pkl
I20241204 07:27:49 2488421 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:49 2488421 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:49 2488421 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:49 2488421 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:49 2488421 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:23 2488421 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:28 2488421 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:28 2488421 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 07:28:34 2488421 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:40 2488421 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:40 2488421 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:40 2488421 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:40 2488421 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:40 2488421 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:40 2488421 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 07:28:45 2488421 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:45 2488421 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:45 2488421 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:45 2488421 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:45 2488421 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:45 2488421 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:45 2488421 dinov2 linear.py:338] Starting training from iteration 0
lr 4.999999921043166e-06
I20241204 07:29:10 2488421 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 12:19:30  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 24.285614  data: 16.912766  max mem: 2706
I20241204 07:29:13 2488421 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
lr 4.999990446229023e-06
I20241204 07:29:31 2488421 dinov2 helpers.py:102] Training  [   10/12500]  eta: 14:30:20  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 4.180958  data: 1.635488  max mem: 3115
lr 4.999965180116501e-06
I20241204 07:29:52 2488421 dinov2 helpers.py:102] Training  [   20/12500]  eta: 11:02:27  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 2.129822  data: 0.054648  max mem: 3115
lr 4.999924122865191e-06
I20241204 07:30:13 2488421 dinov2 helpers.py:102] Training  [   30/12500]  eta: 9:49:31  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.097103  data: 0.001710  max mem: 3115
lr 4.999867274734432e-06
I20241204 07:30:34 2488421 dinov2 helpers.py:102] Training  [   40/12500]  eta: 9:12:18  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.108013  data: 0.001902  max mem: 3115
lr 4.999794636083308e-06
I20241204 07:30:56 2488421 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:49:38  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.112233  data: 0.001622  max mem: 3115
lr 4.999706207370645e-06
I20241204 07:31:17 2488421 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:34:27  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.115782  data: 0.001259  max mem: 3115
lr 4.999601989155004e-06
I20241204 07:31:38 2488421 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:23:38  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.121639  data: 0.001627  max mem: 3115
lr 4.999481982094688e-06
I20241204 07:31:59 2488421 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:15:30  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.126759  data: 0.004259  max mem: 3115
lr 4.9993461869477276e-06
I20241204 07:32:21 2488421 dinov2 helpers.py:102] Training  [   90/12500]  eta: 8:08:59  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.126544  data: 0.004199  max mem: 3115
lr 4.999194604571874e-06
I20241204 07:32:42 2488421 dinov2 helpers.py:102] Training  [  100/12500]  eta: 8:03:39  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.123962  data: 0.001594  max mem: 3115
lr 4.999027235924608e-06
I20241204 07:33:03 2488421 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:59:16  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.124264  data: 0.001777  max mem: 3115
lr 4.998844082063119e-06
I20241204 07:33:24 2488421 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:55:32  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.124753  data: 0.002602  max mem: 3115
lr 4.998645144144304e-06
I20241204 07:33:45 2488421 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:52:16  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.123279  data: 0.003786  max mem: 3115
lr 4.998430423424764e-06
I20241204 07:34:07 2488421 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:49:29  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.123900  data: 0.002970  max mem: 3115
lr 4.9981999212607945e-06
I20241204 07:34:28 2488421 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:47:00  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.125499  data: 0.001524  max mem: 3115
lr 4.997953639108375e-06
I20241204 07:34:49 2488421 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:44:45  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.124039  data: 0.001419  max mem: 3115
lr 4.997691578523149e-06
I20241204 07:35:10 2488421 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:42:45  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.123337  data: 0.001064  max mem: 3115
lr 4.9974137411604395e-06
I20241204 07:35:32 2488421 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:40:57  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.125506  data: 0.001491  max mem: 3115
lr 4.9971201287752166e-06
I20241204 07:35:53 2488421 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:39:17  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.125621  data: 0.001791  max mem: 3115
lr 4.996810743222097e-06
I20241204 07:36:14 2488421 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:37:44  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123576  data: 0.002409  max mem: 3115
lr 4.996485586455328e-06
I20241204 07:36:35 2488421 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:36:20  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124739  data: 0.002682  max mem: 3115
lr 4.996144660528775e-06
I20241204 07:36:57 2488421 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:35:00  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.125366  data: 0.002312  max mem: 3115
lr 4.99578796759591e-06
I20241204 07:37:18 2488421 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:33:46  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124612  data: 0.002184  max mem: 3115
lr 4.995415509909803e-06
I20241204 07:37:39 2488421 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:32:37  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.126455  data: 0.002669  max mem: 3115
lr 4.995027289823097e-06
I20241204 07:38:01 2488421 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:31:30  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.125917  data: 0.002279  max mem: 3115
lr 4.9946233097880025e-06
I20241204 07:38:22 2488421 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:30:28  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.125217  data: 0.001135  max mem: 3115
lr 4.994203572356276e-06
I20241204 07:38:43 2488421 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:29:28  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.124447  data: 0.001319  max mem: 3115
lr 4.9937680801792065e-06
I20241204 07:39:04 2488421 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:28:30  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123510  data: 0.001500  max mem: 3115
lr 4.993316836007601e-06
I20241204 07:39:26 2488421 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:27:36  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.124561  data: 0.001356  max mem: 3115
lr 4.992849842691759e-06
I20241204 07:39:47 2488421 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:26:44  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.124964  data: 0.001301  max mem: 3115
lr 4.99236710318147e-06
I20241204 07:40:08 2488421 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:25:53  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.124828  data: 0.002214  max mem: 3115
lr 4.991868620525976e-06
I20241204 07:40:29 2488421 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:25:04  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.123712  data: 0.002109  max mem: 3115
lr 4.991354397873964e-06
I20241204 07:40:50 2488421 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:24:17  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123839  data: 0.001047  max mem: 3115
lr 4.990824438473544e-06
I20241204 07:41:12 2488421 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:23:33  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.125661  data: 0.002050  max mem: 3115
lr 4.990278745672229e-06
I20241204 07:41:33 2488421 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:22:48  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.125355  data: 0.002370  max mem: 3115
lr 4.98971732291691e-06
I20241204 07:41:54 2488421 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:22:06  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.125007  data: 0.001841  max mem: 3115
lr 4.989140173753839e-06
I20241204 07:42:16 2488421 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:21:24  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.125219  data: 0.001964  max mem: 3115
lr 4.988547301828603e-06
I20241204 07:42:37 2488421 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:20:44  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.123957  data: 0.001546  max mem: 3115
lr 4.987938710886104e-06
I20241204 07:42:58 2488421 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:20:05  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.125063  data: 0.001493  max mem: 3115
lr 4.9873144047705305e-06
I20241204 07:43:19 2488421 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:19:26  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.125751  data: 0.002489  max mem: 3115
lr 4.986674387425343e-06
I20241204 07:43:40 2488421 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:18:48  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.124029  data: 0.002340  max mem: 3115
lr 4.9860186628932356e-06
I20241204 07:44:02 2488421 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:18:10  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.123154  data: 0.001399  max mem: 3115
lr 4.985347235316124e-06
I20241204 07:44:23 2488421 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:17:34  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.122757  data: 0.001433  max mem: 3115
lr 4.984660108935109e-06
I20241204 07:44:44 2488421 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:16:58  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.124174  data: 0.001525  max mem: 3115
lr 4.983957288090453e-06
I20241204 07:45:05 2488421 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:16:23  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.124947  data: 0.002613  max mem: 3115
lr 4.9832387772215545e-06
I20241204 07:45:27 2488421 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:15:49  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124156  data: 0.002710  max mem: 3115
lr 4.982504580866918e-06
I20241204 07:45:48 2488421 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:15:15  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123903  data: 0.001430  max mem: 3115
lr 4.981754703664129e-06
I20241204 07:46:09 2488421 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:14:41  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.122913  data: 0.001207  max mem: 3115
lr 4.980989150349819e-06
I20241204 07:46:30 2488421 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:14:08  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.124242  data: 0.002330  max mem: 3115
lr 4.980207925759636e-06
I20241204 07:46:52 2488421 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:13:36  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.125318  data: 0.002310  max mem: 3115
lr 4.979411034828223e-06
I20241204 07:47:13 2488421 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:13:04  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.124074  data: 0.001541  max mem: 3115
lr 4.978598482589174e-06
I20241204 07:47:34 2488421 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:12:32  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.123551  data: 0.001935  max mem: 3115
lr 4.977770274175011e-06
I20241204 07:47:55 2488421 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:12:01  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.123844  data: 0.002117  max mem: 3115
lr 4.97692641481715e-06
I20241204 07:48:17 2488421 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:11:31  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.125046  data: 0.001851  max mem: 3115
lr 4.976066909845862e-06
I20241204 07:48:38 2488421 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:11:00  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125286  data: 0.001431  max mem: 3115
lr 4.975191764690249e-06
I20241204 07:48:59 2488421 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:10:31  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125504  data: 0.002371  max mem: 3115
lr 4.974300984878205e-06
I20241204 07:49:20 2488421 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:10:01  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125280  data: 0.002286  max mem: 3115
lr 4.973394576036379e-06
I20241204 07:49:42 2488421 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:09:32  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124745  data: 0.001952  max mem: 3115
lr 4.97247254389014e-06
I20241204 07:50:03 2488421 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:09:03  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125709  data: 0.001832  max mem: 3115
lr 4.9715348942635445e-06
I20241204 07:50:24 2488421 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:08:34  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125494  data: 0.001241  max mem: 3115
lr 4.9705816330792985e-06
I20241204 07:50:45 2488421 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:08:05  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.124801  data: 0.001584  max mem: 3115
lr 4.969612766358717e-06
I20241204 07:51:07 2488421 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:07:37  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.124824  data: 0.001588  max mem: 3115
lr 4.9686283002216905e-06
I20241204 07:51:28 2488421 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:07:09  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124902  data: 0.001447  max mem: 3115
lr 4.967628240886639e-06
I20241204 07:51:49 2488421 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:06:41  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.125100  data: 0.001380  max mem: 3115
lr 4.966612594670483e-06
I20241204 07:52:10 2488421 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:06:13  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124467  data: 0.001865  max mem: 3115
lr 4.965581367988594e-06
I20241204 07:52:32 2488421 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:05:45  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.123690  data: 0.002164  max mem: 3115
lr 4.964534567354764e-06
I20241204 07:52:53 2488421 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:05:18  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.124077  data: 0.002934  max mem: 3115
lr 4.96347219938115e-06
I20241204 07:53:14 2488421 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:04:51  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124834  data: 0.002594  max mem: 3115
lr 4.96239427077825e-06
I20241204 07:53:35 2488421 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:04:24  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.125173  data: 0.001400  max mem: 3115
lr 4.961300788354844e-06
I20241204 07:53:57 2488421 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:03:57  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.125791  data: 0.001459  max mem: 3115
lr 4.960191759017962e-06
I20241204 07:54:18 2488421 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:03:30  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125489  data: 0.001360  max mem: 3115
lr 4.959067189772836e-06
I20241204 07:54:39 2488421 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:03:04  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.124372  data: 0.001155  max mem: 3115
lr 4.957927087722856e-06
I20241204 07:55:00 2488421 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:02:38  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125240  data: 0.001926  max mem: 3115
lr 4.956771460069526e-06
I20241204 07:55:22 2488421 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:02:11  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125164  data: 0.003182  max mem: 3115
lr 4.95560031411242e-06
I20241204 07:55:43 2488421 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:01:45  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124697  data: 0.002929  max mem: 3115
lr 4.9544136572491304e-06
I20241204 07:56:04 2488421 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:01:20  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.126362  data: 0.002192  max mem: 3115
lr 4.953211496975229e-06
I20241204 07:56:25 2488421 dinov2 helpers.py:102] Training  [  770/12500]  eta: 7:00:54  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.127151  data: 0.001666  max mem: 3115
lr 4.951993840884212e-06
I20241204 07:56:47 2488421 dinov2 helpers.py:102] Training  [  780/12500]  eta: 7:00:28  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.126560  data: 0.001897  max mem: 3115
lr 4.950760696667457e-06
I20241204 07:57:08 2488421 dinov2 helpers.py:102] Training  [  790/12500]  eta: 7:00:03  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.126959  data: 0.002038  max mem: 3115
lr 4.949512072114174e-06
I20241204 07:57:29 2488421 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:59:38  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.126649  data: 0.001407  max mem: 3115
lr 4.948247975111351e-06
I20241204 07:57:51 2488421 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:59:13  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126929  data: 0.001545  max mem: 3115
lr 4.946968413643719e-06
I20241204 07:58:12 2488421 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:58:48  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.127851  data: 0.001695  max mem: 3115
lr 4.945673395793676e-06
I20241204 07:58:33 2488421 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:58:23  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.126573  data: 0.002384  max mem: 3115
lr 4.9443629297412615e-06
I20241204 07:58:54 2488421 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:57:58  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125950  data: 0.002576  max mem: 3115
lr 4.943037023764093e-06
I20241204 07:59:16 2488421 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:57:33  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.125518  data: 0.001585  max mem: 3115
lr 4.941695686237312e-06
I20241204 07:59:37 2488421 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:57:08  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125083  data: 0.001172  max mem: 3115
lr 4.940338925633534e-06
I20241204 07:59:58 2488421 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:56:43  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.125263  data: 0.001837  max mem: 3115
lr 4.938966750522798e-06
I20241204 08:00:19 2488421 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:56:18  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.125932  data: 0.002097  max mem: 3115
lr 4.937579169572506e-06
I20241204 08:00:41 2488421 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:55:54  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.126568  data: 0.001996  max mem: 3115
lr 4.936176191547377e-06
I20241204 08:01:02 2488421 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:55:30  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.126462  data: 0.001804  max mem: 3115
lr 4.934757825309379e-06
I20241204 08:01:23 2488421 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:55:05  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.126426  data: 0.001297  max mem: 3115
lr 4.933324079817689e-06
I20241204 08:01:44 2488421 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:54:41  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.126757  data: 0.001675  max mem: 3115
lr 4.9318749641286164e-06
I20241204 08:02:06 2488421 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:54:17  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.126166  data: 0.001869  max mem: 3115
lr 4.930410487395568e-06
I20241204 08:02:27 2488421 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:53:52  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.125739  data: 0.001698  max mem: 3115
lr 4.928930658868971e-06
I20241204 08:02:48 2488421 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:53:28  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.125434  data: 0.001499  max mem: 3115
lr 4.927435487896227e-06
I20241204 08:03:09 2488421 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:53:04  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.124517  data: 0.001186  max mem: 3115
lr 4.925924983921652e-06
I20241204 08:03:31 2488421 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:52:39  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.124629  data: 0.001503  max mem: 3115
lr 4.92439915648641e-06
I20241204 08:03:52 2488421 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:52:15  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.124385  data: 0.001591  max mem: 3115
lr 4.922858015228454e-06
I20241204 08:04:13 2488421 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:51:51  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.124221  data: 0.001876  max mem: 3115
lr 4.921301569882469e-06
I20241204 08:04:34 2488421 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:51:27  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.124373  data: 0.002112  max mem: 3115
lr 4.919729830279811e-06
I20241204 08:04:56 2488421 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:51:03  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.124887  data: 0.001938  max mem: 3115
lr 4.918142806348443e-06
I20241204 08:05:17 2488421 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:50:39  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.125925  data: 0.002592  max mem: 3115
lr 4.916540508112869e-06
I20241204 08:05:38 2488421 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:50:16  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.125579  data: 0.002741  max mem: 3115
lr 4.914922945694074e-06
I20241204 08:05:59 2488421 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:49:52  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.124410  data: 0.002066  max mem: 3115
lr 4.913290129309465e-06
I20241204 08:06:21 2488421 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:49:28  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124510  data: 0.001659  max mem: 3115
lr 4.911642069272796e-06
I20241204 08:06:42 2488421 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:49:04  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.123768  data: 0.001571  max mem: 3115
lr 4.909978775994108e-06
I20241204 08:07:03 2488421 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:48:40  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123431  data: 0.001375  max mem: 3115
lr 4.908300259979668e-06
I20241204 08:07:24 2488421 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:48:16  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.123255  data: 0.001176  max mem: 3115
lr 4.906606531831894e-06
I20241204 08:07:46 2488421 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:47:53  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.123130  data: 0.001287  max mem: 3115
lr 4.904897602249294e-06
I20241204 08:08:07 2488421 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:47:29  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.123507  data: 0.001318  max mem: 3115
lr 4.903173482026397e-06
I20241204 08:08:28 2488421 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:47:05  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123003  data: 0.001103  max mem: 3115
lr 4.9014341820536815e-06
I20241204 08:08:49 2488421 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:46:42  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.123699  data: 0.001035  max mem: 3115
lr 4.899679713317512e-06
I20241204 08:09:11 2488421 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:46:18  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.123685  data: 0.002675  max mem: 3115
lr 4.897910086900068e-06
I20241204 08:09:32 2488421 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:45:55  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124273  data: 0.004432  max mem: 3115
lr 4.896125313979271e-06
I20241204 08:09:53 2488421 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:45:32  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.125020  data: 0.003885  max mem: 3115
lr 4.894325405828717e-06
I20241204 08:10:14 2488421 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:45:08  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.123545  data: 0.002393  max mem: 3115
lr 4.8925103738176015e-06
I20241204 08:10:36 2488421 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:44:45  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.122584  data: 0.001381  max mem: 3115
lr 4.890680229410655e-06
I20241204 08:10:57 2488421 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:44:21  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.123291  data: 0.002150  max mem: 3115
lr 4.888834984168066e-06
I20241204 08:11:18 2488421 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:43:58  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.123227  data: 0.002360  max mem: 3115
lr 4.886974649745406e-06
I20241204 08:11:39 2488421 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:43:34  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.122235  data: 0.001689  max mem: 3115
lr 4.885099237893554e-06
I20241204 08:12:00 2488421 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:43:11  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.122975  data: 0.002398  max mem: 3115
lr 4.883208760458633e-06
I20241204 08:12:22 2488421 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:42:48  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.123092  data: 0.002611  max mem: 3115
lr 4.881303229381928e-06
I20241204 08:12:42 2488421 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:42:16  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.073720  data: 0.001663  max mem: 3115
lr 4.8793826566998085e-06
I20241204 08:12:57 2488421 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:57  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.766263  data: 0.001855  max mem: 3115
I20241204 08:13:08 2488421 dinov2 linear.py:272] running validation !
submitit ERROR (2024-12-04 08:13:08,103) - Submitted job triggered an exception
E20241204 08:13:08 2488421 submitit submission.py:68] Submitted job triggered an exception
