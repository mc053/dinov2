submitit INFO (2024-12-04 07:27:40,417) - Starting with JobEnvironment(job_id=2488412, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-04 07:27:40,417) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender/2488412_submitted.pkl
I20241204 07:27:48 2488416 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 07:27:48 2488416 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
comment: 
config_file: CelebA_gt/config.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
exclude: 
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
ngpus: 8
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_class_mapping_fpath: None
val_dataset_str: CelebAOriginalVal
val_metric_type: mean_accuracy
I20241204 07:27:48 2488416 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 07:27:48 2488416 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/linear_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 07:27:48 2488416 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 07:28:22 2488416 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 07:28:27 2488416 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 07:28:27 2488416 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 07:28:34 2488416 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 07:28:39 2488416 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:39 2488416 dinov2 loaders.py:126] sampler: sharded infinite
I20241204 07:28:39 2488416 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:39 2488416 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:39 2488416 dinov2 loaders.py:225] infinite data loader
I20241204 07:28:39 2488416 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 07:28:41 2488416 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 07:28:41 2488416 dinov2 loaders.py:151] sampler: distributed
I20241204 07:28:41 2488416 dinov2 loaders.py:210] using PyTorch data loader
W20241204 07:28:41 2488416 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 07:28:41 2488416 dinov2 loaders.py:223] # of batches: 155
I20241204 07:28:41 2488416 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241204 07:28:41 2488416 dinov2 linear.py:338] Starting training from iteration 0
lr 4.999999921043166e-06
I20241204 07:29:05 2488416 dinov2 helpers.py:102] Training  [    0/12500]  eta: 3 days, 9:00:32  loss: 35.0071 (35.0071)  lr: 0.0000 (0.0000)  time: 23.330566  data: 16.807346  max mem: 2706
I20241204 07:29:06 2488416 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
lr 4.999990446229023e-06
I20241204 07:29:23 2488416 dinov2 helpers.py:102] Training  [   10/12500]  eta: 13:11:16  loss: 28.3533 (31.6802)  lr: 0.0000 (0.0000)  time: 3.801128  data: 1.528547  max mem: 3115
lr 4.999965180116501e-06
I20241204 07:29:44 2488416 dinov2 helpers.py:102] Training  [   20/12500]  eta: 10:20:17  loss: 28.3533 (28.9271)  lr: 0.0000 (0.0000)  time: 1.964764  data: 0.001369  max mem: 3115
lr 4.999924122865191e-06
I20241204 07:30:05 2488416 dinov2 helpers.py:102] Training  [   30/12500]  eta: 9:20:34  loss: 23.4210 (26.6114)  lr: 0.0000 (0.0000)  time: 2.090111  data: 0.001831  max mem: 3115
lr 4.999867274734432e-06
I20241204 07:30:26 2488416 dinov2 helpers.py:102] Training  [   40/12500]  eta: 8:50:22  loss: 23.4210 (25.7554)  lr: 0.0000 (0.0000)  time: 2.104424  data: 0.001357  max mem: 3115
lr 4.999794636083308e-06
I20241204 07:30:47 2488416 dinov2 helpers.py:102] Training  [   50/12500]  eta: 8:32:07  loss: 23.4210 (25.5186)  lr: 0.0000 (0.0000)  time: 2.112813  data: 0.001383  max mem: 3115
lr 4.999706207370645e-06
I20241204 07:31:08 2488416 dinov2 helpers.py:102] Training  [   60/12500]  eta: 8:19:46  loss: 23.4210 (24.9989)  lr: 0.0000 (0.0000)  time: 2.116310  data: 0.001792  max mem: 3115
lr 4.999601989155004e-06
I20241204 07:31:30 2488416 dinov2 helpers.py:102] Training  [   70/12500]  eta: 8:11:00  loss: 22.4402 (24.6791)  lr: 0.0000 (0.0000)  time: 2.120419  data: 0.001753  max mem: 3115
lr 4.999481982094688e-06
I20241204 07:31:51 2488416 dinov2 helpers.py:102] Training  [   80/12500]  eta: 8:04:21  loss: 22.7967 (24.4699)  lr: 0.0000 (0.0000)  time: 2.124487  data: 0.001374  max mem: 3115
lr 4.9993461869477276e-06
I20241204 07:32:12 2488416 dinov2 helpers.py:102] Training  [   90/12500]  eta: 7:59:04  loss: 22.4402 (24.0480)  lr: 0.0000 (0.0000)  time: 2.124832  data: 0.001486  max mem: 3115
lr 4.999194604571874e-06
I20241204 07:32:33 2488416 dinov2 helpers.py:102] Training  [  100/12500]  eta: 7:54:44  loss: 22.4402 (23.2667)  lr: 0.0000 (0.0000)  time: 2.123949  data: 0.002100  max mem: 3115
lr 4.999027235924608e-06
I20241204 07:32:55 2488416 dinov2 helpers.py:102] Training  [  110/12500]  eta: 7:51:09  loss: 22.4402 (23.4675)  lr: 0.0000 (0.0000)  time: 2.124450  data: 0.002220  max mem: 3115
lr 4.998844082063119e-06
I20241204 07:33:16 2488416 dinov2 helpers.py:102] Training  [  120/12500]  eta: 7:48:05  loss: 22.4402 (23.0126)  lr: 0.0000 (0.0000)  time: 2.124618  data: 0.001924  max mem: 3115
lr 4.998645144144304e-06
I20241204 07:33:37 2488416 dinov2 helpers.py:102] Training  [  130/12500]  eta: 7:45:28  loss: 22.3315 (22.9425)  lr: 0.0000 (0.0000)  time: 2.125040  data: 0.001962  max mem: 3115
lr 4.998430423424764e-06
I20241204 07:33:58 2488416 dinov2 helpers.py:102] Training  [  140/12500]  eta: 7:43:08  loss: 22.4402 (22.9368)  lr: 0.0000 (0.0000)  time: 2.125180  data: 0.002157  max mem: 3115
lr 4.9981999212607945e-06
I20241204 07:34:20 2488416 dinov2 helpers.py:102] Training  [  150/12500]  eta: 7:41:07  loss: 22.3315 (22.6942)  lr: 0.0000 (0.0000)  time: 2.126029  data: 0.004408  max mem: 3115
lr 4.997953639108375e-06
I20241204 07:34:41 2488416 dinov2 helpers.py:102] Training  [  160/12500]  eta: 7:39:16  loss: 22.3315 (22.4769)  lr: 0.0000 (0.0000)  time: 2.126029  data: 0.003917  max mem: 3115
lr 4.997691578523149e-06
I20241204 07:35:02 2488416 dinov2 helpers.py:102] Training  [  170/12500]  eta: 7:37:35  loss: 22.0318 (22.2321)  lr: 0.0000 (0.0000)  time: 2.124289  data: 0.001564  max mem: 3115
lr 4.9974137411604395e-06
I20241204 07:35:23 2488416 dinov2 helpers.py:102] Training  [  180/12500]  eta: 7:36:05  loss: 22.0318 (22.0259)  lr: 0.0000 (0.0000)  time: 2.125840  data: 0.002989  max mem: 3115
lr 4.9971201287752166e-06
I20241204 07:35:45 2488416 dinov2 helpers.py:102] Training  [  190/12500]  eta: 7:34:40  loss: 21.8807 (21.8607)  lr: 0.0000 (0.0000)  time: 2.125123  data: 0.002675  max mem: 3115
lr 4.996810743222097e-06
I20241204 07:36:06 2488416 dinov2 helpers.py:102] Training  [  200/12500]  eta: 7:33:21  loss: 20.2507 (21.6656)  lr: 0.0000 (0.0000)  time: 2.123784  data: 0.001124  max mem: 3115
lr 4.996485586455328e-06
I20241204 07:36:27 2488416 dinov2 helpers.py:102] Training  [  210/12500]  eta: 7:32:08  loss: 19.6641 (21.3701)  lr: 0.0000 (0.0000)  time: 2.124102  data: 0.001245  max mem: 3115
lr 4.996144660528775e-06
I20241204 07:36:48 2488416 dinov2 helpers.py:102] Training  [  220/12500]  eta: 7:31:00  loss: 19.2210 (21.2766)  lr: 0.0000 (0.0000)  time: 2.124183  data: 0.001337  max mem: 3115
lr 4.99578796759591e-06
I20241204 07:37:09 2488416 dinov2 helpers.py:102] Training  [  230/12500]  eta: 7:29:56  loss: 19.0553 (21.1400)  lr: 0.0000 (0.0000)  time: 2.124241  data: 0.001059  max mem: 3115
lr 4.995415509909803e-06
I20241204 07:37:31 2488416 dinov2 helpers.py:102] Training  [  240/12500]  eta: 7:28:54  loss: 19.0553 (21.2794)  lr: 0.0000 (0.0000)  time: 2.123193  data: 0.001038  max mem: 3115
lr 4.995027289823097e-06
I20241204 07:37:52 2488416 dinov2 helpers.py:102] Training  [  250/12500]  eta: 7:27:56  loss: 19.0008 (21.1434)  lr: 0.0000 (0.0000)  time: 2.123132  data: 0.001234  max mem: 3115
lr 4.9946233097880025e-06
I20241204 07:38:13 2488416 dinov2 helpers.py:102] Training  [  260/12500]  eta: 7:27:02  loss: 18.7221 (21.0302)  lr: 0.0000 (0.0000)  time: 2.124740  data: 0.001875  max mem: 3115
lr 4.994203572356276e-06
I20241204 07:38:34 2488416 dinov2 helpers.py:102] Training  [  270/12500]  eta: 7:26:10  loss: 18.3134 (20.9124)  lr: 0.0000 (0.0000)  time: 2.125073  data: 0.002279  max mem: 3115
lr 4.9937680801792065e-06
I20241204 07:38:56 2488416 dinov2 helpers.py:102] Training  [  280/12500]  eta: 7:25:20  loss: 18.3134 (20.8617)  lr: 0.0000 (0.0000)  time: 2.123539  data: 0.001653  max mem: 3115
lr 4.993316836007601e-06
I20241204 07:39:17 2488416 dinov2 helpers.py:102] Training  [  290/12500]  eta: 7:24:31  loss: 18.0851 (20.7678)  lr: 0.0000 (0.0000)  time: 2.123024  data: 0.001488  max mem: 3115
lr 4.992849842691759e-06
I20241204 07:39:38 2488416 dinov2 helpers.py:102] Training  [  300/12500]  eta: 7:23:45  loss: 18.0851 (20.6120)  lr: 0.0000 (0.0000)  time: 2.123422  data: 0.001448  max mem: 3115
lr 4.99236710318147e-06
I20241204 07:39:59 2488416 dinov2 helpers.py:102] Training  [  310/12500]  eta: 7:22:59  loss: 18.0851 (20.5954)  lr: 0.0000 (0.0000)  time: 2.122797  data: 0.001324  max mem: 3115
lr 4.991868620525976e-06
I20241204 07:40:21 2488416 dinov2 helpers.py:102] Training  [  320/12500]  eta: 7:22:16  loss: 18.0851 (20.3997)  lr: 0.0000 (0.0000)  time: 2.122729  data: 0.001340  max mem: 3115
lr 4.991354397873964e-06
I20241204 07:40:42 2488416 dinov2 helpers.py:102] Training  [  330/12500]  eta: 7:21:34  loss: 18.0706 (20.2733)  lr: 0.0000 (0.0000)  time: 2.123884  data: 0.001423  max mem: 3115
lr 4.990824438473544e-06
I20241204 07:41:03 2488416 dinov2 helpers.py:102] Training  [  340/12500]  eta: 7:20:53  loss: 18.0441 (20.1098)  lr: 0.0000 (0.0000)  time: 2.123903  data: 0.001424  max mem: 3115
lr 4.990278745672229e-06
I20241204 07:41:24 2488416 dinov2 helpers.py:102] Training  [  350/12500]  eta: 7:20:13  loss: 17.9970 (20.0374)  lr: 0.0000 (0.0000)  time: 2.123368  data: 0.002141  max mem: 3115
lr 4.98971732291691e-06
I20241204 07:41:46 2488416 dinov2 helpers.py:102] Training  [  360/12500]  eta: 7:19:35  loss: 17.8824 (19.9791)  lr: 0.0000 (0.0000)  time: 2.124412  data: 0.002660  max mem: 3115
lr 4.989140173753839e-06
I20241204 07:42:07 2488416 dinov2 helpers.py:102] Training  [  370/12500]  eta: 7:18:59  loss: 17.7647 (19.8769)  lr: 0.0000 (0.0000)  time: 2.126093  data: 0.002281  max mem: 3115
lr 4.988547301828603e-06
I20241204 07:42:28 2488416 dinov2 helpers.py:102] Training  [  380/12500]  eta: 7:18:22  loss: 17.7444 (19.7575)  lr: 0.0000 (0.0000)  time: 2.125057  data: 0.001841  max mem: 3115
lr 4.987938710886104e-06
I20241204 07:42:49 2488416 dinov2 helpers.py:102] Training  [  390/12500]  eta: 7:17:46  loss: 17.7444 (19.7632)  lr: 0.0000 (0.0000)  time: 2.123758  data: 0.001120  max mem: 3115
lr 4.9873144047705305e-06
I20241204 07:43:11 2488416 dinov2 helpers.py:102] Training  [  400/12500]  eta: 7:17:11  loss: 17.7444 (19.7652)  lr: 0.0000 (0.0000)  time: 2.124601  data: 0.001050  max mem: 3115
lr 4.986674387425343e-06
I20241204 07:43:32 2488416 dinov2 helpers.py:102] Training  [  410/12500]  eta: 7:16:37  loss: 17.7444 (19.6207)  lr: 0.0000 (0.0000)  time: 2.125633  data: 0.001217  max mem: 3115
lr 4.9860186628932356e-06
I20241204 07:43:53 2488416 dinov2 helpers.py:102] Training  [  420/12500]  eta: 7:16:03  loss: 17.7319 (19.5235)  lr: 0.0000 (0.0000)  time: 2.125011  data: 0.001551  max mem: 3115
lr 4.985347235316124e-06
I20241204 07:44:14 2488416 dinov2 helpers.py:102] Training  [  430/12500]  eta: 7:15:30  loss: 17.6894 (19.4818)  lr: 0.0000 (0.0000)  time: 2.124787  data: 0.002997  max mem: 3115
lr 4.984660108935109e-06
I20241204 07:44:36 2488416 dinov2 helpers.py:102] Training  [  440/12500]  eta: 7:14:57  loss: 17.6894 (19.4789)  lr: 0.0000 (0.0000)  time: 2.125692  data: 0.003542  max mem: 3115
lr 4.983957288090453e-06
I20241204 07:44:57 2488416 dinov2 helpers.py:102] Training  [  450/12500]  eta: 7:14:25  loss: 17.6894 (19.4742)  lr: 0.0000 (0.0000)  time: 2.124669  data: 0.001950  max mem: 3115
lr 4.9832387772215545e-06
I20241204 07:45:18 2488416 dinov2 helpers.py:102] Training  [  460/12500]  eta: 7:13:53  loss: 17.6894 (19.4527)  lr: 0.0000 (0.0000)  time: 2.124232  data: 0.001681  max mem: 3115
lr 4.982504580866918e-06
I20241204 07:45:39 2488416 dinov2 helpers.py:102] Training  [  470/12500]  eta: 7:13:22  loss: 17.5016 (19.3402)  lr: 0.0000 (0.0000)  time: 2.123637  data: 0.001757  max mem: 3115
lr 4.981754703664129e-06
I20241204 07:46:01 2488416 dinov2 helpers.py:102] Training  [  480/12500]  eta: 7:12:51  loss: 16.1035 (19.2574)  lr: 0.0000 (0.0000)  time: 2.123150  data: 0.001159  max mem: 3115
lr 4.980989150349819e-06
I20241204 07:46:22 2488416 dinov2 helpers.py:102] Training  [  490/12500]  eta: 7:12:20  loss: 16.1035 (19.1995)  lr: 0.0000 (0.0000)  time: 2.122850  data: 0.001191  max mem: 3115
lr 4.980207925759636e-06
I20241204 07:46:43 2488416 dinov2 helpers.py:102] Training  [  500/12500]  eta: 7:11:50  loss: 16.3624 (19.1974)  lr: 0.0000 (0.0000)  time: 2.124516  data: 0.001923  max mem: 3115
lr 4.979411034828223e-06
I20241204 07:47:04 2488416 dinov2 helpers.py:102] Training  [  510/12500]  eta: 7:11:20  loss: 16.3624 (19.2247)  lr: 0.0000 (0.0000)  time: 2.125314  data: 0.002188  max mem: 3115
lr 4.978598482589174e-06
I20241204 07:47:26 2488416 dinov2 helpers.py:102] Training  [  520/12500]  eta: 7:10:51  loss: 17.5016 (19.2485)  lr: 0.0000 (0.0000)  time: 2.124662  data: 0.001912  max mem: 3115
lr 4.977770274175011e-06
I20241204 07:47:47 2488416 dinov2 helpers.py:102] Training  [  530/12500]  eta: 7:10:22  loss: 17.6732 (19.2193)  lr: 0.0000 (0.0000)  time: 2.124525  data: 0.001800  max mem: 3115
lr 4.97692641481715e-06
I20241204 07:48:08 2488416 dinov2 helpers.py:102] Training  [  540/12500]  eta: 7:09:53  loss: 17.6894 (19.2422)  lr: 0.0000 (0.0000)  time: 2.124214  data: 0.001588  max mem: 3115
lr 4.976066909845862e-06
I20241204 07:48:29 2488416 dinov2 helpers.py:102] Training  [  550/12500]  eta: 7:09:25  loss: 17.6894 (19.1543)  lr: 0.0000 (0.0000)  time: 2.125094  data: 0.001474  max mem: 3115
lr 4.975191764690249e-06
I20241204 07:48:51 2488416 dinov2 helpers.py:102] Training  [  560/12500]  eta: 7:08:57  loss: 17.6894 (19.1671)  lr: 0.0000 (0.0000)  time: 2.125842  data: 0.001322  max mem: 3115
lr 4.974300984878205e-06
I20241204 07:49:12 2488416 dinov2 helpers.py:102] Training  [  570/12500]  eta: 7:08:29  loss: 18.4633 (19.2836)  lr: 0.0000 (0.0000)  time: 2.125718  data: 0.001332  max mem: 3115
lr 4.973394576036379e-06
I20241204 07:49:33 2488416 dinov2 helpers.py:102] Training  [  580/12500]  eta: 7:08:01  loss: 18.4633 (19.2653)  lr: 0.0000 (0.0000)  time: 2.124620  data: 0.001500  max mem: 3115
lr 4.97247254389014e-06
I20241204 07:49:54 2488416 dinov2 helpers.py:102] Training  [  590/12500]  eta: 7:07:34  loss: 18.2058 (19.2299)  lr: 0.0000 (0.0000)  time: 2.125233  data: 0.002020  max mem: 3115
lr 4.9715348942635445e-06
I20241204 07:50:16 2488416 dinov2 helpers.py:102] Training  [  600/12500]  eta: 7:07:07  loss: 18.2058 (19.2924)  lr: 0.0000 (0.0000)  time: 2.125940  data: 0.001813  max mem: 3115
lr 4.9705816330792985e-06
I20241204 07:50:37 2488416 dinov2 helpers.py:102] Training  [  610/12500]  eta: 7:06:40  loss: 18.2058 (19.2254)  lr: 0.0000 (0.0000)  time: 2.125717  data: 0.001978  max mem: 3115
lr 4.969612766358717e-06
I20241204 07:50:58 2488416 dinov2 helpers.py:102] Training  [  620/12500]  eta: 7:06:13  loss: 18.4633 (19.2388)  lr: 0.0000 (0.0000)  time: 2.125055  data: 0.002727  max mem: 3115
lr 4.9686283002216905e-06
I20241204 07:51:19 2488416 dinov2 helpers.py:102] Training  [  630/12500]  eta: 7:05:46  loss: 19.0931 (19.2648)  lr: 0.0000 (0.0000)  time: 2.124091  data: 0.001926  max mem: 3115
lr 4.967628240886639e-06
I20241204 07:51:41 2488416 dinov2 helpers.py:102] Training  [  640/12500]  eta: 7:05:19  loss: 18.4633 (19.2509)  lr: 0.0000 (0.0000)  time: 2.124650  data: 0.001213  max mem: 3115
lr 4.966612594670483e-06
I20241204 07:52:02 2488416 dinov2 helpers.py:102] Training  [  650/12500]  eta: 7:04:53  loss: 18.4633 (19.2854)  lr: 0.0000 (0.0000)  time: 2.124877  data: 0.001151  max mem: 3115
lr 4.965581367988594e-06
I20241204 07:52:23 2488416 dinov2 helpers.py:102] Training  [  660/12500]  eta: 7:04:27  loss: 18.6606 (19.2760)  lr: 0.0000 (0.0000)  time: 2.125142  data: 0.001329  max mem: 3115
lr 4.964534567354764e-06
I20241204 07:52:44 2488416 dinov2 helpers.py:102] Training  [  670/12500]  eta: 7:04:00  loss: 19.0931 (19.2801)  lr: 0.0000 (0.0000)  time: 2.125548  data: 0.002069  max mem: 3115
lr 4.96347219938115e-06
I20241204 07:53:06 2488416 dinov2 helpers.py:102] Training  [  680/12500]  eta: 7:03:35  loss: 19.0931 (19.1778)  lr: 0.0000 (0.0000)  time: 2.124955  data: 0.001979  max mem: 3115
lr 4.96239427077825e-06
I20241204 07:53:27 2488416 dinov2 helpers.py:102] Training  [  690/12500]  eta: 7:03:09  loss: 19.0931 (19.1367)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.002233  max mem: 3115
lr 4.961300788354844e-06
I20241204 07:53:48 2488416 dinov2 helpers.py:102] Training  [  700/12500]  eta: 7:02:43  loss: 18.6606 (19.0698)  lr: 0.0000 (0.0000)  time: 2.126149  data: 0.002426  max mem: 3115
lr 4.960191759017962e-06
I20241204 07:54:09 2488416 dinov2 helpers.py:102] Training  [  710/12500]  eta: 7:02:18  loss: 18.3591 (19.0223)  lr: 0.0000 (0.0000)  time: 2.125513  data: 0.001547  max mem: 3115
lr 4.959067189772836e-06
I20241204 07:54:31 2488416 dinov2 helpers.py:102] Training  [  720/12500]  eta: 7:01:52  loss: 18.3591 (19.0205)  lr: 0.0000 (0.0000)  time: 2.124794  data: 0.001520  max mem: 3115
lr 4.957927087722856e-06
I20241204 07:54:52 2488416 dinov2 helpers.py:102] Training  [  730/12500]  eta: 7:01:27  loss: 18.3591 (18.9595)  lr: 0.0000 (0.0000)  time: 2.125748  data: 0.002344  max mem: 3115
lr 4.956771460069526e-06
I20241204 07:55:13 2488416 dinov2 helpers.py:102] Training  [  740/12500]  eta: 7:01:02  loss: 18.2058 (18.9393)  lr: 0.0000 (0.0000)  time: 2.125624  data: 0.002148  max mem: 3115
lr 4.95560031411242e-06
I20241204 07:55:34 2488416 dinov2 helpers.py:102] Training  [  750/12500]  eta: 7:00:37  loss: 18.2058 (18.8640)  lr: 0.0000 (0.0000)  time: 2.124903  data: 0.001262  max mem: 3115
lr 4.9544136572491304e-06
I20241204 07:55:56 2488416 dinov2 helpers.py:102] Training  [  760/12500]  eta: 7:00:12  loss: 17.7059 (18.8490)  lr: 0.0000 (0.0000)  time: 2.125560  data: 0.001586  max mem: 3115
lr 4.953211496975229e-06
I20241204 07:56:17 2488416 dinov2 helpers.py:102] Training  [  770/12500]  eta: 6:59:47  loss: 17.4451 (18.7924)  lr: 0.0000 (0.0000)  time: 2.126182  data: 0.002332  max mem: 3115
lr 4.951993840884212e-06
I20241204 07:56:38 2488416 dinov2 helpers.py:102] Training  [  780/12500]  eta: 6:59:22  loss: 17.4451 (18.8405)  lr: 0.0000 (0.0000)  time: 2.125021  data: 0.001834  max mem: 3115
lr 4.950760696667457e-06
I20241204 07:56:59 2488416 dinov2 helpers.py:102] Training  [  790/12500]  eta: 6:58:58  loss: 17.7059 (18.8895)  lr: 0.0000 (0.0000)  time: 2.124550  data: 0.001173  max mem: 3115
lr 4.949512072114174e-06
I20241204 07:57:21 2488416 dinov2 helpers.py:102] Training  [  800/12500]  eta: 6:58:33  loss: 17.4451 (18.8565)  lr: 0.0000 (0.0000)  time: 2.125658  data: 0.001388  max mem: 3115
lr 4.948247975111351e-06
I20241204 07:57:42 2488416 dinov2 helpers.py:102] Training  [  810/12500]  eta: 6:58:09  loss: 17.7059 (18.8523)  lr: 0.0000 (0.0000)  time: 2.126716  data: 0.001440  max mem: 3115
lr 4.946968413643719e-06
I20241204 07:58:03 2488416 dinov2 helpers.py:102] Training  [  820/12500]  eta: 6:57:45  loss: 17.4451 (18.8344)  lr: 0.0000 (0.0000)  time: 2.127187  data: 0.001147  max mem: 3115
lr 4.945673395793676e-06
I20241204 07:58:24 2488416 dinov2 helpers.py:102] Training  [  830/12500]  eta: 6:57:21  loss: 17.4451 (18.8530)  lr: 0.0000 (0.0000)  time: 2.127015  data: 0.001522  max mem: 3115
lr 4.9443629297412615e-06
I20241204 07:58:46 2488416 dinov2 helpers.py:102] Training  [  840/12500]  eta: 6:56:56  loss: 17.4451 (18.8693)  lr: 0.0000 (0.0000)  time: 2.125133  data: 0.001611  max mem: 3115
lr 4.943037023764093e-06
I20241204 07:59:07 2488416 dinov2 helpers.py:102] Training  [  850/12500]  eta: 6:56:32  loss: 17.3665 (18.8427)  lr: 0.0000 (0.0000)  time: 2.124855  data: 0.001345  max mem: 3115
lr 4.941695686237312e-06
I20241204 07:59:28 2488416 dinov2 helpers.py:102] Training  [  860/12500]  eta: 6:56:08  loss: 16.7900 (18.8191)  lr: 0.0000 (0.0000)  time: 2.125691  data: 0.001535  max mem: 3115
lr 4.940338925633534e-06
I20241204 07:59:49 2488416 dinov2 helpers.py:102] Training  [  870/12500]  eta: 6:55:44  loss: 16.5791 (18.7531)  lr: 0.0000 (0.0000)  time: 2.125742  data: 0.001550  max mem: 3115
lr 4.938966750522798e-06
I20241204 08:00:11 2488416 dinov2 helpers.py:102] Training  [  880/12500]  eta: 6:55:20  loss: 16.5791 (18.6811)  lr: 0.0000 (0.0000)  time: 2.127882  data: 0.003304  max mem: 3115
lr 4.937579169572506e-06
I20241204 08:00:32 2488416 dinov2 helpers.py:102] Training  [  890/12500]  eta: 6:54:57  loss: 16.7900 (18.7006)  lr: 0.0000 (0.0000)  time: 2.128030  data: 0.004065  max mem: 3115
lr 4.936176191547377e-06
I20241204 08:00:53 2488416 dinov2 helpers.py:102] Training  [  900/12500]  eta: 6:54:33  loss: 17.3665 (18.7135)  lr: 0.0000 (0.0000)  time: 2.127131  data: 0.003192  max mem: 3115
lr 4.934757825309379e-06
I20241204 08:01:15 2488416 dinov2 helpers.py:102] Training  [  910/12500]  eta: 6:54:09  loss: 17.4451 (18.7050)  lr: 0.0000 (0.0000)  time: 2.127130  data: 0.002670  max mem: 3115
lr 4.933324079817689e-06
I20241204 08:01:36 2488416 dinov2 helpers.py:102] Training  [  920/12500]  eta: 6:53:46  loss: 17.3665 (18.6299)  lr: 0.0000 (0.0000)  time: 2.127039  data: 0.002778  max mem: 3115
lr 4.9318749641286164e-06
I20241204 08:01:57 2488416 dinov2 helpers.py:102] Training  [  930/12500]  eta: 6:53:22  loss: 17.3665 (18.5940)  lr: 0.0000 (0.0000)  time: 2.126924  data: 0.002415  max mem: 3115
lr 4.930410487395568e-06
I20241204 08:02:18 2488416 dinov2 helpers.py:102] Training  [  940/12500]  eta: 6:52:58  loss: 17.1657 (18.5789)  lr: 0.0000 (0.0000)  time: 2.125026  data: 0.001348  max mem: 3115
lr 4.928930658868971e-06
I20241204 08:02:40 2488416 dinov2 helpers.py:102] Training  [  950/12500]  eta: 6:52:35  loss: 17.1657 (18.5422)  lr: 0.0000 (0.0000)  time: 2.126028  data: 0.001689  max mem: 3115
lr 4.927435487896227e-06
I20241204 08:03:01 2488416 dinov2 helpers.py:102] Training  [  960/12500]  eta: 6:52:11  loss: 16.7900 (18.5134)  lr: 0.0000 (0.0000)  time: 2.127217  data: 0.001684  max mem: 3115
lr 4.925924983921652e-06
I20241204 08:03:22 2488416 dinov2 helpers.py:102] Training  [  970/12500]  eta: 6:51:48  loss: 17.1657 (18.5643)  lr: 0.0000 (0.0000)  time: 2.125635  data: 0.001326  max mem: 3115
lr 4.92439915648641e-06
I20241204 08:03:43 2488416 dinov2 helpers.py:102] Training  [  980/12500]  eta: 6:51:24  loss: 16.7900 (18.5196)  lr: 0.0000 (0.0000)  time: 2.125000  data: 0.001495  max mem: 3115
lr 4.922858015228454e-06
I20241204 08:04:05 2488416 dinov2 helpers.py:102] Training  [  990/12500]  eta: 6:51:01  loss: 16.5791 (18.4958)  lr: 0.0000 (0.0000)  time: 2.123971  data: 0.001459  max mem: 3115
lr 4.921301569882469e-06
I20241204 08:04:26 2488416 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 6:50:37  loss: 16.7900 (18.4888)  lr: 0.0000 (0.0000)  time: 2.123626  data: 0.001895  max mem: 3115
lr 4.919729830279811e-06
I20241204 08:04:47 2488416 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 6:50:14  loss: 16.7900 (18.4787)  lr: 0.0000 (0.0000)  time: 2.124681  data: 0.001811  max mem: 3115
lr 4.918142806348443e-06
I20241204 08:05:08 2488416 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 6:49:50  loss: 16.5791 (18.4554)  lr: 0.0000 (0.0000)  time: 2.124000  data: 0.001235  max mem: 3115
lr 4.916540508112869e-06
I20241204 08:05:30 2488416 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 6:49:27  loss: 16.1401 (18.3996)  lr: 0.0000 (0.0000)  time: 2.123172  data: 0.001301  max mem: 3115
lr 4.914922945694074e-06
I20241204 08:05:51 2488416 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 6:49:03  loss: 16.0797 (18.3741)  lr: 0.0000 (0.0000)  time: 2.123295  data: 0.001207  max mem: 3115
lr 4.913290129309465e-06
I20241204 08:06:12 2488416 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 6:48:40  loss: 15.7510 (18.3391)  lr: 0.0000 (0.0000)  time: 2.124233  data: 0.002279  max mem: 3115
lr 4.911642069272796e-06
I20241204 08:06:33 2488416 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 6:48:17  loss: 15.7192 (18.3128)  lr: 0.0000 (0.0000)  time: 2.124705  data: 0.002352  max mem: 3115
lr 4.909978775994108e-06
I20241204 08:06:55 2488416 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 6:47:53  loss: 15.7192 (18.2755)  lr: 0.0000 (0.0000)  time: 2.123809  data: 0.001313  max mem: 3115
lr 4.908300259979668e-06
I20241204 08:07:16 2488416 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 6:47:30  loss: 15.7510 (18.2947)  lr: 0.0000 (0.0000)  time: 2.124628  data: 0.001589  max mem: 3115
lr 4.906606531831894e-06
I20241204 08:07:37 2488416 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 6:47:07  loss: 15.7510 (18.2818)  lr: 0.0000 (0.0000)  time: 2.125288  data: 0.001673  max mem: 3115
lr 4.904897602249294e-06
I20241204 08:07:58 2488416 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 6:46:44  loss: 15.7192 (18.2486)  lr: 0.0000 (0.0000)  time: 2.124583  data: 0.001366  max mem: 3115
lr 4.903173482026397e-06
I20241204 08:08:20 2488416 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 6:46:21  loss: 15.7192 (18.2569)  lr: 0.0000 (0.0000)  time: 2.123936  data: 0.001211  max mem: 3115
lr 4.9014341820536815e-06
I20241204 08:08:41 2488416 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 6:45:58  loss: 15.7192 (18.2268)  lr: 0.0000 (0.0000)  time: 2.124100  data: 0.002015  max mem: 3115
lr 4.899679713317512e-06
I20241204 08:09:02 2488416 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 6:45:35  loss: 15.7510 (18.2233)  lr: 0.0000 (0.0000)  time: 2.124687  data: 0.002192  max mem: 3115
lr 4.897910086900068e-06
I20241204 08:09:23 2488416 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 6:45:12  loss: 15.7510 (18.2492)  lr: 0.0000 (0.0000)  time: 2.124651  data: 0.001361  max mem: 3115
lr 4.896125313979271e-06
I20241204 08:09:45 2488416 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 6:44:49  loss: 15.7510 (18.2209)  lr: 0.0000 (0.0000)  time: 2.123862  data: 0.001150  max mem: 3115
lr 4.894325405828717e-06
I20241204 08:10:06 2488416 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 6:44:26  loss: 15.7192 (18.1935)  lr: 0.0000 (0.0000)  time: 2.122900  data: 0.001813  max mem: 3115
lr 4.8925103738176015e-06
I20241204 08:10:27 2488416 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 6:44:03  loss: 15.7192 (18.1924)  lr: 0.0000 (0.0000)  time: 2.124458  data: 0.002206  max mem: 3115
lr 4.890680229410655e-06
I20241204 08:10:48 2488416 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 6:43:40  loss: 15.7192 (18.1408)  lr: 0.0000 (0.0000)  time: 2.124146  data: 0.001833  max mem: 3115
lr 4.888834984168066e-06
I20241204 08:11:09 2488416 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 6:43:17  loss: 15.7192 (18.1479)  lr: 0.0000 (0.0000)  time: 2.121975  data: 0.003006  max mem: 3115
lr 4.886974649745406e-06
I20241204 08:11:31 2488416 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 6:42:54  loss: 15.5240 (18.1258)  lr: 0.0000 (0.0000)  time: 2.121676  data: 0.002789  max mem: 3115
lr 4.885099237893554e-06
I20241204 08:11:52 2488416 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 6:42:31  loss: 15.5240 (18.1248)  lr: 0.0000 (0.0000)  time: 2.121569  data: 0.001847  max mem: 3115
lr 4.883208760458633e-06
I20241204 08:12:13 2488416 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 6:42:08  loss: 15.5240 (18.1057)  lr: 0.0000 (0.0000)  time: 2.122008  data: 0.002140  max mem: 3115
lr 4.881303229381928e-06
I20241204 08:12:34 2488416 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 6:41:45  loss: 15.7192 (18.1216)  lr: 0.0000 (0.0000)  time: 2.121747  data: 0.001610  max mem: 3115
lr 4.8793826566998085e-06
I20241204 08:12:52 2488416 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 6:40:46  loss: 15.7819 (18.1306)  lr: 0.0000 (0.0000)  time: 1.925996  data: 0.001376  max mem: 3115
I20241204 08:13:03 2488416 dinov2 linear.py:272] running validation !
submitit ERROR (2024-12-04 08:13:03,753) - Submitted job triggered an exception
E20241204 08:13:03 2488416 submitit submission.py:68] Submitted job triggered an exception
