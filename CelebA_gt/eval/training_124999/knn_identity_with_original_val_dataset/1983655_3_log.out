submitit INFO (2024-12-03 07:39:19,656) - Starting with JobEnvironment(job_id=1983655, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-03 07:39:19,656) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn/1983655_submitted.pkl
I20241203 07:39:27 1983661 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 07:39:27 1983661 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 07:39:27 1983661 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 07:39:27 1983661 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 07:39:27 1983661 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 07:39:59 1983661 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 07:40:04 1983661 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 07:40:04 1983661 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 07:40:08 1983661 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 07:40:08 1983661 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 07:40:09 1983661 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 07:40:09 1983661 dinov2 knn.py:260] Extracting features for train set...
I20241203 07:40:09 1983661 dinov2 loaders.py:151] sampler: distributed
I20241203 07:40:09 1983661 dinov2 loaders.py:210] using PyTorch data loader
W20241203 07:40:09 1983661 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 07:40:09 1983661 dinov2 loaders.py:223] # of batches: 634
I20241203 07:40:42 1983661 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 07:40:42 1983661 dinov2 helpers.py:102]   [  0/634]  eta: 5:51:52    time: 33.300415  data: 16.900663  max mem: 3463
I20241203 07:40:46 1983661 dinov2 helpers.py:102]   [ 10/634]  eta: 0:35:36    time: 3.424170  data: 1.536794  max mem: 4109
I20241203 07:40:58 1983661 dinov2 helpers.py:102]   [ 20/634]  eta: 0:23:58    time: 0.795407  data: 0.000368  max mem: 4109
I20241203 07:41:32 1983661 dinov2 helpers.py:102]   [ 30/634]  eta: 0:27:04    time: 2.284359  data: 0.000800  max mem: 4109
I20241203 07:42:11 1983661 dinov2 helpers.py:102]   [ 40/634]  eta: 0:29:32    time: 3.655207  data: 0.001074  max mem: 4109
I20241203 07:42:50 1983661 dinov2 helpers.py:102]   [ 50/634]  eta: 0:30:50    time: 3.912515  data: 0.000898  max mem: 4109
I20241203 07:43:30 1983661 dinov2 helpers.py:102]   [ 60/634]  eta: 0:31:32    time: 3.941156  data: 0.000880  max mem: 4109
I20241203 07:44:09 1983661 dinov2 helpers.py:102]   [ 70/634]  eta: 0:31:51    time: 3.954310  data: 0.000708  max mem: 4109
I20241203 07:44:49 1983661 dinov2 helpers.py:102]   [ 80/634]  eta: 0:31:56    time: 3.954306  data: 0.001029  max mem: 4109
I20241203 07:45:29 1983661 dinov2 helpers.py:102]   [ 90/634]  eta: 0:31:51    time: 3.956956  data: 0.001184  max mem: 4109
I20241203 07:46:08 1983661 dinov2 helpers.py:102]   [100/634]  eta: 0:31:40    time: 3.965159  data: 0.001348  max mem: 4109
I20241203 07:46:48 1983661 dinov2 helpers.py:102]   [110/634]  eta: 0:31:24    time: 3.972052  data: 0.002155  max mem: 4109
I20241203 07:47:28 1983661 dinov2 helpers.py:102]   [120/634]  eta: 0:31:04    time: 3.974028  data: 0.003105  max mem: 4109
I20241203 07:48:07 1983661 dinov2 helpers.py:102]   [130/634]  eta: 0:30:41    time: 3.973950  data: 0.002149  max mem: 4109
I20241203 07:48:47 1983661 dinov2 helpers.py:102]   [140/634]  eta: 0:30:16    time: 3.974160  data: 0.000579  max mem: 4109
I20241203 07:49:27 1983661 dinov2 helpers.py:102]   [150/634]  eta: 0:29:49    time: 3.972976  data: 0.000877  max mem: 4109
I20241203 07:50:07 1983661 dinov2 helpers.py:102]   [160/634]  eta: 0:29:20    time: 3.971302  data: 0.001007  max mem: 4109
I20241203 07:50:46 1983661 dinov2 helpers.py:102]   [170/634]  eta: 0:28:50    time: 3.968661  data: 0.000908  max mem: 4109
I20241203 07:51:26 1983661 dinov2 helpers.py:102]   [180/634]  eta: 0:28:18    time: 3.964760  data: 0.000923  max mem: 4109
I20241203 07:52:06 1983661 dinov2 helpers.py:102]   [190/634]  eta: 0:27:46    time: 3.960847  data: 0.000861  max mem: 4109
I20241203 07:52:45 1983661 dinov2 helpers.py:102]   [200/634]  eta: 0:27:13    time: 3.957997  data: 0.001652  max mem: 4109
I20241203 07:53:25 1983661 dinov2 helpers.py:102]   [210/634]  eta: 0:26:39    time: 3.962857  data: 0.001771  max mem: 4109
I20241203 07:54:04 1983661 dinov2 helpers.py:102]   [220/634]  eta: 0:26:05    time: 3.969688  data: 0.001112  max mem: 4109
I20241203 07:54:44 1983661 dinov2 helpers.py:102]   [230/634]  eta: 0:25:31    time: 3.970962  data: 0.001108  max mem: 4109
I20241203 07:55:24 1983661 dinov2 helpers.py:102]   [240/634]  eta: 0:24:56    time: 3.972524  data: 0.000815  max mem: 4109
I20241203 07:56:04 1983661 dinov2 helpers.py:102]   [250/634]  eta: 0:24:20    time: 3.973418  data: 0.000752  max mem: 4109
I20241203 07:56:43 1983661 dinov2 helpers.py:102]   [260/634]  eta: 0:23:45    time: 3.973241  data: 0.000842  max mem: 4109
I20241203 07:57:23 1983661 dinov2 helpers.py:102]   [270/634]  eta: 0:23:09    time: 3.973641  data: 0.000637  max mem: 4109
I20241203 07:58:03 1983661 dinov2 helpers.py:102]   [280/634]  eta: 0:22:33    time: 3.973602  data: 0.000651  max mem: 4109
I20241203 07:58:43 1983661 dinov2 helpers.py:102]   [290/634]  eta: 0:21:56    time: 3.973528  data: 0.000881  max mem: 4109
I20241203 07:59:22 1983661 dinov2 helpers.py:102]   [300/634]  eta: 0:21:20    time: 3.972771  data: 0.000900  max mem: 4109
I20241203 08:00:02 1983661 dinov2 helpers.py:102]   [310/634]  eta: 0:20:43    time: 3.971926  data: 0.001019  max mem: 4109
I20241203 08:00:42 1983661 dinov2 helpers.py:102]   [320/634]  eta: 0:20:06    time: 3.972860  data: 0.001895  max mem: 4109
I20241203 08:01:22 1983661 dinov2 helpers.py:102]   [330/634]  eta: 0:19:29    time: 3.975599  data: 0.001675  max mem: 4109
I20241203 08:02:01 1983661 dinov2 helpers.py:102]   [340/634]  eta: 0:18:51    time: 3.975653  data: 0.000841  max mem: 4109
I20241203 08:02:41 1983661 dinov2 helpers.py:102]   [350/634]  eta: 0:18:14    time: 3.974161  data: 0.000949  max mem: 4109
I20241203 08:03:21 1983661 dinov2 helpers.py:102]   [360/634]  eta: 0:17:36    time: 3.976277  data: 0.001055  max mem: 4109
I20241203 08:04:01 1983661 dinov2 helpers.py:102]   [370/634]  eta: 0:16:58    time: 3.976067  data: 0.000949  max mem: 4109
I20241203 08:04:40 1983661 dinov2 helpers.py:102]   [380/634]  eta: 0:16:21    time: 3.973991  data: 0.000753  max mem: 4109
I20241203 08:05:20 1983661 dinov2 helpers.py:102]   [390/634]  eta: 0:15:43    time: 3.975238  data: 0.000675  max mem: 4109
I20241203 08:06:00 1983661 dinov2 helpers.py:102]   [400/634]  eta: 0:15:05    time: 3.976282  data: 0.001142  max mem: 4109
I20241203 08:06:40 1983661 dinov2 helpers.py:102]   [410/634]  eta: 0:14:27    time: 3.977957  data: 0.001088  max mem: 4109
I20241203 08:07:19 1983661 dinov2 helpers.py:102]   [420/634]  eta: 0:13:48    time: 3.977132  data: 0.000710  max mem: 4109
I20241203 08:07:59 1983661 dinov2 helpers.py:102]   [430/634]  eta: 0:13:10    time: 3.974402  data: 0.000895  max mem: 4109
I20241203 08:08:39 1983661 dinov2 helpers.py:102]   [440/634]  eta: 0:12:32    time: 3.974340  data: 0.001358  max mem: 4109
I20241203 08:09:19 1983661 dinov2 helpers.py:102]   [450/634]  eta: 0:11:53    time: 3.974489  data: 0.001310  max mem: 4109
I20241203 08:09:58 1983661 dinov2 helpers.py:102]   [460/634]  eta: 0:11:15    time: 3.974502  data: 0.001005  max mem: 4109
I20241203 08:10:38 1983661 dinov2 helpers.py:102]   [470/634]  eta: 0:10:36    time: 3.976100  data: 0.000987  max mem: 4109
I20241203 08:11:18 1983661 dinov2 helpers.py:102]   [480/634]  eta: 0:09:58    time: 3.977886  data: 0.000878  max mem: 4109
I20241203 08:11:58 1983661 dinov2 helpers.py:102]   [490/634]  eta: 0:09:19    time: 3.977906  data: 0.001121  max mem: 4109
I20241203 08:12:37 1983661 dinov2 helpers.py:102]   [500/634]  eta: 0:08:41    time: 3.975972  data: 0.001662  max mem: 4109
I20241203 08:13:17 1983661 dinov2 helpers.py:102]   [510/634]  eta: 0:08:02    time: 3.975971  data: 0.001561  max mem: 4109
I20241203 08:13:57 1983661 dinov2 helpers.py:102]   [520/634]  eta: 0:07:23    time: 3.976070  data: 0.000817  max mem: 4109
I20241203 08:14:37 1983661 dinov2 helpers.py:102]   [530/634]  eta: 0:06:45    time: 3.976101  data: 0.000646  max mem: 4109
I20241203 08:15:17 1983661 dinov2 helpers.py:102]   [540/634]  eta: 0:06:06    time: 3.977824  data: 0.000862  max mem: 4109
I20241203 08:15:56 1983661 dinov2 helpers.py:102]   [550/634]  eta: 0:05:27    time: 3.978829  data: 0.001738  max mem: 4109
I20241203 08:16:36 1983661 dinov2 helpers.py:102]   [560/634]  eta: 0:04:48    time: 3.977970  data: 0.001534  max mem: 4109
I20241203 08:17:16 1983661 dinov2 helpers.py:102]   [570/634]  eta: 0:04:09    time: 3.977816  data: 0.000642  max mem: 4109
I20241203 08:17:56 1983661 dinov2 helpers.py:102]   [580/634]  eta: 0:03:30    time: 3.977011  data: 0.000840  max mem: 4109
I20241203 08:18:35 1983661 dinov2 helpers.py:102]   [590/634]  eta: 0:02:51    time: 3.976181  data: 0.001116  max mem: 4109
I20241203 08:19:15 1983661 dinov2 helpers.py:102]   [600/634]  eta: 0:02:12    time: 3.976005  data: 0.001680  max mem: 4109
I20241203 08:19:55 1983661 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.972862  data: 0.002158  max mem: 4109
I20241203 08:20:35 1983661 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.972358  data: 0.001702  max mem: 4109
I20241203 08:21:14 1983661 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.973027  data: 0.001321  max mem: 4109
I20241203 08:21:34 1983661 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.351031  data: 0.001123  max mem: 4109
I20241203 08:21:34 1983661 dinov2 helpers.py:130]  Total time: 0:41:25 (3.920232 s / it)
I20241203 08:21:34 1983661 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 08:21:34 1983661 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 08:21:35 1983661 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 08:21:35 1983661 dinov2 loaders.py:151] sampler: distributed
I20241203 08:21:35 1983661 dinov2 loaders.py:210] using PyTorch data loader
I20241203 08:21:35 1983661 dinov2 loaders.py:223] # of batches: 78
I20241203 08:22:43 1983661 dinov2 knn.py:299] Start the k-NN classification.
I20241203 08:22:49 1983661 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:08:11    time: 6.305489  data: 5.672287  max mem: 8496
I20241203 08:23:08 1983661 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:02:31    time: 2.225355  data: 1.599151  max mem: 8536
I20241203 08:23:17 1983661 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:01:33    time: 1.379610  data: 0.636773  max mem: 8536
I20241203 08:23:27 1983661 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:01:08    time: 0.986085  data: 0.042724  max mem: 8536
I20241203 08:23:38 1983661 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:00:50    time: 1.037393  data: 0.003230  max mem: 8536
I20241203 08:23:48 1983661 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:00:35    time: 1.042904  data: 0.001457  max mem: 8536
I20241203 08:23:59 1983661 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:00:22    time: 1.041962  data: 0.000194  max mem: 8536
I20241203 08:24:09 1983661 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:09    time: 1.043213  data: 0.000203  max mem: 8536
I20241203 08:24:16 1983661 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:01    time: 1.015405  data: 0.000170  max mem: 8536
I20241203 08:24:16 1983661 dinov2 helpers.py:130] Test: Total time: 0:01:32 (1.188045 s / it)
I20241203 08:24:16 1983661 dinov2 utils.py:79] Averaged stats: 
I20241203 08:24:16 1983661 dinov2 knn.py:367] ('full', 10) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:16 1983661 dinov2 knn.py:367] ('full', 20) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:16 1983661 dinov2 knn.py:367] ('full', 100) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:16 1983661 dinov2 knn.py:367] ('full', 200) classifier result: Top1: 0.00 Top5: 0.00
submitit INFO (2024-12-03 08:24:16,752) - Job completed successfully
I20241203 08:24:16 1983661 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-03 08:24:16,754) - Exiting after successful completion
I20241203 08:24:16 1983661 submitit submission.py:61] Exiting after successful completion
