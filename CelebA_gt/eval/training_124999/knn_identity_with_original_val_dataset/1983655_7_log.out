submitit INFO (2024-12-03 07:39:19,671) - Starting with JobEnvironment(job_id=1983655, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-03 07:39:19,671) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn/1983655_submitted.pkl
I20241203 07:39:27 1983665 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 07:39:27 1983665 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 07:39:27 1983665 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 07:39:27 1983665 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 07:39:27 1983665 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 07:39:59 1983665 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 07:40:05 1983665 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 07:40:05 1983665 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 07:40:10 1983665 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 07:40:10 1983665 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 07:40:11 1983665 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 07:40:11 1983665 dinov2 knn.py:260] Extracting features for train set...
I20241203 07:40:11 1983665 dinov2 loaders.py:151] sampler: distributed
I20241203 07:40:11 1983665 dinov2 loaders.py:210] using PyTorch data loader
W20241203 07:40:11 1983665 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 07:40:11 1983665 dinov2 loaders.py:223] # of batches: 634
I20241203 07:40:47 1983665 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 07:40:47 1983665 dinov2 helpers.py:102]   [  0/634]  eta: 6:13:14    time: 35.322475  data: 11.619547  max mem: 3463
I20241203 07:40:55 1983665 dinov2 helpers.py:102]   [ 10/634]  eta: 0:41:47    time: 4.019159  data: 1.060436  max mem: 4109
I20241203 07:41:26 1983665 dinov2 helpers.py:102]   [ 20/634]  eta: 0:36:14    time: 1.952705  data: 0.002492  max mem: 4109
I20241203 07:42:05 1983665 dinov2 helpers.py:102]   [ 30/634]  eta: 0:36:45    time: 3.450413  data: 0.003082  max mem: 4109
I20241203 07:42:44 1983665 dinov2 helpers.py:102]   [ 40/634]  eta: 0:36:44    time: 3.890985  data: 0.003398  max mem: 4109
I20241203 07:43:24 1983665 dinov2 helpers.py:102]   [ 50/634]  eta: 0:36:35    time: 3.924790  data: 0.001228  max mem: 4109
I20241203 07:44:03 1983665 dinov2 helpers.py:102]   [ 60/634]  eta: 0:36:16    time: 3.954544  data: 0.002212  max mem: 4109
I20241203 07:44:43 1983665 dinov2 helpers.py:102]   [ 70/634]  eta: 0:35:51    time: 3.955228  data: 0.002192  max mem: 4109
I20241203 07:45:22 1983665 dinov2 helpers.py:102]   [ 80/634]  eta: 0:35:23    time: 3.956031  data: 0.001198  max mem: 4109
I20241203 07:46:02 1983665 dinov2 helpers.py:102]   [ 90/634]  eta: 0:34:52    time: 3.964085  data: 0.000948  max mem: 4109
I20241203 07:46:42 1983665 dinov2 helpers.py:102]   [100/634]  eta: 0:34:21    time: 3.973672  data: 0.000746  max mem: 4109
I20241203 07:47:22 1983665 dinov2 helpers.py:102]   [110/634]  eta: 0:33:48    time: 3.977204  data: 0.001661  max mem: 4109
I20241203 07:48:01 1983665 dinov2 helpers.py:102]   [120/634]  eta: 0:33:13    time: 3.975246  data: 0.001916  max mem: 4109
I20241203 07:48:41 1983665 dinov2 helpers.py:102]   [130/634]  eta: 0:32:38    time: 3.974097  data: 0.000920  max mem: 4109
I20241203 07:49:21 1983665 dinov2 helpers.py:102]   [140/634]  eta: 0:32:03    time: 3.974684  data: 0.000815  max mem: 4109
I20241203 07:50:01 1983665 dinov2 helpers.py:102]   [150/634]  eta: 0:31:26    time: 3.973144  data: 0.001093  max mem: 4109
I20241203 07:50:40 1983665 dinov2 helpers.py:102]   [160/634]  eta: 0:30:49    time: 3.967116  data: 0.001756  max mem: 4109
I20241203 07:51:20 1983665 dinov2 helpers.py:102]   [170/634]  eta: 0:30:12    time: 3.963092  data: 0.001544  max mem: 4109
I20241203 07:52:00 1983665 dinov2 helpers.py:102]   [180/634]  eta: 0:29:34    time: 3.962689  data: 0.000871  max mem: 4109
I20241203 07:52:39 1983665 dinov2 helpers.py:102]   [190/634]  eta: 0:28:56    time: 3.962535  data: 0.002147  max mem: 4109
I20241203 07:53:19 1983665 dinov2 helpers.py:102]   [200/634]  eta: 0:28:18    time: 3.960772  data: 0.002385  max mem: 4109
I20241203 07:53:58 1983665 dinov2 helpers.py:102]   [210/634]  eta: 0:27:40    time: 3.963897  data: 0.001042  max mem: 4109
I20241203 07:54:38 1983665 dinov2 helpers.py:102]   [220/634]  eta: 0:27:02    time: 3.969977  data: 0.000719  max mem: 4109
I20241203 07:55:18 1983665 dinov2 helpers.py:102]   [230/634]  eta: 0:26:24    time: 3.972137  data: 0.001011  max mem: 4109
I20241203 07:55:58 1983665 dinov2 helpers.py:102]   [240/634]  eta: 0:25:45    time: 3.973399  data: 0.001479  max mem: 4109
I20241203 07:56:37 1983665 dinov2 helpers.py:102]   [250/634]  eta: 0:25:07    time: 3.973467  data: 0.001512  max mem: 4109
I20241203 07:57:17 1983665 dinov2 helpers.py:102]   [260/634]  eta: 0:24:28    time: 3.973886  data: 0.001057  max mem: 4109
I20241203 07:57:57 1983665 dinov2 helpers.py:102]   [270/634]  eta: 0:23:50    time: 3.973681  data: 0.000969  max mem: 4109
I20241203 07:58:37 1983665 dinov2 helpers.py:102]   [280/634]  eta: 0:23:11    time: 3.973281  data: 0.002351  max mem: 4109
I20241203 07:59:16 1983665 dinov2 helpers.py:102]   [290/634]  eta: 0:22:32    time: 3.973611  data: 0.002228  max mem: 4109
I20241203 07:59:56 1983665 dinov2 helpers.py:102]   [300/634]  eta: 0:21:53    time: 3.974629  data: 0.000905  max mem: 4109
I20241203 08:00:36 1983665 dinov2 helpers.py:102]   [310/634]  eta: 0:21:14    time: 3.974775  data: 0.001003  max mem: 4109
I20241203 08:01:16 1983665 dinov2 helpers.py:102]   [320/634]  eta: 0:20:35    time: 3.973893  data: 0.002016  max mem: 4109
I20241203 08:01:55 1983665 dinov2 helpers.py:102]   [330/634]  eta: 0:19:56    time: 3.973795  data: 0.002055  max mem: 4109
I20241203 08:02:35 1983665 dinov2 helpers.py:102]   [340/634]  eta: 0:19:17    time: 3.975802  data: 0.001083  max mem: 4109
I20241203 08:03:15 1983665 dinov2 helpers.py:102]   [350/634]  eta: 0:18:38    time: 3.976270  data: 0.000765  max mem: 4109
I20241203 08:03:55 1983665 dinov2 helpers.py:102]   [360/634]  eta: 0:17:59    time: 3.974236  data: 0.000553  max mem: 4109
I20241203 08:04:34 1983665 dinov2 helpers.py:102]   [370/634]  eta: 0:17:20    time: 3.975780  data: 0.000944  max mem: 4109
I20241203 08:05:14 1983665 dinov2 helpers.py:102]   [380/634]  eta: 0:16:41    time: 3.976073  data: 0.001081  max mem: 4109
I20241203 08:05:54 1983665 dinov2 helpers.py:102]   [390/634]  eta: 0:16:02    time: 3.974511  data: 0.001440  max mem: 4109
I20241203 08:06:34 1983665 dinov2 helpers.py:102]   [400/634]  eta: 0:15:22    time: 3.974574  data: 0.001528  max mem: 4109
I20241203 08:07:13 1983665 dinov2 helpers.py:102]   [410/634]  eta: 0:14:43    time: 3.974488  data: 0.001136  max mem: 4109
I20241203 08:07:53 1983665 dinov2 helpers.py:102]   [420/634]  eta: 0:14:04    time: 3.976108  data: 0.001430  max mem: 4109
I20241203 08:08:33 1983665 dinov2 helpers.py:102]   [430/634]  eta: 0:13:24    time: 3.975978  data: 0.001263  max mem: 4109
I20241203 08:09:13 1983665 dinov2 helpers.py:102]   [440/634]  eta: 0:12:45    time: 3.977078  data: 0.000876  max mem: 4109
I20241203 08:09:52 1983665 dinov2 helpers.py:102]   [450/634]  eta: 0:12:06    time: 3.977385  data: 0.000858  max mem: 4109
I20241203 08:10:32 1983665 dinov2 helpers.py:102]   [460/634]  eta: 0:11:26    time: 3.976179  data: 0.001617  max mem: 4109
I20241203 08:11:12 1983665 dinov2 helpers.py:102]   [470/634]  eta: 0:10:47    time: 3.975926  data: 0.001605  max mem: 4109
I20241203 08:11:52 1983665 dinov2 helpers.py:102]   [480/634]  eta: 0:10:08    time: 3.976063  data: 0.000965  max mem: 4109
I20241203 08:12:31 1983665 dinov2 helpers.py:102]   [490/634]  eta: 0:09:28    time: 3.977731  data: 0.000913  max mem: 4109
I20241203 08:13:11 1983665 dinov2 helpers.py:102]   [500/634]  eta: 0:08:49    time: 3.976125  data: 0.000913  max mem: 4109
I20241203 08:13:51 1983665 dinov2 helpers.py:102]   [510/634]  eta: 0:08:09    time: 3.976132  data: 0.001047  max mem: 4109
I20241203 08:14:31 1983665 dinov2 helpers.py:102]   [520/634]  eta: 0:07:30    time: 3.976002  data: 0.001070  max mem: 4109
I20241203 08:15:10 1983665 dinov2 helpers.py:102]   [530/634]  eta: 0:06:50    time: 3.976074  data: 0.000933  max mem: 4109
I20241203 08:15:50 1983665 dinov2 helpers.py:102]   [540/634]  eta: 0:06:11    time: 3.977852  data: 0.000658  max mem: 4109
I20241203 08:16:30 1983665 dinov2 helpers.py:102]   [550/634]  eta: 0:05:32    time: 3.978810  data: 0.000712  max mem: 4109
I20241203 08:17:10 1983665 dinov2 helpers.py:102]   [560/634]  eta: 0:04:52    time: 3.978905  data: 0.000884  max mem: 4109
I20241203 08:17:50 1983665 dinov2 helpers.py:102]   [570/634]  eta: 0:04:13    time: 3.977938  data: 0.001725  max mem: 4109
I20241203 08:18:29 1983665 dinov2 helpers.py:102]   [580/634]  eta: 0:03:33    time: 3.976110  data: 0.002140  max mem: 4109
I20241203 08:19:09 1983665 dinov2 helpers.py:102]   [590/634]  eta: 0:02:53    time: 3.974270  data: 0.001620  max mem: 4109
I20241203 08:19:49 1983665 dinov2 helpers.py:102]   [600/634]  eta: 0:02:14    time: 3.972914  data: 0.001277  max mem: 4109
I20241203 08:20:29 1983665 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.972457  data: 0.001946  max mem: 4109
I20241203 08:21:08 1983665 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.972160  data: 0.002087  max mem: 4109
I20241203 08:21:46 1983665 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.892575  data: 0.001434  max mem: 4109
I20241203 08:22:03 1983665 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.150450  data: 0.001085  max mem: 4109
I20241203 08:22:04 1983665 dinov2 helpers.py:130]  Total time: 0:41:52 (3.962869 s / it)
I20241203 08:22:04 1983665 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 08:22:04 1983665 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 08:22:05 1983665 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 08:22:05 1983665 dinov2 loaders.py:151] sampler: distributed
I20241203 08:22:05 1983665 dinov2 loaders.py:210] using PyTorch data loader
I20241203 08:22:05 1983665 dinov2 loaders.py:223] # of batches: 78
I20241203 08:22:44 1983665 dinov2 knn.py:299] Start the k-NN classification.
I20241203 08:22:56 1983665 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:15:44    time: 12.105674  data: 11.478812  max mem: 8496
I20241203 08:23:10 1983665 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:02:40    time: 2.358572  data: 1.646876  max mem: 8536
I20241203 08:23:20 1983665 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:01:39    time: 1.188739  data: 0.334764  max mem: 8536
I20241203 08:23:31 1983665 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:01:11    time: 1.016978  data: 0.003587  max mem: 8536
I20241203 08:23:41 1983665 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:00:52    time: 1.040553  data: 0.000945  max mem: 8536
I20241203 08:23:51 1983665 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:00:36    time: 1.041677  data: 0.000374  max mem: 8536
I20241203 08:24:02 1983665 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:00:22    time: 1.042982  data: 0.000183  max mem: 8536
I20241203 08:24:12 1983665 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:09    time: 1.043754  data: 0.000196  max mem: 8536
I20241203 08:24:17 1983665 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:01    time: 0.928455  data: 0.000169  max mem: 8536
I20241203 08:24:17 1983665 dinov2 helpers.py:130] Test: Total time: 0:01:33 (1.193210 s / it)
I20241203 08:24:17 1983665 dinov2 utils.py:79] Averaged stats: 
I20241203 08:24:17 1983665 dinov2 knn.py:367] ('full', 10) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:17 1983665 dinov2 knn.py:367] ('full', 20) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:17 1983665 dinov2 knn.py:367] ('full', 100) classifier result: Top1: 0.00 Top5: 0.00
I20241203 08:24:17 1983665 dinov2 knn.py:367] ('full', 200) classifier result: Top1: 0.00 Top5: 0.00
submitit INFO (2024-12-03 08:24:17,990) - Job completed successfully
I20241203 08:24:17 1983665 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-03 08:24:17,992) - Exiting after successful completion
I20241203 08:24:17 1983665 submitit submission.py:61] Exiting after successful completion
