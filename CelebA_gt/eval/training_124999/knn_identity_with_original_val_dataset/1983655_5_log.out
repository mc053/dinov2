submitit INFO (2024-12-03 07:39:19,667) - Starting with JobEnvironment(job_id=1983655, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-03 07:39:19,667) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn/1983655_submitted.pkl
I20241203 07:39:27 1983663 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 07:39:27 1983663 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 07:39:27 1983663 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 07:39:27 1983663 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 07:39:27 1983663 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 07:39:59 1983663 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 07:40:05 1983663 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 07:40:05 1983663 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 07:40:10 1983663 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 07:40:10 1983663 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 07:40:12 1983663 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 07:40:12 1983663 dinov2 knn.py:260] Extracting features for train set...
I20241203 07:40:12 1983663 dinov2 loaders.py:151] sampler: distributed
I20241203 07:40:12 1983663 dinov2 loaders.py:210] using PyTorch data loader
W20241203 07:40:12 1983663 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 07:40:12 1983663 dinov2 loaders.py:223] # of batches: 634
I20241203 07:40:52 1983663 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 07:40:56 1983663 dinov2 helpers.py:102]   [  0/634]  eta: 7:49:28    time: 44.429382  data: 12.934300  max mem: 3463
I20241203 07:41:18 1983663 dinov2 helpers.py:102]   [ 10/634]  eta: 1:02:28    time: 6.006802  data: 1.180099  max mem: 4109
I20241203 07:41:57 1983663 dinov2 helpers.py:102]   [ 20/634]  eta: 0:51:08    time: 3.025911  data: 0.002950  max mem: 4109
I20241203 07:42:36 1983663 dinov2 helpers.py:102]   [ 30/634]  eta: 0:46:49    time: 3.906825  data: 0.001107  max mem: 4109
I20241203 07:43:15 1983663 dinov2 helpers.py:102]   [ 40/634]  eta: 0:44:22    time: 3.942691  data: 0.001281  max mem: 4109
I20241203 07:43:55 1983663 dinov2 helpers.py:102]   [ 50/634]  eta: 0:42:37    time: 3.958199  data: 0.001132  max mem: 4109
I20241203 07:44:35 1983663 dinov2 helpers.py:102]   [ 60/634]  eta: 0:41:15    time: 3.964370  data: 0.001111  max mem: 4109
I20241203 07:45:14 1983663 dinov2 helpers.py:102]   [ 70/634]  eta: 0:40:06    time: 3.975912  data: 0.001137  max mem: 4109
I20241203 07:45:54 1983663 dinov2 helpers.py:102]   [ 80/634]  eta: 0:39:03    time: 3.980946  data: 0.000900  max mem: 4109
I20241203 07:46:34 1983663 dinov2 helpers.py:102]   [ 90/634]  eta: 0:38:07    time: 3.987755  data: 0.000940  max mem: 4109
I20241203 07:47:14 1983663 dinov2 helpers.py:102]   [100/634]  eta: 0:37:14    time: 3.993245  data: 0.001134  max mem: 4109
I20241203 07:47:54 1983663 dinov2 helpers.py:102]   [110/634]  eta: 0:36:23    time: 3.992537  data: 0.001109  max mem: 4109
I20241203 07:48:34 1983663 dinov2 helpers.py:102]   [120/634]  eta: 0:35:34    time: 3.991382  data: 0.000760  max mem: 4109
I20241203 07:49:14 1983663 dinov2 helpers.py:102]   [130/634]  eta: 0:34:46    time: 3.988132  data: 0.000626  max mem: 4109
I20241203 07:49:54 1983663 dinov2 helpers.py:102]   [140/634]  eta: 0:33:59    time: 3.980378  data: 0.000573  max mem: 4109
I20241203 07:50:33 1983663 dinov2 helpers.py:102]   [150/634]  eta: 0:33:13    time: 3.977009  data: 0.000752  max mem: 4109
I20241203 07:51:13 1983663 dinov2 helpers.py:102]   [160/634]  eta: 0:32:27    time: 3.976691  data: 0.000841  max mem: 4109
I20241203 07:51:53 1983663 dinov2 helpers.py:102]   [170/634]  eta: 0:31:42    time: 3.973621  data: 0.002187  max mem: 4109
I20241203 07:52:33 1983663 dinov2 helpers.py:102]   [180/634]  eta: 0:30:58    time: 3.975885  data: 0.002219  max mem: 4109
I20241203 07:53:12 1983663 dinov2 helpers.py:102]   [190/634]  eta: 0:30:15    time: 3.975244  data: 0.000725  max mem: 4109
I20241203 07:53:52 1983663 dinov2 helpers.py:102]   [200/634]  eta: 0:29:31    time: 3.973136  data: 0.001246  max mem: 4109
I20241203 07:54:32 1983663 dinov2 helpers.py:102]   [210/634]  eta: 0:28:48    time: 3.974456  data: 0.001272  max mem: 4109
I20241203 07:55:12 1983663 dinov2 helpers.py:102]   [220/634]  eta: 0:28:06    time: 3.974741  data: 0.000605  max mem: 4109
I20241203 07:55:51 1983663 dinov2 helpers.py:102]   [230/634]  eta: 0:27:23    time: 3.976871  data: 0.000598  max mem: 4109
I20241203 07:56:31 1983663 dinov2 helpers.py:102]   [240/634]  eta: 0:26:41    time: 3.979800  data: 0.000744  max mem: 4109
I20241203 07:57:11 1983663 dinov2 helpers.py:102]   [250/634]  eta: 0:25:59    time: 3.979877  data: 0.001444  max mem: 4109
I20241203 07:57:51 1983663 dinov2 helpers.py:102]   [260/634]  eta: 0:25:17    time: 3.980816  data: 0.001232  max mem: 4109
I20241203 07:58:31 1983663 dinov2 helpers.py:102]   [270/634]  eta: 0:24:36    time: 3.982673  data: 0.000516  max mem: 4109
I20241203 07:59:10 1983663 dinov2 helpers.py:102]   [280/634]  eta: 0:23:54    time: 3.982628  data: 0.000572  max mem: 4109
I20241203 07:59:50 1983663 dinov2 helpers.py:102]   [290/634]  eta: 0:23:13    time: 3.981712  data: 0.003134  max mem: 4109
I20241203 08:00:30 1983663 dinov2 helpers.py:102]   [300/634]  eta: 0:22:32    time: 3.981232  data: 0.003301  max mem: 4109
I20241203 08:01:10 1983663 dinov2 helpers.py:102]   [310/634]  eta: 0:21:50    time: 3.977463  data: 0.000775  max mem: 4109
I20241203 08:01:50 1983663 dinov2 helpers.py:102]   [320/634]  eta: 0:21:09    time: 3.982643  data: 0.000619  max mem: 4109
I20241203 08:02:30 1983663 dinov2 helpers.py:102]   [330/634]  eta: 0:20:28    time: 3.990226  data: 0.000786  max mem: 4109
I20241203 08:03:10 1983663 dinov2 helpers.py:102]   [340/634]  eta: 0:19:48    time: 3.991481  data: 0.000832  max mem: 4109
I20241203 08:03:49 1983663 dinov2 helpers.py:102]   [350/634]  eta: 0:19:07    time: 3.992699  data: 0.000702  max mem: 4109
I20241203 08:04:29 1983663 dinov2 helpers.py:102]   [360/634]  eta: 0:18:26    time: 3.989305  data: 0.000698  max mem: 4109
I20241203 08:05:09 1983663 dinov2 helpers.py:102]   [370/634]  eta: 0:17:45    time: 3.989115  data: 0.000770  max mem: 4109
I20241203 08:05:49 1983663 dinov2 helpers.py:102]   [380/634]  eta: 0:17:05    time: 3.993347  data: 0.000856  max mem: 4109
I20241203 08:06:29 1983663 dinov2 helpers.py:102]   [390/634]  eta: 0:16:24    time: 3.990600  data: 0.000803  max mem: 4109
I20241203 08:07:09 1983663 dinov2 helpers.py:102]   [400/634]  eta: 0:15:43    time: 3.987865  data: 0.000750  max mem: 4109
I20241203 08:07:49 1983663 dinov2 helpers.py:102]   [410/634]  eta: 0:15:03    time: 3.990735  data: 0.000667  max mem: 4109
I20241203 08:08:29 1983663 dinov2 helpers.py:102]   [420/634]  eta: 0:14:22    time: 3.990531  data: 0.001250  max mem: 4109
I20241203 08:09:09 1983663 dinov2 helpers.py:102]   [430/634]  eta: 0:13:42    time: 3.992284  data: 0.001248  max mem: 4109
I20241203 08:09:49 1983663 dinov2 helpers.py:102]   [440/634]  eta: 0:13:01    time: 3.994366  data: 0.000835  max mem: 4109
I20241203 08:10:29 1983663 dinov2 helpers.py:102]   [450/634]  eta: 0:12:21    time: 3.993228  data: 0.000775  max mem: 4109
I20241203 08:11:09 1983663 dinov2 helpers.py:102]   [460/634]  eta: 0:11:40    time: 3.994052  data: 0.000692  max mem: 4109
I20241203 08:11:49 1983663 dinov2 helpers.py:102]   [470/634]  eta: 0:11:00    time: 3.995038  data: 0.001362  max mem: 4109
I20241203 08:12:28 1983663 dinov2 helpers.py:102]   [480/634]  eta: 0:10:20    time: 3.991154  data: 0.001812  max mem: 4109
I20241203 08:13:08 1983663 dinov2 helpers.py:102]   [490/634]  eta: 0:09:39    time: 3.991177  data: 0.001155  max mem: 4109
I20241203 08:13:48 1983663 dinov2 helpers.py:102]   [500/634]  eta: 0:08:59    time: 3.993271  data: 0.000478  max mem: 4109
I20241203 08:14:28 1983663 dinov2 helpers.py:102]   [510/634]  eta: 0:08:19    time: 3.994121  data: 0.000617  max mem: 4109
I20241203 08:15:08 1983663 dinov2 helpers.py:102]   [520/634]  eta: 0:07:38    time: 3.993993  data: 0.000936  max mem: 4109
I20241203 08:15:48 1983663 dinov2 helpers.py:102]   [530/634]  eta: 0:06:58    time: 3.992266  data: 0.000906  max mem: 4109
I20241203 08:16:28 1983663 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.993227  data: 0.000698  max mem: 4109
I20241203 08:17:08 1983663 dinov2 helpers.py:102]   [550/634]  eta: 0:05:37    time: 3.994040  data: 0.000815  max mem: 4109
I20241203 08:17:48 1983663 dinov2 helpers.py:102]   [560/634]  eta: 0:04:57    time: 3.993218  data: 0.001216  max mem: 4109
I20241203 08:18:28 1983663 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.992458  data: 0.001315  max mem: 4109
I20241203 08:19:08 1983663 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.991392  data: 0.000920  max mem: 4109
I20241203 08:19:48 1983663 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.989100  data: 0.001374  max mem: 4109
I20241203 08:20:27 1983663 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.981426  data: 0.001374  max mem: 4109
I20241203 08:21:07 1983663 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.976751  data: 0.000771  max mem: 4109
I20241203 08:21:45 1983663 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.904752  data: 0.001000  max mem: 4109
I20241203 08:22:18 1983663 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.558726  data: 0.001688  max mem: 4109
I20241203 08:22:33 1983663 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 3.701361  data: 0.001609  max mem: 4109
I20241203 08:22:33 1983663 dinov2 helpers.py:130]  Total time: 0:42:21 (4.009004 s / it)
I20241203 08:22:33 1983663 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 08:22:33 1983663 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 08:22:34 1983663 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 08:22:34 1983663 dinov2 loaders.py:151] sampler: distributed
I20241203 08:22:34 1983663 dinov2 loaders.py:210] using PyTorch data loader
I20241203 08:22:34 1983663 dinov2 loaders.py:223] # of batches: 78
I20241203 08:22:46 1983663 dinov2 knn.py:299] Start the k-NN classification.
submitit ERROR (2024-12-03 08:23:01,934) - Submitted job triggered an exception
E20241203 08:23:01 1983663 submitit submission.py:68] Submitted job triggered an exception
