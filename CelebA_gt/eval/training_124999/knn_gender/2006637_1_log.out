submitit INFO (2024-12-03 08:52:42,216) - Starting with JobEnvironment(job_id=2006637, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-03 08:52:42,216) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2006637_submitted.pkl
I20241203 08:52:49 2006639 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 08:52:49 2006639 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 08:52:49 2006639 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 08:52:49 2006639 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 08:52:49 2006639 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 08:53:15 2006639 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 08:53:20 2006639 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 08:53:21 2006639 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 08:53:31 2006639 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 08:53:31 2006639 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 08:53:33 2006639 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 08:53:33 2006639 dinov2 knn.py:260] Extracting features for train set...
I20241203 08:53:33 2006639 dinov2 loaders.py:151] sampler: distributed
I20241203 08:53:33 2006639 dinov2 loaders.py:210] using PyTorch data loader
W20241203 08:53:33 2006639 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 08:53:33 2006639 dinov2 loaders.py:223] # of batches: 634
I20241203 08:54:06 2006639 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 08:54:06 2006639 dinov2 helpers.py:102]   [  0/634]  eta: 5:48:45    time: 33.006252  data: 12.284622  max mem: 3464
I20241203 08:54:15 2006639 dinov2 helpers.py:102]   [ 10/634]  eta: 0:40:03    time: 3.851883  data: 1.118505  max mem: 4109
I20241203 08:54:31 2006639 dinov2 helpers.py:102]   [ 20/634]  eta: 0:28:36    time: 1.284249  data: 0.002330  max mem: 4109
I20241203 08:55:10 2006639 dinov2 helpers.py:102]   [ 30/634]  eta: 0:31:29    time: 2.730355  data: 0.001842  max mem: 4109
I20241203 08:55:49 2006639 dinov2 helpers.py:102]   [ 40/634]  eta: 0:32:59    time: 3.895993  data: 0.000750  max mem: 4109
I20241203 08:56:29 2006639 dinov2 helpers.py:102]   [ 50/634]  eta: 0:33:39    time: 3.967729  data: 0.000632  max mem: 4109
I20241203 08:57:09 2006639 dinov2 helpers.py:102]   [ 60/634]  eta: 0:33:53    time: 3.972223  data: 0.000793  max mem: 4109
I20241203 08:57:49 2006639 dinov2 helpers.py:102]   [ 70/634]  eta: 0:33:52    time: 3.975492  data: 0.001353  max mem: 4109
I20241203 08:58:28 2006639 dinov2 helpers.py:102]   [ 80/634]  eta: 0:33:41    time: 3.976069  data: 0.001342  max mem: 4109
I20241203 08:59:08 2006639 dinov2 helpers.py:102]   [ 90/634]  eta: 0:33:24    time: 3.976272  data: 0.000745  max mem: 4109
I20241203 08:59:48 2006639 dinov2 helpers.py:102]   [100/634]  eta: 0:33:03    time: 3.978193  data: 0.000725  max mem: 4109
I20241203 09:00:28 2006639 dinov2 helpers.py:102]   [110/634]  eta: 0:32:38    time: 3.975204  data: 0.000943  max mem: 4109
I20241203 09:01:07 2006639 dinov2 helpers.py:102]   [120/634]  eta: 0:32:11    time: 3.973089  data: 0.001225  max mem: 4109
I20241203 09:01:47 2006639 dinov2 helpers.py:102]   [130/634]  eta: 0:31:41    time: 3.973344  data: 0.001208  max mem: 4109
I20241203 09:02:27 2006639 dinov2 helpers.py:102]   [140/634]  eta: 0:31:11    time: 3.973022  data: 0.000749  max mem: 4109
I20241203 09:03:07 2006639 dinov2 helpers.py:102]   [150/634]  eta: 0:30:39    time: 3.972718  data: 0.000739  max mem: 4109
I20241203 09:03:46 2006639 dinov2 helpers.py:102]   [160/634]  eta: 0:30:06    time: 3.972882  data: 0.000953  max mem: 4109
I20241203 09:04:26 2006639 dinov2 helpers.py:102]   [170/634]  eta: 0:29:32    time: 3.973119  data: 0.000777  max mem: 4109
I20241203 09:05:06 2006639 dinov2 helpers.py:102]   [180/634]  eta: 0:28:58    time: 3.973407  data: 0.000695  max mem: 4109
I20241203 09:05:45 2006639 dinov2 helpers.py:102]   [190/634]  eta: 0:28:23    time: 3.973420  data: 0.001546  max mem: 4109
I20241203 09:06:25 2006639 dinov2 helpers.py:102]   [200/634]  eta: 0:27:47    time: 3.974528  data: 0.001505  max mem: 4109
I20241203 09:07:05 2006639 dinov2 helpers.py:102]   [210/634]  eta: 0:27:12    time: 3.975349  data: 0.000844  max mem: 4109
I20241203 09:07:45 2006639 dinov2 helpers.py:102]   [220/634]  eta: 0:26:35    time: 3.974238  data: 0.002398  max mem: 4109
I20241203 09:08:24 2006639 dinov2 helpers.py:102]   [230/634]  eta: 0:25:59    time: 3.973282  data: 0.002928  max mem: 4109
I20241203 09:09:04 2006639 dinov2 helpers.py:102]   [240/634]  eta: 0:25:22    time: 3.973194  data: 0.001626  max mem: 4109
I20241203 09:09:44 2006639 dinov2 helpers.py:102]   [250/634]  eta: 0:24:45    time: 3.975033  data: 0.001224  max mem: 4109
I20241203 09:10:24 2006639 dinov2 helpers.py:102]   [260/634]  eta: 0:24:08    time: 3.975145  data: 0.000912  max mem: 4109
I20241203 09:11:03 2006639 dinov2 helpers.py:102]   [270/634]  eta: 0:23:31    time: 3.975401  data: 0.002364  max mem: 4109
I20241203 09:11:43 2006639 dinov2 helpers.py:102]   [280/634]  eta: 0:22:53    time: 3.981278  data: 0.002443  max mem: 4109
I20241203 09:12:23 2006639 dinov2 helpers.py:102]   [290/634]  eta: 0:22:16    time: 3.983194  data: 0.000650  max mem: 4109
I20241203 09:13:03 2006639 dinov2 helpers.py:102]   [300/634]  eta: 0:21:38    time: 3.980314  data: 0.001646  max mem: 4109
I20241203 09:13:43 2006639 dinov2 helpers.py:102]   [310/634]  eta: 0:21:00    time: 3.981249  data: 0.001664  max mem: 4109
I20241203 09:14:23 2006639 dinov2 helpers.py:102]   [320/634]  eta: 0:20:22    time: 3.985218  data: 0.001559  max mem: 4109
I20241203 09:15:02 2006639 dinov2 helpers.py:102]   [330/634]  eta: 0:19:44    time: 3.983437  data: 0.001827  max mem: 4109
I20241203 09:15:42 2006639 dinov2 helpers.py:102]   [340/634]  eta: 0:19:06    time: 3.978521  data: 0.001574  max mem: 4109
I20241203 09:16:22 2006639 dinov2 helpers.py:102]   [350/634]  eta: 0:18:27    time: 3.982932  data: 0.001566  max mem: 4109
I20241203 09:17:02 2006639 dinov2 helpers.py:102]   [360/634]  eta: 0:17:49    time: 3.990091  data: 0.001141  max mem: 4109
I20241203 09:17:42 2006639 dinov2 helpers.py:102]   [370/634]  eta: 0:17:11    time: 3.989098  data: 0.000915  max mem: 4109
I20241203 09:18:22 2006639 dinov2 helpers.py:102]   [380/634]  eta: 0:16:32    time: 3.979890  data: 0.001507  max mem: 4109
I20241203 09:19:01 2006639 dinov2 helpers.py:102]   [390/634]  eta: 0:15:53    time: 3.974179  data: 0.001555  max mem: 4109
I20241203 09:19:41 2006639 dinov2 helpers.py:102]   [400/634]  eta: 0:15:15    time: 3.975024  data: 0.001832  max mem: 4109
I20241203 09:20:21 2006639 dinov2 helpers.py:102]   [410/634]  eta: 0:14:36    time: 3.974367  data: 0.002120  max mem: 4109
I20241203 09:21:01 2006639 dinov2 helpers.py:102]   [420/634]  eta: 0:13:57    time: 3.975573  data: 0.001403  max mem: 4109
I20241203 09:21:40 2006639 dinov2 helpers.py:102]   [430/634]  eta: 0:13:18    time: 3.975838  data: 0.001947  max mem: 4109
I20241203 09:22:20 2006639 dinov2 helpers.py:102]   [440/634]  eta: 0:12:39    time: 3.976573  data: 0.001804  max mem: 4109
I20241203 09:23:00 2006639 dinov2 helpers.py:102]   [450/634]  eta: 0:12:00    time: 3.976484  data: 0.002087  max mem: 4109
I20241203 09:23:40 2006639 dinov2 helpers.py:102]   [460/634]  eta: 0:11:21    time: 3.973998  data: 0.002040  max mem: 4109
I20241203 09:24:19 2006639 dinov2 helpers.py:102]   [470/634]  eta: 0:10:43    time: 3.977718  data: 0.001463  max mem: 4109
I20241203 09:24:59 2006639 dinov2 helpers.py:102]   [480/634]  eta: 0:10:04    time: 3.983100  data: 0.001621  max mem: 4109
I20241203 09:25:39 2006639 dinov2 helpers.py:102]   [490/634]  eta: 0:09:24    time: 3.987463  data: 0.001367  max mem: 4109
I20241203 09:26:19 2006639 dinov2 helpers.py:102]   [500/634]  eta: 0:08:45    time: 3.988375  data: 0.002128  max mem: 4109
I20241203 09:26:59 2006639 dinov2 helpers.py:102]   [510/634]  eta: 0:08:06    time: 3.985689  data: 0.002760  max mem: 4109
I20241203 09:27:39 2006639 dinov2 helpers.py:102]   [520/634]  eta: 0:07:27    time: 3.979578  data: 0.001744  max mem: 4109
I20241203 09:28:18 2006639 dinov2 helpers.py:102]   [530/634]  eta: 0:06:48    time: 3.976699  data: 0.001128  max mem: 4109
I20241203 09:28:58 2006639 dinov2 helpers.py:102]   [540/634]  eta: 0:06:09    time: 3.981929  data: 0.001165  max mem: 4109
I20241203 09:29:38 2006639 dinov2 helpers.py:102]   [550/634]  eta: 0:05:30    time: 3.984723  data: 0.000851  max mem: 4109
I20241203 09:30:18 2006639 dinov2 helpers.py:102]   [560/634]  eta: 0:04:50    time: 3.984924  data: 0.000868  max mem: 4109
I20241203 09:30:58 2006639 dinov2 helpers.py:102]   [570/634]  eta: 0:04:11    time: 3.979475  data: 0.000804  max mem: 4109
I20241203 09:31:38 2006639 dinov2 helpers.py:102]   [580/634]  eta: 0:03:32    time: 3.975470  data: 0.001091  max mem: 4109
I20241203 09:32:17 2006639 dinov2 helpers.py:102]   [590/634]  eta: 0:02:53    time: 3.975478  data: 0.001621  max mem: 4109
I20241203 09:32:57 2006639 dinov2 helpers.py:102]   [600/634]  eta: 0:02:13    time: 3.973564  data: 0.002127  max mem: 4109
I20241203 09:33:37 2006639 dinov2 helpers.py:102]   [610/634]  eta: 0:01:34    time: 3.974260  data: 0.002384  max mem: 4109
I20241203 09:34:16 2006639 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.974000  data: 0.001853  max mem: 4109
I20241203 09:34:56 2006639 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.972856  data: 0.001360  max mem: 4109
I20241203 09:35:16 2006639 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.346170  data: 0.001021  max mem: 4109
I20241203 09:35:16 2006639 dinov2 helpers.py:130]  Total time: 0:41:43 (3.948327 s / it)
I20241203 09:35:16 2006639 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 09:35:16 2006639 dinov2 utils.py:142] Labels shape: (162127,)
submitit ERROR (2024-12-03 09:35:17,378) - Submitted job triggered an exception
E20241203 09:35:17 2006639 submitit submission.py:68] Submitted job triggered an exception
