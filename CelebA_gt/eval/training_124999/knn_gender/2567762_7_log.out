submitit INFO (2024-12-04 10:17:37,628) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=7(8), node=0(1), global_rank=7(8))
submitit INFO (2024-12-04 10:17:37,628) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567770 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567770 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567770 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567770 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567770 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:17 2567770 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:22 2567770 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:22 2567770 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:29 2567770 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:29 2567770 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:33 2567770 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:33 2567770 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:33 2567770 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:33 2567770 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:33 2567770 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:33 2567770 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:10 2567770 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:10 2567770 dinov2 helpers.py:102]   [  0/634]  eta: 6:37:08    time: 37.583649  data: 18.742971  max mem: 3463
I20241204 10:19:33 2567770 dinov2 helpers.py:102]   [ 10/634]  eta: 0:56:55    time: 5.473314  data: 1.705938  max mem: 4109
I20241204 10:20:12 2567770 dinov2 helpers.py:102]   [ 20/634]  eta: 0:48:18    time: 3.076952  data: 0.001726  max mem: 4109
I20241204 10:20:51 2567770 dinov2 helpers.py:102]   [ 30/634]  eta: 0:44:59    time: 3.916665  data: 0.001302  max mem: 4109
I20241204 10:21:31 2567770 dinov2 helpers.py:102]   [ 40/634]  eta: 0:43:02    time: 3.954946  data: 0.001116  max mem: 4109
I20241204 10:22:11 2567770 dinov2 helpers.py:102]   [ 50/634]  eta: 0:41:35    time: 3.970791  data: 0.000848  max mem: 4109
I20241204 10:22:51 2567770 dinov2 helpers.py:102]   [ 60/634]  eta: 0:40:25    time: 3.978135  data: 0.001596  max mem: 4109
I20241204 10:23:31 2567770 dinov2 helpers.py:102]   [ 70/634]  eta: 0:39:24    time: 3.983774  data: 0.001850  max mem: 4109
I20241204 10:24:10 2567770 dinov2 helpers.py:102]   [ 80/634]  eta: 0:38:27    time: 3.982177  data: 0.001727  max mem: 4109
I20241204 10:24:50 2567770 dinov2 helpers.py:102]   [ 90/634]  eta: 0:37:34    time: 3.978734  data: 0.001571  max mem: 4109
I20241204 10:25:30 2567770 dinov2 helpers.py:102]   [100/634]  eta: 0:36:45    time: 3.981401  data: 0.001012  max mem: 4109
I20241204 10:26:10 2567770 dinov2 helpers.py:102]   [110/634]  eta: 0:35:56    time: 3.982703  data: 0.000776  max mem: 4109
I20241204 10:26:50 2567770 dinov2 helpers.py:102]   [120/634]  eta: 0:35:09    time: 3.981735  data: 0.000753  max mem: 4109
I20241204 10:27:29 2567770 dinov2 helpers.py:102]   [130/634]  eta: 0:34:23    time: 3.978404  data: 0.002163  max mem: 4109
I20241204 10:28:09 2567770 dinov2 helpers.py:102]   [140/634]  eta: 0:33:38    time: 3.978376  data: 0.002218  max mem: 4109
I20241204 10:28:49 2567770 dinov2 helpers.py:102]   [150/634]  eta: 0:32:54    time: 3.979694  data: 0.000936  max mem: 4109
I20241204 10:29:29 2567770 dinov2 helpers.py:102]   [160/634]  eta: 0:32:10    time: 3.977947  data: 0.001602  max mem: 4109
I20241204 10:30:09 2567770 dinov2 helpers.py:102]   [170/634]  eta: 0:31:27    time: 3.980452  data: 0.001757  max mem: 4109
I20241204 10:30:48 2567770 dinov2 helpers.py:102]   [180/634]  eta: 0:30:44    time: 3.978619  data: 0.001140  max mem: 4109
I20241204 10:31:28 2567770 dinov2 helpers.py:102]   [190/634]  eta: 0:30:02    time: 3.977780  data: 0.001047  max mem: 4109
I20241204 10:32:08 2567770 dinov2 helpers.py:102]   [200/634]  eta: 0:29:19    time: 3.983223  data: 0.001937  max mem: 4109
I20241204 10:32:48 2567770 dinov2 helpers.py:102]   [210/634]  eta: 0:28:37    time: 3.984941  data: 0.001943  max mem: 4109
I20241204 10:33:28 2567770 dinov2 helpers.py:102]   [220/634]  eta: 0:27:56    time: 3.981516  data: 0.001222  max mem: 4109
I20241204 10:34:07 2567770 dinov2 helpers.py:102]   [230/634]  eta: 0:27:14    time: 3.979676  data: 0.001270  max mem: 4109
I20241204 10:34:47 2567770 dinov2 helpers.py:102]   [240/634]  eta: 0:26:32    time: 3.978525  data: 0.002254  max mem: 4109
I20241204 10:35:27 2567770 dinov2 helpers.py:102]   [250/634]  eta: 0:25:51    time: 3.975896  data: 0.002820  max mem: 4109
I20241204 10:36:07 2567770 dinov2 helpers.py:102]   [260/634]  eta: 0:25:10    time: 3.976466  data: 0.001726  max mem: 4109
I20241204 10:36:47 2567770 dinov2 helpers.py:102]   [270/634]  eta: 0:24:28    time: 3.979253  data: 0.001543  max mem: 4109
I20241204 10:37:26 2567770 dinov2 helpers.py:102]   [280/634]  eta: 0:23:47    time: 3.976859  data: 0.001452  max mem: 4109
I20241204 10:38:06 2567770 dinov2 helpers.py:102]   [290/634]  eta: 0:23:06    time: 3.978590  data: 0.000950  max mem: 4109
I20241204 10:38:46 2567770 dinov2 helpers.py:102]   [300/634]  eta: 0:22:25    time: 3.978622  data: 0.001005  max mem: 4109
I20241204 10:39:26 2567770 dinov2 helpers.py:102]   [310/634]  eta: 0:21:45    time: 3.979720  data: 0.001274  max mem: 4109
I20241204 10:40:06 2567770 dinov2 helpers.py:102]   [320/634]  eta: 0:21:04    time: 3.984931  data: 0.001564  max mem: 4109
I20241204 10:40:45 2567770 dinov2 helpers.py:102]   [330/634]  eta: 0:20:23    time: 3.985673  data: 0.001397  max mem: 4109
I20241204 10:41:25 2567770 dinov2 helpers.py:102]   [340/634]  eta: 0:19:43    time: 3.985023  data: 0.001070  max mem: 4109
I20241204 10:42:05 2567770 dinov2 helpers.py:102]   [350/634]  eta: 0:19:02    time: 3.983283  data: 0.001084  max mem: 4109
I20241204 10:42:45 2567770 dinov2 helpers.py:102]   [360/634]  eta: 0:18:22    time: 3.983277  data: 0.001128  max mem: 4109
I20241204 10:43:25 2567770 dinov2 helpers.py:102]   [370/634]  eta: 0:17:41    time: 3.983217  data: 0.001311  max mem: 4109
I20241204 10:44:05 2567770 dinov2 helpers.py:102]   [380/634]  eta: 0:17:01    time: 3.985034  data: 0.001058  max mem: 4109
I20241204 10:44:44 2567770 dinov2 helpers.py:102]   [390/634]  eta: 0:16:20    time: 3.987734  data: 0.001503  max mem: 4109
I20241204 10:45:24 2567770 dinov2 helpers.py:102]   [400/634]  eta: 0:15:40    time: 3.985182  data: 0.001827  max mem: 4109
I20241204 10:46:04 2567770 dinov2 helpers.py:102]   [410/634]  eta: 0:14:59    time: 3.985271  data: 0.001807  max mem: 4109
I20241204 10:46:44 2567770 dinov2 helpers.py:102]   [420/634]  eta: 0:14:19    time: 3.986076  data: 0.001608  max mem: 4109
I20241204 10:47:24 2567770 dinov2 helpers.py:102]   [430/634]  eta: 0:13:39    time: 3.985040  data: 0.000761  max mem: 4109
I20241204 10:48:04 2567770 dinov2 helpers.py:102]   [440/634]  eta: 0:12:58    time: 3.982275  data: 0.000617  max mem: 4109
I20241204 10:48:43 2567770 dinov2 helpers.py:102]   [450/634]  eta: 0:12:18    time: 3.979261  data: 0.000922  max mem: 4109
I20241204 10:49:23 2567770 dinov2 helpers.py:102]   [460/634]  eta: 0:11:38    time: 3.979996  data: 0.000994  max mem: 4109
I20241204 10:50:03 2567770 dinov2 helpers.py:102]   [470/634]  eta: 0:10:58    time: 3.979505  data: 0.001223  max mem: 4109
I20241204 10:50:43 2567770 dinov2 helpers.py:102]   [480/634]  eta: 0:10:17    time: 3.981392  data: 0.001356  max mem: 4109
I20241204 10:51:23 2567770 dinov2 helpers.py:102]   [490/634]  eta: 0:09:37    time: 3.984067  data: 0.000943  max mem: 4109
I20241204 10:52:03 2567770 dinov2 helpers.py:102]   [500/634]  eta: 0:08:57    time: 3.985041  data: 0.000816  max mem: 4109
I20241204 10:52:43 2567770 dinov2 helpers.py:102]   [510/634]  eta: 0:08:17    time: 3.987731  data: 0.000692  max mem: 4109
I20241204 10:53:22 2567770 dinov2 helpers.py:102]   [520/634]  eta: 0:07:37    time: 3.989689  data: 0.000578  max mem: 4109
I20241204 10:54:02 2567770 dinov2 helpers.py:102]   [530/634]  eta: 0:06:57    time: 3.987722  data: 0.000934  max mem: 4109
I20241204 10:54:42 2567770 dinov2 helpers.py:102]   [540/634]  eta: 0:06:16    time: 3.989557  data: 0.001109  max mem: 4109
I20241204 10:55:22 2567770 dinov2 helpers.py:102]   [550/634]  eta: 0:05:36    time: 3.990485  data: 0.000924  max mem: 4109
I20241204 10:56:02 2567770 dinov2 helpers.py:102]   [560/634]  eta: 0:04:56    time: 3.987738  data: 0.000775  max mem: 4109
I20241204 10:56:42 2567770 dinov2 helpers.py:102]   [570/634]  eta: 0:04:16    time: 3.987958  data: 0.000798  max mem: 4109
I20241204 10:57:22 2567770 dinov2 helpers.py:102]   [580/634]  eta: 0:03:36    time: 3.987024  data: 0.001280  max mem: 4109
I20241204 10:58:02 2567770 dinov2 helpers.py:102]   [590/634]  eta: 0:02:56    time: 3.987793  data: 0.001195  max mem: 4109
I20241204 10:58:41 2567770 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.989583  data: 0.001616  max mem: 4109
I20241204 10:59:21 2567770 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.986801  data: 0.001901  max mem: 4109
I20241204 11:00:01 2567770 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.985829  data: 0.001295  max mem: 4109
I20241204 11:00:40 2567770 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.943086  data: 0.001357  max mem: 4109
I20241204 11:00:59 2567770 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.279008  data: 0.001038  max mem: 4109
I20241204 11:00:59 2567770 dinov2 helpers.py:130]  Total time: 0:42:26 (4.016438 s / it)
I20241204 11:00:59 2567770 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:00:59 2567770 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:00 2567770 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:00 2567770 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:00 2567770 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:00 2567770 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:00 2567770 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:08 2567770 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:09:40    time: 7.444987  data: 4.105533  max mem: 4109
I20241204 11:01:40 2567770 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:03:59    time: 3.528290  data: 0.378419  max mem: 4109
I20241204 11:02:19 2567770 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:36    time: 3.556161  data: 0.005678  max mem: 4109
I20241204 11:02:59 2567770 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:03    time: 3.984932  data: 0.005922  max mem: 4109
I20241204 11:03:39 2567770 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:26    time: 3.994153  data: 0.006194  max mem: 4109
I20241204 11:04:19 2567770 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:48    time: 3.993205  data: 0.005729  max mem: 4109
I20241204 11:04:59 2567770 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 3.989959  data: 0.005183  max mem: 4109
I20241204 11:05:39 2567770 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 3.978251  data: 0.006432  max mem: 4109
I20241204 11:06:01 2567770 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.679665  data: 0.004363  max mem: 4109
I20241204 11:06:01 2567770 dinov2 helpers.py:130] Test: Total time: 0:04:59 (3.844593 s / it)
I20241204 11:06:01 2567770 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:02 2567770 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:02 2567770 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:02 2567770 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:02 2567770 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:02,532) - Job completed successfully
I20241204 11:06:02 2567770 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:02,543) - Exiting after successful completion
I20241204 11:06:02 2567770 submitit submission.py:61] Exiting after successful completion
