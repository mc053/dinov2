submitit INFO (2024-12-03 08:52:42,221) - Starting with JobEnvironment(job_id=2006637, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-03 08:52:42,222) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2006637_submitted.pkl
I20241203 08:52:50 2006642 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 08:52:50 2006642 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 08:52:50 2006642 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 08:52:50 2006642 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 08:52:50 2006642 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 08:53:25 2006642 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 08:53:29 2006642 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 08:53:29 2006642 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 08:53:41 2006642 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 08:53:41 2006642 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 08:53:48 2006642 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 08:53:48 2006642 dinov2 knn.py:260] Extracting features for train set...
I20241203 08:53:48 2006642 dinov2 loaders.py:151] sampler: distributed
I20241203 08:53:48 2006642 dinov2 loaders.py:210] using PyTorch data loader
W20241203 08:53:48 2006642 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 08:53:48 2006642 dinov2 loaders.py:223] # of batches: 634
I20241203 08:54:35 2006642 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 08:54:35 2006642 dinov2 helpers.py:102]   [  0/634]  eta: 8:21:48    time: 47.489548  data: 16.730663  max mem: 3464
I20241203 08:55:07 2006642 dinov2 helpers.py:102]   [ 10/634]  eta: 1:15:21    time: 7.245647  data: 1.522142  max mem: 4109
I20241203 08:55:47 2006642 dinov2 helpers.py:102]   [ 20/634]  eta: 0:58:10    time: 3.594160  data: 0.001254  max mem: 4109
I20241203 08:56:27 2006642 dinov2 helpers.py:102]   [ 30/634]  eta: 0:51:39    time: 3.969493  data: 0.001288  max mem: 4109
I20241203 08:57:07 2006642 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:00    time: 3.972960  data: 0.001123  max mem: 4109
I20241203 08:57:46 2006642 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:31    time: 3.973865  data: 0.001565  max mem: 4109
I20241203 08:58:26 2006642 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:38    time: 3.974367  data: 0.002884  max mem: 4109
I20241203 08:59:06 2006642 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:07    time: 3.978814  data: 0.002495  max mem: 4109
I20241203 08:59:46 2006642 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:47    time: 3.978137  data: 0.001156  max mem: 4109
I20241203 09:00:25 2006642 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:36    time: 3.973394  data: 0.000779  max mem: 4109
I20241203 09:01:05 2006642 dinov2 helpers.py:102]   [100/634]  eta: 0:38:32    time: 3.974021  data: 0.001003  max mem: 4109
I20241203 09:01:45 2006642 dinov2 helpers.py:102]   [110/634]  eta: 0:37:32    time: 3.974872  data: 0.001094  max mem: 4109
I20241203 09:02:25 2006642 dinov2 helpers.py:102]   [120/634]  eta: 0:36:35    time: 3.973935  data: 0.000718  max mem: 4109
I20241203 09:03:04 2006642 dinov2 helpers.py:102]   [130/634]  eta: 0:35:41    time: 3.973906  data: 0.000744  max mem: 4109
I20241203 09:03:44 2006642 dinov2 helpers.py:102]   [140/634]  eta: 0:34:49    time: 3.974699  data: 0.003963  max mem: 4109
I20241203 09:04:24 2006642 dinov2 helpers.py:102]   [150/634]  eta: 0:33:58    time: 3.973974  data: 0.003977  max mem: 4109
I20241203 09:05:03 2006642 dinov2 helpers.py:102]   [160/634]  eta: 0:33:09    time: 3.973370  data: 0.000947  max mem: 4109
I20241203 09:05:43 2006642 dinov2 helpers.py:102]   [170/634]  eta: 0:32:21    time: 3.974340  data: 0.000957  max mem: 4109
I20241203 09:06:23 2006642 dinov2 helpers.py:102]   [180/634]  eta: 0:31:34    time: 3.974478  data: 0.001064  max mem: 4109
I20241203 09:07:03 2006642 dinov2 helpers.py:102]   [190/634]  eta: 0:30:48    time: 3.973619  data: 0.001228  max mem: 4109
I20241203 09:07:42 2006642 dinov2 helpers.py:102]   [200/634]  eta: 0:30:02    time: 3.973378  data: 0.001458  max mem: 4109
I20241203 09:08:22 2006642 dinov2 helpers.py:102]   [210/634]  eta: 0:29:17    time: 3.973225  data: 0.001602  max mem: 4109
I20241203 09:09:02 2006642 dinov2 helpers.py:102]   [220/634]  eta: 0:28:32    time: 3.974070  data: 0.001268  max mem: 4109
I20241203 09:09:42 2006642 dinov2 helpers.py:102]   [230/634]  eta: 0:27:48    time: 3.974173  data: 0.001919  max mem: 4109
I20241203 09:10:21 2006642 dinov2 helpers.py:102]   [240/634]  eta: 0:27:04    time: 3.973350  data: 0.002089  max mem: 4109
I20241203 09:11:01 2006642 dinov2 helpers.py:102]   [250/634]  eta: 0:26:21    time: 3.975339  data: 0.002112  max mem: 4109
I20241203 09:11:41 2006642 dinov2 helpers.py:102]   [260/634]  eta: 0:25:37    time: 3.979293  data: 0.002388  max mem: 4109
I20241203 09:12:21 2006642 dinov2 helpers.py:102]   [270/634]  eta: 0:24:55    time: 3.985728  data: 0.001402  max mem: 4109
I20241203 09:13:01 2006642 dinov2 helpers.py:102]   [280/634]  eta: 0:24:12    time: 3.991249  data: 0.001029  max mem: 4109
I20241203 09:13:41 2006642 dinov2 helpers.py:102]   [290/634]  eta: 0:23:30    time: 3.994064  data: 0.001125  max mem: 4109
I20241203 09:14:21 2006642 dinov2 helpers.py:102]   [300/634]  eta: 0:22:48    time: 3.994926  data: 0.001961  max mem: 4109
I20241203 09:15:01 2006642 dinov2 helpers.py:102]   [310/634]  eta: 0:22:06    time: 3.990455  data: 0.003802  max mem: 4109
I20241203 09:15:40 2006642 dinov2 helpers.py:102]   [320/634]  eta: 0:21:24    time: 3.988535  data: 0.003348  max mem: 4109
I20241203 09:16:20 2006642 dinov2 helpers.py:102]   [330/634]  eta: 0:20:42    time: 3.982435  data: 0.001338  max mem: 4109
I20241203 09:17:00 2006642 dinov2 helpers.py:102]   [340/634]  eta: 0:20:00    time: 3.973988  data: 0.000886  max mem: 4109
I20241203 09:17:40 2006642 dinov2 helpers.py:102]   [350/634]  eta: 0:19:18    time: 3.979715  data: 0.002175  max mem: 4109
I20241203 09:18:20 2006642 dinov2 helpers.py:102]   [360/634]  eta: 0:18:37    time: 3.983486  data: 0.002118  max mem: 4109
I20241203 09:18:59 2006642 dinov2 helpers.py:102]   [370/634]  eta: 0:17:55    time: 3.976943  data: 0.001070  max mem: 4109
I20241203 09:19:39 2006642 dinov2 helpers.py:102]   [380/634]  eta: 0:17:14    time: 3.977676  data: 0.001949  max mem: 4109
I20241203 09:20:19 2006642 dinov2 helpers.py:102]   [390/634]  eta: 0:16:33    time: 3.985130  data: 0.001881  max mem: 4109
I20241203 09:20:59 2006642 dinov2 helpers.py:102]   [400/634]  eta: 0:15:51    time: 3.989030  data: 0.001224  max mem: 4109
I20241203 09:21:39 2006642 dinov2 helpers.py:102]   [410/634]  eta: 0:15:10    time: 3.987647  data: 0.000899  max mem: 4109
I20241203 09:22:19 2006642 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.981502  data: 0.000809  max mem: 4109
I20241203 09:22:58 2006642 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.982656  data: 0.000902  max mem: 4109
I20241203 09:23:38 2006642 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.988045  data: 0.001902  max mem: 4109
I20241203 09:24:18 2006642 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.990376  data: 0.001964  max mem: 4109
I20241203 09:24:58 2006642 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.988454  data: 0.000795  max mem: 4109
I20241203 09:25:38 2006642 dinov2 helpers.py:102]   [470/634]  eta: 0:11:05    time: 3.979345  data: 0.001868  max mem: 4109
I20241203 09:26:18 2006642 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.976707  data: 0.001936  max mem: 4109
I20241203 09:26:58 2006642 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.982093  data: 0.000773  max mem: 4109
I20241203 09:27:37 2006642 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.988512  data: 0.001044  max mem: 4109
I20241203 09:28:17 2006642 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.989233  data: 0.001576  max mem: 4109
I20241203 09:28:57 2006642 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.980114  data: 0.001089  max mem: 4109
I20241203 09:29:37 2006642 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.976577  data: 0.001082  max mem: 4109
I20241203 09:30:17 2006642 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.982181  data: 0.001896  max mem: 4109
I20241203 09:30:57 2006642 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.986652  data: 0.002186  max mem: 4109
I20241203 09:31:36 2006642 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.984586  data: 0.002393  max mem: 4109
I20241203 09:32:16 2006642 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.979130  data: 0.001579  max mem: 4109
I20241203 09:32:56 2006642 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.975311  data: 0.000782  max mem: 4109
I20241203 09:33:36 2006642 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.974171  data: 0.000981  max mem: 4109
I20241203 09:34:15 2006642 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.974782  data: 0.002770  max mem: 4109
I20241203 09:34:55 2006642 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.973855  data: 0.002826  max mem: 4109
I20241203 09:35:30 2006642 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.748797  data: 0.001285  max mem: 4109
I20241203 09:35:56 2006642 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.027928  data: 0.000986  max mem: 4109
I20241203 09:36:04 2006642 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.823281  data: 0.000849  max mem: 4109
I20241203 09:36:04 2006642 dinov2 helpers.py:130]  Total time: 0:42:16 (4.000008 s / it)
I20241203 09:36:04 2006642 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 09:36:04 2006642 dinov2 utils.py:142] Labels shape: (162127,)
submitit ERROR (2024-12-03 09:36:04,191) - Submitted job triggered an exception
E20241203 09:36:04 2006642 submitit submission.py:68] Submitted job triggered an exception
