submitit INFO (2024-12-03 10:27:42,188) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-03 10:27:42,188) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:49 2030088 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:49 2030088 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:49 2030088 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:49 2030088 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:49 2030088 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:18 2030088 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:23 2030088 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:24 2030088 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:31 2030088 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:31 2030088 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:32 2030088 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:32 2030088 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:32 2030088 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:32 2030088 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:32 2030088 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:32 2030088 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:04 2030088 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:04 2030088 dinov2 helpers.py:102]   [  0/634]  eta: 5:32:19    time: 31.450174  data: 12.099624  max mem: 3463
I20241203 10:29:10 2030088 dinov2 helpers.py:102]   [ 10/634]  eta: 0:35:51    time: 3.447234  data: 1.102566  max mem: 4109
I20241203 10:29:22 2030088 dinov2 helpers.py:102]   [ 20/634]  eta: 0:24:09    time: 0.906174  data: 0.001622  max mem: 4109
I20241203 10:29:46 2030088 dinov2 helpers.py:102]   [ 30/634]  eta: 0:23:51    time: 1.777208  data: 0.000422  max mem: 4109
I20241203 10:30:25 2030088 dinov2 helpers.py:102]   [ 40/634]  eta: 0:27:08    time: 3.142532  data: 0.001331  max mem: 4109
I20241203 10:31:04 2030088 dinov2 helpers.py:102]   [ 50/634]  eta: 0:28:58    time: 3.917352  data: 0.001718  max mem: 4109
I20241203 10:31:44 2030088 dinov2 helpers.py:102]   [ 60/634]  eta: 0:30:00    time: 3.945817  data: 0.001530  max mem: 4109
I20241203 10:32:23 2030088 dinov2 helpers.py:102]   [ 70/634]  eta: 0:30:33    time: 3.953020  data: 0.002771  max mem: 4109
I20241203 10:33:03 2030088 dinov2 helpers.py:102]   [ 80/634]  eta: 0:30:50    time: 3.958974  data: 0.002490  max mem: 4109
I20241203 10:33:43 2030088 dinov2 helpers.py:102]   [ 90/634]  eta: 0:30:54    time: 3.969191  data: 0.002138  max mem: 4109
I20241203 10:34:22 2030088 dinov2 helpers.py:102]   [100/634]  eta: 0:30:50    time: 3.973544  data: 0.003629  max mem: 4109
I20241203 10:35:02 2030088 dinov2 helpers.py:102]   [110/634]  eta: 0:30:39    time: 3.973731  data: 0.002454  max mem: 4109
I20241203 10:35:42 2030088 dinov2 helpers.py:102]   [120/634]  eta: 0:30:24    time: 3.973367  data: 0.001835  max mem: 4109
I20241203 10:36:22 2030088 dinov2 helpers.py:102]   [130/634]  eta: 0:30:05    time: 3.973098  data: 0.002864  max mem: 4109
I20241203 10:37:01 2030088 dinov2 helpers.py:102]   [140/634]  eta: 0:29:43    time: 3.973623  data: 0.005199  max mem: 4109
I20241203 10:37:41 2030088 dinov2 helpers.py:102]   [150/634]  eta: 0:29:18    time: 3.973294  data: 0.004363  max mem: 4109
I20241203 10:38:21 2030088 dinov2 helpers.py:102]   [160/634]  eta: 0:28:52    time: 3.973031  data: 0.001906  max mem: 4109
I20241203 10:39:00 2030088 dinov2 helpers.py:102]   [170/634]  eta: 0:28:24    time: 3.972010  data: 0.001994  max mem: 4109
I20241203 10:39:40 2030088 dinov2 helpers.py:102]   [180/634]  eta: 0:27:55    time: 3.971635  data: 0.002708  max mem: 4109
I20241203 10:40:20 2030088 dinov2 helpers.py:102]   [190/634]  eta: 0:27:24    time: 3.972876  data: 0.002635  max mem: 4109
I20241203 10:41:00 2030088 dinov2 helpers.py:102]   [200/634]  eta: 0:26:53    time: 3.973317  data: 0.001936  max mem: 4109
I20241203 10:41:39 2030088 dinov2 helpers.py:102]   [210/634]  eta: 0:26:21    time: 3.973048  data: 0.004191  max mem: 4109
I20241203 10:42:19 2030088 dinov2 helpers.py:102]   [220/634]  eta: 0:25:48    time: 3.968940  data: 0.003303  max mem: 4109
I20241203 10:42:59 2030088 dinov2 helpers.py:102]   [230/634]  eta: 0:25:15    time: 3.966202  data: 0.001173  max mem: 4109
I20241203 10:43:38 2030088 dinov2 helpers.py:102]   [240/634]  eta: 0:24:41    time: 3.968992  data: 0.001687  max mem: 4109
I20241203 10:44:18 2030088 dinov2 helpers.py:102]   [250/634]  eta: 0:24:06    time: 3.971914  data: 0.001638  max mem: 4109
I20241203 10:44:58 2030088 dinov2 helpers.py:102]   [260/634]  eta: 0:23:32    time: 3.973159  data: 0.001208  max mem: 4109
I20241203 10:45:38 2030088 dinov2 helpers.py:102]   [270/634]  eta: 0:22:57    time: 3.974637  data: 0.002249  max mem: 4109
I20241203 10:46:17 2030088 dinov2 helpers.py:102]   [280/634]  eta: 0:22:21    time: 3.976933  data: 0.003720  max mem: 4109
I20241203 10:46:57 2030088 dinov2 helpers.py:102]   [290/634]  eta: 0:21:46    time: 3.978973  data: 0.004087  max mem: 4109
I20241203 10:47:37 2030088 dinov2 helpers.py:102]   [300/634]  eta: 0:21:10    time: 3.978022  data: 0.002589  max mem: 4109
I20241203 10:48:17 2030088 dinov2 helpers.py:102]   [310/634]  eta: 0:20:33    time: 3.981660  data: 0.001069  max mem: 4109
I20241203 10:48:57 2030088 dinov2 helpers.py:102]   [320/634]  eta: 0:19:57    time: 3.986162  data: 0.001200  max mem: 4109
I20241203 10:49:36 2030088 dinov2 helpers.py:102]   [330/634]  eta: 0:19:20    time: 3.980665  data: 0.001080  max mem: 4109
I20241203 10:50:16 2030088 dinov2 helpers.py:102]   [340/634]  eta: 0:18:44    time: 3.979858  data: 0.001088  max mem: 4109
I20241203 10:50:56 2030088 dinov2 helpers.py:102]   [350/634]  eta: 0:18:07    time: 3.983400  data: 0.001190  max mem: 4109
I20241203 10:51:36 2030088 dinov2 helpers.py:102]   [360/634]  eta: 0:17:30    time: 3.983298  data: 0.000895  max mem: 4109
I20241203 10:52:16 2030088 dinov2 helpers.py:102]   [370/634]  eta: 0:16:52    time: 3.980836  data: 0.001596  max mem: 4109
I20241203 10:52:56 2030088 dinov2 helpers.py:102]   [380/634]  eta: 0:16:15    time: 3.981822  data: 0.002254  max mem: 4109
I20241203 10:53:35 2030088 dinov2 helpers.py:102]   [390/634]  eta: 0:15:37    time: 3.982632  data: 0.002535  max mem: 4109
I20241203 10:54:15 2030088 dinov2 helpers.py:102]   [400/634]  eta: 0:15:00    time: 3.983567  data: 0.002469  max mem: 4109
I20241203 10:54:55 2030088 dinov2 helpers.py:102]   [410/634]  eta: 0:14:22    time: 3.983507  data: 0.001467  max mem: 4109
I20241203 10:55:35 2030088 dinov2 helpers.py:102]   [420/634]  eta: 0:13:44    time: 3.980843  data: 0.001533  max mem: 4109
I20241203 10:56:15 2030088 dinov2 helpers.py:102]   [430/634]  eta: 0:13:06    time: 3.984488  data: 0.001917  max mem: 4109
I20241203 10:56:55 2030088 dinov2 helpers.py:102]   [440/634]  eta: 0:12:28    time: 3.984449  data: 0.001329  max mem: 4109
I20241203 10:57:34 2030088 dinov2 helpers.py:102]   [450/634]  eta: 0:11:50    time: 3.979877  data: 0.000850  max mem: 4109
I20241203 10:58:14 2030088 dinov2 helpers.py:102]   [460/634]  eta: 0:11:12    time: 3.978925  data: 0.001349  max mem: 4109
I20241203 10:58:54 2030088 dinov2 helpers.py:102]   [470/634]  eta: 0:10:34    time: 3.980752  data: 0.001736  max mem: 4109
I20241203 10:59:34 2030088 dinov2 helpers.py:102]   [480/634]  eta: 0:09:55    time: 3.978175  data: 0.001417  max mem: 4109
I20241203 11:00:14 2030088 dinov2 helpers.py:102]   [490/634]  eta: 0:09:17    time: 3.978200  data: 0.001443  max mem: 4109
I20241203 11:00:53 2030088 dinov2 helpers.py:102]   [500/634]  eta: 0:08:39    time: 3.984461  data: 0.003372  max mem: 4109
I20241203 11:01:33 2030088 dinov2 helpers.py:102]   [510/634]  eta: 0:08:00    time: 3.987037  data: 0.003395  max mem: 4109
I20241203 11:02:13 2030088 dinov2 helpers.py:102]   [520/634]  eta: 0:07:22    time: 3.985262  data: 0.001643  max mem: 4109
I20241203 11:02:53 2030088 dinov2 helpers.py:102]   [530/634]  eta: 0:06:43    time: 3.981810  data: 0.001243  max mem: 4109
I20241203 11:03:33 2030088 dinov2 helpers.py:102]   [540/634]  eta: 0:06:04    time: 3.977311  data: 0.000928  max mem: 4109
I20241203 11:04:12 2030088 dinov2 helpers.py:102]   [550/634]  eta: 0:05:26    time: 3.977226  data: 0.002142  max mem: 4109
I20241203 11:04:52 2030088 dinov2 helpers.py:102]   [560/634]  eta: 0:04:47    time: 3.981860  data: 0.002181  max mem: 4109
I20241203 11:05:32 2030088 dinov2 helpers.py:102]   [570/634]  eta: 0:04:08    time: 3.987178  data: 0.000992  max mem: 4109
I20241203 11:06:12 2030088 dinov2 helpers.py:102]   [580/634]  eta: 0:03:30    time: 3.987013  data: 0.002101  max mem: 4109
I20241203 11:06:52 2030088 dinov2 helpers.py:102]   [590/634]  eta: 0:02:51    time: 3.984535  data: 0.003468  max mem: 4109
I20241203 11:07:32 2030088 dinov2 helpers.py:102]   [600/634]  eta: 0:02:12    time: 3.984507  data: 0.003032  max mem: 4109
I20241203 11:08:12 2030088 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.984133  data: 0.001936  max mem: 4109
I20241203 11:08:51 2030088 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.986853  data: 0.001527  max mem: 4109
I20241203 11:09:31 2030088 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.986293  data: 0.001642  max mem: 4109
I20241203 11:09:51 2030088 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.360571  data: 0.001383  max mem: 4109
I20241203 11:09:51 2030088 dinov2 helpers.py:130]  Total time: 0:41:18 (3.909793 s / it)
I20241203 11:09:51 2030088 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:09:51 2030088 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:09:52 2030088 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:09:52 2030088 dinov2 loaders.py:151] sampler: distributed
I20241203 11:09:52 2030088 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:09:52 2030088 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:09:52,568) - Submitted job triggered an exception
E20241203 11:09:52 2030088 submitit submission.py:68] Submitted job triggered an exception
