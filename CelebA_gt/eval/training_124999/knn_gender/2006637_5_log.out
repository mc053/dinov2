submitit INFO (2024-12-03 08:52:42,228) - Starting with JobEnvironment(job_id=2006637, hostname=tars, local_rank=5(8), node=0(1), global_rank=5(8))
submitit INFO (2024-12-03 08:52:42,228) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2006637_submitted.pkl
I20241203 08:52:50 2006643 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 08:52:50 2006643 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 08:52:50 2006643 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 08:52:50 2006643 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 08:52:50 2006643 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 08:53:18 2006643 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 08:53:23 2006643 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 08:53:24 2006643 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 08:53:32 2006643 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 08:53:32 2006643 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 08:53:35 2006643 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 08:53:35 2006643 dinov2 knn.py:260] Extracting features for train set...
I20241203 08:53:35 2006643 dinov2 loaders.py:151] sampler: distributed
I20241203 08:53:35 2006643 dinov2 loaders.py:210] using PyTorch data loader
W20241203 08:53:35 2006643 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 08:53:35 2006643 dinov2 loaders.py:223] # of batches: 634
I20241203 08:54:12 2006643 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 08:54:12 2006643 dinov2 helpers.py:102]   [  0/634]  eta: 6:37:35    time: 37.627125  data: 15.110686  max mem: 3463
I20241203 08:54:25 2006643 dinov2 helpers.py:102]   [ 10/634]  eta: 0:47:12    time: 4.539339  data: 1.374440  max mem: 4109
I20241203 08:54:55 2006643 dinov2 helpers.py:102]   [ 20/634]  eta: 0:39:10    time: 2.138731  data: 0.000730  max mem: 4109
I20241203 08:55:35 2006643 dinov2 helpers.py:102]   [ 30/634]  eta: 0:38:55    time: 3.495932  data: 0.001042  max mem: 4109
I20241203 08:56:14 2006643 dinov2 helpers.py:102]   [ 40/634]  eta: 0:38:29    time: 3.950534  data: 0.001189  max mem: 4109
I20241203 08:56:54 2006643 dinov2 helpers.py:102]   [ 50/634]  eta: 0:37:58    time: 3.957024  data: 0.000995  max mem: 4109
I20241203 08:57:33 2006643 dinov2 helpers.py:102]   [ 60/634]  eta: 0:37:25    time: 3.963622  data: 0.000915  max mem: 4109
I20241203 08:58:13 2006643 dinov2 helpers.py:102]   [ 70/634]  eta: 0:36:51    time: 3.971361  data: 0.000840  max mem: 4109
I20241203 08:58:53 2006643 dinov2 helpers.py:102]   [ 80/634]  eta: 0:36:16    time: 3.973506  data: 0.000865  max mem: 4109
I20241203 08:59:33 2006643 dinov2 helpers.py:102]   [ 90/634]  eta: 0:35:39    time: 3.973644  data: 0.001230  max mem: 4109
I20241203 09:00:12 2006643 dinov2 helpers.py:102]   [100/634]  eta: 0:35:02    time: 3.973584  data: 0.001429  max mem: 4109
I20241203 09:00:52 2006643 dinov2 helpers.py:102]   [110/634]  eta: 0:34:24    time: 3.973266  data: 0.000896  max mem: 4109
I20241203 09:01:32 2006643 dinov2 helpers.py:102]   [120/634]  eta: 0:33:46    time: 3.970431  data: 0.000702  max mem: 4109
I20241203 09:02:11 2006643 dinov2 helpers.py:102]   [130/634]  eta: 0:33:08    time: 3.969515  data: 0.001019  max mem: 4109
I20241203 09:02:51 2006643 dinov2 helpers.py:102]   [140/634]  eta: 0:32:29    time: 3.969259  data: 0.001244  max mem: 4109
I20241203 09:03:31 2006643 dinov2 helpers.py:102]   [150/634]  eta: 0:31:50    time: 3.968311  data: 0.001051  max mem: 4109
I20241203 09:04:10 2006643 dinov2 helpers.py:102]   [160/634]  eta: 0:31:11    time: 3.967601  data: 0.000812  max mem: 4109
I20241203 09:04:50 2006643 dinov2 helpers.py:102]   [170/634]  eta: 0:30:32    time: 3.969208  data: 0.001578  max mem: 4109
I20241203 09:05:30 2006643 dinov2 helpers.py:102]   [180/634]  eta: 0:29:53    time: 3.972593  data: 0.001646  max mem: 4109
I20241203 09:06:10 2006643 dinov2 helpers.py:102]   [190/634]  eta: 0:29:14    time: 3.973114  data: 0.000978  max mem: 4109
I20241203 09:06:49 2006643 dinov2 helpers.py:102]   [200/634]  eta: 0:28:35    time: 3.973951  data: 0.001037  max mem: 4109
I20241203 09:07:29 2006643 dinov2 helpers.py:102]   [210/634]  eta: 0:27:56    time: 3.973537  data: 0.000938  max mem: 4109
I20241203 09:08:09 2006643 dinov2 helpers.py:102]   [220/634]  eta: 0:27:17    time: 3.972931  data: 0.000828  max mem: 4109
I20241203 09:08:49 2006643 dinov2 helpers.py:102]   [230/634]  eta: 0:26:38    time: 3.973132  data: 0.001659  max mem: 4109
I20241203 09:09:28 2006643 dinov2 helpers.py:102]   [240/634]  eta: 0:25:59    time: 3.973230  data: 0.003731  max mem: 4109
I20241203 09:10:08 2006643 dinov2 helpers.py:102]   [250/634]  eta: 0:25:19    time: 3.973328  data: 0.002989  max mem: 4109
I20241203 09:10:48 2006643 dinov2 helpers.py:102]   [260/634]  eta: 0:24:40    time: 3.974306  data: 0.001858  max mem: 4109
I20241203 09:11:28 2006643 dinov2 helpers.py:102]   [270/634]  eta: 0:24:01    time: 3.974630  data: 0.001654  max mem: 4109
I20241203 09:12:07 2006643 dinov2 helpers.py:102]   [280/634]  eta: 0:23:21    time: 3.974062  data: 0.001223  max mem: 4109
I20241203 09:12:47 2006643 dinov2 helpers.py:102]   [290/634]  eta: 0:22:42    time: 3.975115  data: 0.001801  max mem: 4109
I20241203 09:13:27 2006643 dinov2 helpers.py:102]   [300/634]  eta: 0:22:02    time: 3.975163  data: 0.001410  max mem: 4109
I20241203 09:14:07 2006643 dinov2 helpers.py:102]   [310/634]  eta: 0:21:23    time: 3.974207  data: 0.001326  max mem: 4109
I20241203 09:14:46 2006643 dinov2 helpers.py:102]   [320/634]  eta: 0:20:43    time: 3.976107  data: 0.001292  max mem: 4109
I20241203 09:15:26 2006643 dinov2 helpers.py:102]   [330/634]  eta: 0:20:04    time: 3.976023  data: 0.000959  max mem: 4109
I20241203 09:16:06 2006643 dinov2 helpers.py:102]   [340/634]  eta: 0:19:24    time: 3.974061  data: 0.000917  max mem: 4109
I20241203 09:16:46 2006643 dinov2 helpers.py:102]   [350/634]  eta: 0:18:45    time: 3.973965  data: 0.000742  max mem: 4109
I20241203 09:17:25 2006643 dinov2 helpers.py:102]   [360/634]  eta: 0:18:05    time: 3.973884  data: 0.000501  max mem: 4109
I20241203 09:18:05 2006643 dinov2 helpers.py:102]   [370/634]  eta: 0:17:26    time: 3.973739  data: 0.000567  max mem: 4109
I20241203 09:18:45 2006643 dinov2 helpers.py:102]   [380/634]  eta: 0:16:46    time: 3.972390  data: 0.000975  max mem: 4109
I20241203 09:19:24 2006643 dinov2 helpers.py:102]   [390/634]  eta: 0:16:07    time: 3.972253  data: 0.000963  max mem: 4109
I20241203 09:20:04 2006643 dinov2 helpers.py:102]   [400/634]  eta: 0:15:27    time: 3.973439  data: 0.000630  max mem: 4109
I20241203 09:20:44 2006643 dinov2 helpers.py:102]   [410/634]  eta: 0:14:47    time: 3.973715  data: 0.000913  max mem: 4109
I20241203 09:21:24 2006643 dinov2 helpers.py:102]   [420/634]  eta: 0:14:08    time: 3.973915  data: 0.001017  max mem: 4109
I20241203 09:22:03 2006643 dinov2 helpers.py:102]   [430/634]  eta: 0:13:28    time: 3.973870  data: 0.000845  max mem: 4109
I20241203 09:22:43 2006643 dinov2 helpers.py:102]   [440/634]  eta: 0:12:49    time: 3.973804  data: 0.000836  max mem: 4109
I20241203 09:23:23 2006643 dinov2 helpers.py:102]   [450/634]  eta: 0:12:09    time: 3.973916  data: 0.001061  max mem: 4109
I20241203 09:24:03 2006643 dinov2 helpers.py:102]   [460/634]  eta: 0:11:29    time: 3.974385  data: 0.000972  max mem: 4109
I20241203 09:24:42 2006643 dinov2 helpers.py:102]   [470/634]  eta: 0:10:50    time: 3.975931  data: 0.000923  max mem: 4109
I20241203 09:25:22 2006643 dinov2 helpers.py:102]   [480/634]  eta: 0:10:10    time: 3.975735  data: 0.001445  max mem: 4109
I20241203 09:26:02 2006643 dinov2 helpers.py:102]   [490/634]  eta: 0:09:31    time: 3.973577  data: 0.001201  max mem: 4109
I20241203 09:26:42 2006643 dinov2 helpers.py:102]   [500/634]  eta: 0:08:51    time: 3.972886  data: 0.000800  max mem: 4109
I20241203 09:27:21 2006643 dinov2 helpers.py:102]   [510/634]  eta: 0:08:11    time: 3.973639  data: 0.001523  max mem: 4109
I20241203 09:28:01 2006643 dinov2 helpers.py:102]   [520/634]  eta: 0:07:32    time: 3.974073  data: 0.001527  max mem: 4109
I20241203 09:28:41 2006643 dinov2 helpers.py:102]   [530/634]  eta: 0:06:52    time: 3.973853  data: 0.000935  max mem: 4109
I20241203 09:29:21 2006643 dinov2 helpers.py:102]   [540/634]  eta: 0:06:12    time: 3.973877  data: 0.001337  max mem: 4109
I20241203 09:30:00 2006643 dinov2 helpers.py:102]   [550/634]  eta: 0:05:33    time: 3.974496  data: 0.001924  max mem: 4109
I20241203 09:30:40 2006643 dinov2 helpers.py:102]   [560/634]  eta: 0:04:53    time: 3.974094  data: 0.001305  max mem: 4109
I20241203 09:31:20 2006643 dinov2 helpers.py:102]   [570/634]  eta: 0:04:13    time: 3.973538  data: 0.000833  max mem: 4109
I20241203 09:32:00 2006643 dinov2 helpers.py:102]   [580/634]  eta: 0:03:34    time: 3.973751  data: 0.001157  max mem: 4109
I20241203 09:32:39 2006643 dinov2 helpers.py:102]   [590/634]  eta: 0:02:54    time: 3.973502  data: 0.000945  max mem: 4109
I20241203 09:33:19 2006643 dinov2 helpers.py:102]   [600/634]  eta: 0:02:14    time: 3.973444  data: 0.000811  max mem: 4109
I20241203 09:33:59 2006643 dinov2 helpers.py:102]   [610/634]  eta: 0:01:35    time: 3.973145  data: 0.001297  max mem: 4109
I20241203 09:34:38 2006643 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.972888  data: 0.001346  max mem: 4109
I20241203 09:35:18 2006643 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.953950  data: 0.000774  max mem: 4109
I20241203 09:35:32 2006643 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.085832  data: 0.000606  max mem: 4109
I20241203 09:35:33 2006643 dinov2 helpers.py:130]  Total time: 0:41:57 (3.971394 s / it)
I20241203 09:35:33 2006643 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 09:35:33 2006643 dinov2 utils.py:142] Labels shape: (162127,)
submitit ERROR (2024-12-03 09:35:33,782) - Submitted job triggered an exception
E20241203 09:35:33 2006643 submitit submission.py:68] Submitted job triggered an exception
