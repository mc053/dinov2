submitit INFO (2024-12-03 10:27:42,208) - Starting with JobEnvironment(job_id=2030087, hostname=tars, local_rank=3(8), node=0(1), global_rank=3(8))
submitit INFO (2024-12-03 10:27:42,208) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2030087_submitted.pkl
I20241203 10:27:50 2030091 dinov2 config.py:59] git:
  sha: f012955340146e72b47b4b757ccbd120c3c06fa9, status: has uncommitted changes, branch: main

I20241203 10:27:50 2030091 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241203 10:27:50 2030091 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241203 10:27:50 2030091 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241203 10:27:50 2030091 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241203 10:28:24 2030091 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241203 10:28:29 2030091 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241203 10:28:29 2030091 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241203 10:28:46 2030091 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241203 10:28:46 2030091 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241203 10:28:51 2030091 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241203 10:28:51 2030091 dinov2 knn.py:260] Extracting features for train set...
I20241203 10:28:51 2030091 dinov2 loaders.py:151] sampler: distributed
I20241203 10:28:51 2030091 dinov2 loaders.py:210] using PyTorch data loader
W20241203 10:28:51 2030091 py.warnings warnings.py:109] /opt/miniconda3/envs/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241203 10:28:51 2030091 dinov2 loaders.py:223] # of batches: 634
I20241203 10:29:42 2030091 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241203 10:29:42 2030091 dinov2 helpers.py:102]   [  0/634]  eta: 9:01:55    time: 51.285805  data: 13.101794  max mem: 3463
I20241203 10:30:14 2030091 dinov2 helpers.py:102]   [ 10/634]  eta: 1:18:44    time: 7.571315  data: 1.193448  max mem: 4109
I20241203 10:30:54 2030091 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:44    time: 3.566221  data: 0.001710  max mem: 4109
I20241203 10:31:33 2030091 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:39    time: 3.944110  data: 0.000984  max mem: 4109
I20241203 10:32:13 2030091 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:42    time: 3.956170  data: 0.001742  max mem: 4109
I20241203 10:32:52 2030091 dinov2 helpers.py:102]   [ 50/634]  eta: 0:46:03    time: 3.956890  data: 0.001801  max mem: 4109
I20241203 10:33:32 2030091 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:04    time: 3.965281  data: 0.000929  max mem: 4109
I20241203 10:34:12 2030091 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:28    time: 3.973455  data: 0.001124  max mem: 4109
I20241203 10:34:51 2030091 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:05    time: 3.973473  data: 0.001068  max mem: 4109
I20241203 10:35:31 2030091 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:52    time: 3.973415  data: 0.000534  max mem: 4109
I20241203 10:36:11 2030091 dinov2 helpers.py:102]   [100/634]  eta: 0:38:46    time: 3.973359  data: 0.000966  max mem: 4109
I20241203 10:36:51 2030091 dinov2 helpers.py:102]   [110/634]  eta: 0:37:44    time: 3.973347  data: 0.001279  max mem: 4109
I20241203 10:37:30 2030091 dinov2 helpers.py:102]   [120/634]  eta: 0:36:46    time: 3.973297  data: 0.001085  max mem: 4109
I20241203 10:38:10 2030091 dinov2 helpers.py:102]   [130/634]  eta: 0:35:51    time: 3.973266  data: 0.000760  max mem: 4109
I20241203 10:38:50 2030091 dinov2 helpers.py:102]   [140/634]  eta: 0:34:58    time: 3.972102  data: 0.000660  max mem: 4109
I20241203 10:39:30 2030091 dinov2 helpers.py:102]   [150/634]  eta: 0:34:06    time: 3.971845  data: 0.000771  max mem: 4109
I20241203 10:40:09 2030091 dinov2 helpers.py:102]   [160/634]  eta: 0:33:17    time: 3.972781  data: 0.001083  max mem: 4109
I20241203 10:40:49 2030091 dinov2 helpers.py:102]   [170/634]  eta: 0:32:28    time: 3.973109  data: 0.001074  max mem: 4109
I20241203 10:41:29 2030091 dinov2 helpers.py:102]   [180/634]  eta: 0:31:40    time: 3.972319  data: 0.000913  max mem: 4109
I20241203 10:42:08 2030091 dinov2 helpers.py:102]   [190/634]  eta: 0:30:53    time: 3.971810  data: 0.000808  max mem: 4109
I20241203 10:42:48 2030091 dinov2 helpers.py:102]   [200/634]  eta: 0:30:07    time: 3.972525  data: 0.000934  max mem: 4109
I20241203 10:43:28 2030091 dinov2 helpers.py:102]   [210/634]  eta: 0:29:22    time: 3.972572  data: 0.001026  max mem: 4109
I20241203 10:44:08 2030091 dinov2 helpers.py:102]   [220/634]  eta: 0:28:37    time: 3.972656  data: 0.001227  max mem: 4109
I20241203 10:44:47 2030091 dinov2 helpers.py:102]   [230/634]  eta: 0:27:52    time: 3.973120  data: 0.001433  max mem: 4109
I20241203 10:45:27 2030091 dinov2 helpers.py:102]   [240/634]  eta: 0:27:08    time: 3.974377  data: 0.000917  max mem: 4109
I20241203 10:46:07 2030091 dinov2 helpers.py:102]   [250/634]  eta: 0:26:24    time: 3.975081  data: 0.001340  max mem: 4109
I20241203 10:46:47 2030091 dinov2 helpers.py:102]   [260/634]  eta: 0:25:41    time: 3.977499  data: 0.002019  max mem: 4109
I20241203 10:47:27 2030091 dinov2 helpers.py:102]   [270/634]  eta: 0:24:58    time: 3.982506  data: 0.001318  max mem: 4109
I20241203 10:48:06 2030091 dinov2 helpers.py:102]   [280/634]  eta: 0:24:15    time: 3.982246  data: 0.000766  max mem: 4109
I20241203 10:48:46 2030091 dinov2 helpers.py:102]   [290/634]  eta: 0:23:32    time: 3.981612  data: 0.000770  max mem: 4109
I20241203 10:49:26 2030091 dinov2 helpers.py:102]   [300/634]  eta: 0:22:50    time: 3.984393  data: 0.000655  max mem: 4109
I20241203 10:50:06 2030091 dinov2 helpers.py:102]   [310/634]  eta: 0:22:08    time: 3.984345  data: 0.001480  max mem: 4109
I20241203 10:50:46 2030091 dinov2 helpers.py:102]   [320/634]  eta: 0:21:26    time: 3.983360  data: 0.001652  max mem: 4109
I20241203 10:51:25 2030091 dinov2 helpers.py:102]   [330/634]  eta: 0:20:44    time: 3.979754  data: 0.000989  max mem: 4109
I20241203 10:52:05 2030091 dinov2 helpers.py:102]   [340/634]  eta: 0:20:02    time: 3.981564  data: 0.000987  max mem: 4109
I20241203 10:52:45 2030091 dinov2 helpers.py:102]   [350/634]  eta: 0:19:20    time: 3.982638  data: 0.001115  max mem: 4109
I20241203 10:53:25 2030091 dinov2 helpers.py:102]   [360/634]  eta: 0:18:38    time: 3.979137  data: 0.001957  max mem: 4109
I20241203 10:54:05 2030091 dinov2 helpers.py:102]   [370/634]  eta: 0:17:57    time: 3.978186  data: 0.001811  max mem: 4109
I20241203 10:54:44 2030091 dinov2 helpers.py:102]   [380/634]  eta: 0:17:15    time: 3.979036  data: 0.000720  max mem: 4109
I20241203 10:55:24 2030091 dinov2 helpers.py:102]   [390/634]  eta: 0:16:34    time: 3.980704  data: 0.000853  max mem: 4109
I20241203 10:56:04 2030091 dinov2 helpers.py:102]   [400/634]  eta: 0:15:53    time: 3.982722  data: 0.001204  max mem: 4109
I20241203 10:56:44 2030091 dinov2 helpers.py:102]   [410/634]  eta: 0:15:11    time: 3.986359  data: 0.001683  max mem: 4109
I20241203 10:57:24 2030091 dinov2 helpers.py:102]   [420/634]  eta: 0:14:30    time: 3.987871  data: 0.001559  max mem: 4109
I20241203 10:58:04 2030091 dinov2 helpers.py:102]   [430/634]  eta: 0:13:49    time: 3.986092  data: 0.000960  max mem: 4109
I20241203 10:58:44 2030091 dinov2 helpers.py:102]   [440/634]  eta: 0:13:08    time: 3.984384  data: 0.000878  max mem: 4109
I20241203 10:59:23 2030091 dinov2 helpers.py:102]   [450/634]  eta: 0:12:27    time: 3.987135  data: 0.000956  max mem: 4109
I20241203 11:00:03 2030091 dinov2 helpers.py:102]   [460/634]  eta: 0:11:46    time: 3.988120  data: 0.000843  max mem: 4109
I20241203 11:00:43 2030091 dinov2 helpers.py:102]   [470/634]  eta: 0:11:05    time: 3.988837  data: 0.000550  max mem: 4109
I20241203 11:01:23 2030091 dinov2 helpers.py:102]   [480/634]  eta: 0:10:25    time: 3.989825  data: 0.000685  max mem: 4109
I20241203 11:02:03 2030091 dinov2 helpers.py:102]   [490/634]  eta: 0:09:44    time: 3.989830  data: 0.001052  max mem: 4109
I20241203 11:02:43 2030091 dinov2 helpers.py:102]   [500/634]  eta: 0:09:03    time: 3.989748  data: 0.001087  max mem: 4109
I20241203 11:03:23 2030091 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.990927  data: 0.000809  max mem: 4109
I20241203 11:04:03 2030091 dinov2 helpers.py:102]   [520/634]  eta: 0:07:42    time: 3.991152  data: 0.000670  max mem: 4109
I20241203 11:04:43 2030091 dinov2 helpers.py:102]   [530/634]  eta: 0:07:01    time: 3.985221  data: 0.001103  max mem: 4109
I20241203 11:05:22 2030091 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.983176  data: 0.001198  max mem: 4109
I20241203 11:06:02 2030091 dinov2 helpers.py:102]   [550/634]  eta: 0:05:40    time: 3.988056  data: 0.000700  max mem: 4109
I20241203 11:06:42 2030091 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.989885  data: 0.000931  max mem: 4109
I20241203 11:07:22 2030091 dinov2 helpers.py:102]   [570/634]  eta: 0:04:19    time: 3.986270  data: 0.001092  max mem: 4109
I20241203 11:08:02 2030091 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.984327  data: 0.001053  max mem: 4109
I20241203 11:08:42 2030091 dinov2 helpers.py:102]   [590/634]  eta: 0:02:58    time: 3.988792  data: 0.001520  max mem: 4109
I20241203 11:09:22 2030091 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.991651  data: 0.001319  max mem: 4109
I20241203 11:10:00 2030091 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.926688  data: 0.000959  max mem: 4109
I20241203 11:10:31 2030091 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.450924  data: 0.000894  max mem: 4109
I20241203 11:10:56 2030091 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 2.772884  data: 0.000624  max mem: 4109
I20241203 11:11:03 2030091 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 2.670759  data: 0.000443  max mem: 4109
I20241203 11:11:03 2030091 dinov2 helpers.py:130]  Total time: 0:42:12 (3.994414 s / it)
I20241203 11:11:03 2030091 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241203 11:11:03 2030091 dinov2 utils.py:142] Labels shape: (162127,)
I20241203 11:11:03 2030091 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241203 11:11:03 2030091 dinov2 loaders.py:151] sampler: distributed
I20241203 11:11:03 2030091 dinov2 loaders.py:210] using PyTorch data loader
I20241203 11:11:03 2030091 dinov2 loaders.py:223] # of batches: 78
submitit ERROR (2024-12-03 11:11:03,902) - Submitted job triggered an exception
E20241203 11:11:03 2030091 submitit submission.py:68] Submitted job triggered an exception
