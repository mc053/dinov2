submitit INFO (2024-12-04 10:17:37,610) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-04 10:17:37,610) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:45 2567769 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:45 2567769 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:45 2567769 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:45 2567769 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:45 2567769 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:18 2567769 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:23 2567769 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:24 2567769 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:31 2567769 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:31 2567769 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:40 2567769 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:40 2567769 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:40 2567769 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:40 2567769 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:40 2567769 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:40 2567769 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:27 2567769 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:27 2567769 dinov2 helpers.py:102]   [  0/634]  eta: 8:25:13    time: 47.813057  data: 18.686615  max mem: 3463
I20241204 10:19:59 2567769 dinov2 helpers.py:102]   [ 10/634]  eta: 1:15:24    time: 7.250986  data: 1.702397  max mem: 4109
I20241204 10:20:39 2567769 dinov2 helpers.py:102]   [ 20/634]  eta: 0:57:59    time: 3.559860  data: 0.002318  max mem: 4109
I20241204 10:21:18 2567769 dinov2 helpers.py:102]   [ 30/634]  eta: 0:51:30    time: 3.941985  data: 0.001004  max mem: 4109
I20241204 10:21:58 2567769 dinov2 helpers.py:102]   [ 40/634]  eta: 0:47:53    time: 3.965066  data: 0.001144  max mem: 4109
I20241204 10:22:38 2567769 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:26    time: 3.974152  data: 0.000955  max mem: 4109
I20241204 10:23:17 2567769 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:34    time: 3.978351  data: 0.001093  max mem: 4109
I20241204 10:23:57 2567769 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:03    time: 3.979738  data: 0.000953  max mem: 4109
I20241204 10:24:37 2567769 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:44    time: 3.977837  data: 0.001327  max mem: 4109
I20241204 10:25:17 2567769 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:34    time: 3.978573  data: 0.001745  max mem: 4109
I20241204 10:25:57 2567769 dinov2 helpers.py:102]   [100/634]  eta: 0:38:30    time: 3.977942  data: 0.002615  max mem: 4109
I20241204 10:26:36 2567769 dinov2 helpers.py:102]   [110/634]  eta: 0:37:30    time: 3.975811  data: 0.004625  max mem: 4109
I20241204 10:27:16 2567769 dinov2 helpers.py:102]   [120/634]  eta: 0:36:34    time: 3.978375  data: 0.003236  max mem: 4109
I20241204 10:27:56 2567769 dinov2 helpers.py:102]   [130/634]  eta: 0:35:40    time: 3.978696  data: 0.001978  max mem: 4109
I20241204 10:28:36 2567769 dinov2 helpers.py:102]   [140/634]  eta: 0:34:48    time: 3.978075  data: 0.001934  max mem: 4109
I20241204 10:29:16 2567769 dinov2 helpers.py:102]   [150/634]  eta: 0:33:58    time: 3.980713  data: 0.000912  max mem: 4109
I20241204 10:29:55 2567769 dinov2 helpers.py:102]   [160/634]  eta: 0:33:09    time: 3.980402  data: 0.001331  max mem: 4109
I20241204 10:30:35 2567769 dinov2 helpers.py:102]   [170/634]  eta: 0:32:21    time: 3.978578  data: 0.003004  max mem: 4109
I20241204 10:31:15 2567769 dinov2 helpers.py:102]   [180/634]  eta: 0:31:34    time: 3.976753  data: 0.003246  max mem: 4109
I20241204 10:31:55 2567769 dinov2 helpers.py:102]   [190/634]  eta: 0:30:48    time: 3.976914  data: 0.001469  max mem: 4109
I20241204 10:32:34 2567769 dinov2 helpers.py:102]   [200/634]  eta: 0:30:02    time: 3.979289  data: 0.000988  max mem: 4109
I20241204 10:33:14 2567769 dinov2 helpers.py:102]   [210/634]  eta: 0:29:17    time: 3.979281  data: 0.002246  max mem: 4109
I20241204 10:33:54 2567769 dinov2 helpers.py:102]   [220/634]  eta: 0:28:32    time: 3.977217  data: 0.002753  max mem: 4109
I20241204 10:34:34 2567769 dinov2 helpers.py:102]   [230/634]  eta: 0:27:48    time: 3.976876  data: 0.003179  max mem: 4109
I20241204 10:35:14 2567769 dinov2 helpers.py:102]   [240/634]  eta: 0:27:04    time: 3.978144  data: 0.002751  max mem: 4109
I20241204 10:35:53 2567769 dinov2 helpers.py:102]   [250/634]  eta: 0:26:21    time: 3.975516  data: 0.002228  max mem: 4109
I20241204 10:36:33 2567769 dinov2 helpers.py:102]   [260/634]  eta: 0:25:38    time: 3.976672  data: 0.001967  max mem: 4109
I20241204 10:37:13 2567769 dinov2 helpers.py:102]   [270/634]  eta: 0:24:55    time: 3.976846  data: 0.001863  max mem: 4109
I20241204 10:37:53 2567769 dinov2 helpers.py:102]   [280/634]  eta: 0:24:12    time: 3.978596  data: 0.002110  max mem: 4109
I20241204 10:38:32 2567769 dinov2 helpers.py:102]   [290/634]  eta: 0:23:30    time: 3.981442  data: 0.001405  max mem: 4109
I20241204 10:39:12 2567769 dinov2 helpers.py:102]   [300/634]  eta: 0:22:47    time: 3.982509  data: 0.000988  max mem: 4109
I20241204 10:39:52 2567769 dinov2 helpers.py:102]   [310/634]  eta: 0:22:05    time: 3.979399  data: 0.000964  max mem: 4109
I20241204 10:40:32 2567769 dinov2 helpers.py:102]   [320/634]  eta: 0:21:23    time: 3.975665  data: 0.001011  max mem: 4109
I20241204 10:41:12 2567769 dinov2 helpers.py:102]   [330/634]  eta: 0:20:41    time: 3.980659  data: 0.000936  max mem: 4109
I20241204 10:41:51 2567769 dinov2 helpers.py:102]   [340/634]  eta: 0:20:00    time: 3.982440  data: 0.001117  max mem: 4109
I20241204 10:42:31 2567769 dinov2 helpers.py:102]   [350/634]  eta: 0:19:18    time: 3.981337  data: 0.000881  max mem: 4109
I20241204 10:43:11 2567769 dinov2 helpers.py:102]   [360/634]  eta: 0:18:36    time: 3.981410  data: 0.000894  max mem: 4109
I20241204 10:43:51 2567769 dinov2 helpers.py:102]   [370/634]  eta: 0:17:55    time: 3.982348  data: 0.001116  max mem: 4109
I20241204 10:44:31 2567769 dinov2 helpers.py:102]   [380/634]  eta: 0:17:14    time: 3.981490  data: 0.003838  max mem: 4109
I20241204 10:45:11 2567769 dinov2 helpers.py:102]   [390/634]  eta: 0:16:32    time: 3.982482  data: 0.004020  max mem: 4109
I20241204 10:45:50 2567769 dinov2 helpers.py:102]   [400/634]  eta: 0:15:51    time: 3.986205  data: 0.001395  max mem: 4109
I20241204 10:46:30 2567769 dinov2 helpers.py:102]   [410/634]  eta: 0:15:10    time: 3.985226  data: 0.002225  max mem: 4109
I20241204 10:47:10 2567769 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.985938  data: 0.002518  max mem: 4109
I20241204 10:47:50 2567769 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.983136  data: 0.001592  max mem: 4109
I20241204 10:48:30 2567769 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.978581  data: 0.001194  max mem: 4109
I20241204 10:49:10 2567769 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.978349  data: 0.001006  max mem: 4109
I20241204 10:49:49 2567769 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.977507  data: 0.000955  max mem: 4109
I20241204 10:50:29 2567769 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.980445  data: 0.001152  max mem: 4109
I20241204 10:51:09 2567769 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.983351  data: 0.001056  max mem: 4109
I20241204 10:51:49 2567769 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.983355  data: 0.000644  max mem: 4109
I20241204 10:52:29 2567769 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.985104  data: 0.001282  max mem: 4109
I20241204 10:53:09 2567769 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.985965  data: 0.001519  max mem: 4109
I20241204 10:53:48 2567769 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.985743  data: 0.001015  max mem: 4109
I20241204 10:54:28 2567769 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.986795  data: 0.000746  max mem: 4109
I20241204 10:55:08 2567769 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.987840  data: 0.000943  max mem: 4109
I20241204 10:55:48 2567769 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.987814  data: 0.001124  max mem: 4109
I20241204 10:56:28 2567769 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.987991  data: 0.000844  max mem: 4109
I20241204 10:57:08 2567769 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.990534  data: 0.000788  max mem: 4109
I20241204 10:57:48 2567769 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.987687  data: 0.000743  max mem: 4109
I20241204 10:58:27 2567769 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.982430  data: 0.000940  max mem: 4109
I20241204 10:59:07 2567769 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.982388  data: 0.002279  max mem: 4109
I20241204 10:59:47 2567769 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.981396  data: 0.002092  max mem: 4109
I20241204 11:00:27 2567769 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.979600  data: 0.001040  max mem: 4109
I20241204 11:01:04 2567769 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.862672  data: 0.000931  max mem: 4109
I20241204 11:01:19 2567769 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 3.997010  data: 0.000641  max mem: 4109
I20241204 11:01:20 2567769 dinov2 helpers.py:130]  Total time: 0:42:40 (4.038077 s / it)
I20241204 11:01:20 2567769 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:01:20 2567769 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:20 2567769 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:20 2567769 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:20 2567769 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:20 2567769 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:20 2567769 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:36 2567769 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:19:34    time: 15.055059  data: 11.204338  max mem: 4109
I20241204 11:02:16 2567769 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:42    time: 5.029595  data: 1.025900  max mem: 4109
I20241204 11:02:56 2567769 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:23    time: 4.014062  data: 0.006234  max mem: 4109
I20241204 11:03:36 2567769 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:29    time: 3.994848  data: 0.005628  max mem: 4109
I20241204 11:04:16 2567769 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:42    time: 3.992849  data: 0.005895  max mem: 4109
I20241204 11:04:56 2567769 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:58    time: 3.993712  data: 0.006840  max mem: 4109
I20241204 11:05:36 2567769 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:15    time: 3.988939  data: 0.010153  max mem: 4109
I20241204 11:06:07 2567769 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:32    time: 3.537823  data: 0.008493  max mem: 4109
I20241204 11:06:16 2567769 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.616139  data: 0.003340  max mem: 4109
I20241204 11:06:16 2567769 dinov2 helpers.py:130] Test: Total time: 0:04:55 (3.786798 s / it)
I20241204 11:06:16 2567769 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:17 2567769 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:17 2567769 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:17 2567769 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:17 2567769 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:17,413) - Job completed successfully
I20241204 11:06:17 2567769 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:17,415) - Exiting after successful completion
I20241204 11:06:17 2567769 submitit submission.py:61] Exiting after successful completion
