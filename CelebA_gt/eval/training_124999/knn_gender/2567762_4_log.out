submitit INFO (2024-12-04 10:17:37,549) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=4(8), node=0(1), global_rank=4(8))
submitit INFO (2024-12-04 10:17:37,549) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567767 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567767 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567767 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567767 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567767 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:19 2567767 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:23 2567767 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:23 2567767 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:31 2567767 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:31 2567767 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:39 2567767 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:39 2567767 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:39 2567767 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:39 2567767 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:39 2567767 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:39 2567767 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:27 2567767 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:27 2567767 dinov2 helpers.py:102]   [  0/634]  eta: 8:19:01    time: 47.227104  data: 16.220337  max mem: 3463
I20241204 10:19:59 2567767 dinov2 helpers.py:102]   [ 10/634]  eta: 1:14:50    time: 7.196226  data: 1.480084  max mem: 4109
I20241204 10:20:38 2567767 dinov2 helpers.py:102]   [ 20/634]  eta: 0:57:41    time: 3.558905  data: 0.003591  max mem: 4109
I20241204 10:21:18 2567767 dinov2 helpers.py:102]   [ 30/634]  eta: 0:51:18    time: 3.942034  data: 0.001619  max mem: 4109
I20241204 10:21:57 2567767 dinov2 helpers.py:102]   [ 40/634]  eta: 0:47:44    time: 3.966266  data: 0.001584  max mem: 4109
I20241204 10:22:37 2567767 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:19    time: 3.974981  data: 0.000856  max mem: 4109
I20241204 10:23:17 2567767 dinov2 helpers.py:102]   [ 60/634]  eta: 0:43:29    time: 3.979957  data: 0.000832  max mem: 4109
I20241204 10:23:57 2567767 dinov2 helpers.py:102]   [ 70/634]  eta: 0:41:59    time: 3.984032  data: 0.002452  max mem: 4109
I20241204 10:24:37 2567767 dinov2 helpers.py:102]   [ 80/634]  eta: 0:40:41    time: 3.984105  data: 0.002449  max mem: 4109
I20241204 10:25:16 2567767 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:32    time: 3.981560  data: 0.001270  max mem: 4109
I20241204 10:25:56 2567767 dinov2 helpers.py:102]   [100/634]  eta: 0:38:28    time: 3.981411  data: 0.001105  max mem: 4109
I20241204 10:26:36 2567767 dinov2 helpers.py:102]   [110/634]  eta: 0:37:29    time: 3.981303  data: 0.000878  max mem: 4109
I20241204 10:27:16 2567767 dinov2 helpers.py:102]   [120/634]  eta: 0:36:32    time: 3.978802  data: 0.000953  max mem: 4109
I20241204 10:27:56 2567767 dinov2 helpers.py:102]   [130/634]  eta: 0:35:39    time: 3.978699  data: 0.001043  max mem: 4109
I20241204 10:28:35 2567767 dinov2 helpers.py:102]   [140/634]  eta: 0:34:47    time: 3.983236  data: 0.002221  max mem: 4109
I20241204 10:29:15 2567767 dinov2 helpers.py:102]   [150/634]  eta: 0:33:57    time: 3.986969  data: 0.002011  max mem: 4109
I20241204 10:29:55 2567767 dinov2 helpers.py:102]   [160/634]  eta: 0:33:08    time: 3.981369  data: 0.000734  max mem: 4109
I20241204 10:30:35 2567767 dinov2 helpers.py:102]   [170/634]  eta: 0:32:20    time: 3.974998  data: 0.000764  max mem: 4109
I20241204 10:31:15 2567767 dinov2 helpers.py:102]   [180/634]  eta: 0:31:34    time: 3.980456  data: 0.000896  max mem: 4109
I20241204 10:31:54 2567767 dinov2 helpers.py:102]   [190/634]  eta: 0:30:47    time: 3.984137  data: 0.001368  max mem: 4109
I20241204 10:32:34 2567767 dinov2 helpers.py:102]   [200/634]  eta: 0:30:02    time: 3.981400  data: 0.001450  max mem: 4109
I20241204 10:33:14 2567767 dinov2 helpers.py:102]   [210/634]  eta: 0:29:17    time: 3.980492  data: 0.001085  max mem: 4109
I20241204 10:33:54 2567767 dinov2 helpers.py:102]   [220/634]  eta: 0:28:32    time: 3.979860  data: 0.001250  max mem: 4109
I20241204 10:34:34 2567767 dinov2 helpers.py:102]   [230/634]  eta: 0:27:48    time: 3.977808  data: 0.000985  max mem: 4109
I20241204 10:35:13 2567767 dinov2 helpers.py:102]   [240/634]  eta: 0:27:04    time: 3.975482  data: 0.001051  max mem: 4109
I20241204 10:35:53 2567767 dinov2 helpers.py:102]   [250/634]  eta: 0:26:21    time: 3.976402  data: 0.001356  max mem: 4109
I20241204 10:36:33 2567767 dinov2 helpers.py:102]   [260/634]  eta: 0:25:38    time: 3.979661  data: 0.001390  max mem: 4109
I20241204 10:37:13 2567767 dinov2 helpers.py:102]   [270/634]  eta: 0:24:55    time: 3.981375  data: 0.001185  max mem: 4109
I20241204 10:37:53 2567767 dinov2 helpers.py:102]   [280/634]  eta: 0:24:12    time: 3.981060  data: 0.000798  max mem: 4109
I20241204 10:38:32 2567767 dinov2 helpers.py:102]   [290/634]  eta: 0:23:30    time: 3.980712  data: 0.001549  max mem: 4109
I20241204 10:39:12 2567767 dinov2 helpers.py:102]   [300/634]  eta: 0:22:47    time: 3.982197  data: 0.002804  max mem: 4109
I20241204 10:39:52 2567767 dinov2 helpers.py:102]   [310/634]  eta: 0:22:05    time: 3.982034  data: 0.004766  max mem: 4109
I20241204 10:40:32 2567767 dinov2 helpers.py:102]   [320/634]  eta: 0:21:23    time: 3.981609  data: 0.003444  max mem: 4109
I20241204 10:41:12 2567767 dinov2 helpers.py:102]   [330/634]  eta: 0:20:41    time: 3.984897  data: 0.000658  max mem: 4109
I20241204 10:41:52 2567767 dinov2 helpers.py:102]   [340/634]  eta: 0:20:00    time: 3.986715  data: 0.001027  max mem: 4109
I20241204 10:42:31 2567767 dinov2 helpers.py:102]   [350/634]  eta: 0:19:18    time: 3.984207  data: 0.001345  max mem: 4109
I20241204 10:43:11 2567767 dinov2 helpers.py:102]   [360/634]  eta: 0:18:37    time: 3.983190  data: 0.001299  max mem: 4109
I20241204 10:43:51 2567767 dinov2 helpers.py:102]   [370/634]  eta: 0:17:55    time: 3.987731  data: 0.001173  max mem: 4109
I20241204 10:44:31 2567767 dinov2 helpers.py:102]   [380/634]  eta: 0:17:14    time: 3.987790  data: 0.001159  max mem: 4109
I20241204 10:45:11 2567767 dinov2 helpers.py:102]   [390/634]  eta: 0:16:33    time: 3.983335  data: 0.001219  max mem: 4109
I20241204 10:45:51 2567767 dinov2 helpers.py:102]   [400/634]  eta: 0:15:51    time: 3.981677  data: 0.002028  max mem: 4109
I20241204 10:46:31 2567767 dinov2 helpers.py:102]   [410/634]  eta: 0:15:10    time: 3.985217  data: 0.002058  max mem: 4109
I20241204 10:47:10 2567767 dinov2 helpers.py:102]   [420/634]  eta: 0:14:29    time: 3.985043  data: 0.001137  max mem: 4109
I20241204 10:47:50 2567767 dinov2 helpers.py:102]   [430/634]  eta: 0:13:48    time: 3.980453  data: 0.001011  max mem: 4109
I20241204 10:48:30 2567767 dinov2 helpers.py:102]   [440/634]  eta: 0:13:07    time: 3.979457  data: 0.000820  max mem: 4109
I20241204 10:49:10 2567767 dinov2 helpers.py:102]   [450/634]  eta: 0:12:26    time: 3.977564  data: 0.000942  max mem: 4109
I20241204 10:49:50 2567767 dinov2 helpers.py:102]   [460/634]  eta: 0:11:45    time: 3.978605  data: 0.001169  max mem: 4109
I20241204 10:50:29 2567767 dinov2 helpers.py:102]   [470/634]  eta: 0:11:04    time: 3.979429  data: 0.001331  max mem: 4109
I20241204 10:51:09 2567767 dinov2 helpers.py:102]   [480/634]  eta: 0:10:24    time: 3.978565  data: 0.001770  max mem: 4109
I20241204 10:51:49 2567767 dinov2 helpers.py:102]   [490/634]  eta: 0:09:43    time: 3.982359  data: 0.001682  max mem: 4109
I20241204 10:52:29 2567767 dinov2 helpers.py:102]   [500/634]  eta: 0:09:02    time: 3.982340  data: 0.001773  max mem: 4109
I20241204 10:53:09 2567767 dinov2 helpers.py:102]   [510/634]  eta: 0:08:22    time: 3.983374  data: 0.001658  max mem: 4109
I20241204 10:53:48 2567767 dinov2 helpers.py:102]   [520/634]  eta: 0:07:41    time: 3.984120  data: 0.002024  max mem: 4109
I20241204 10:54:28 2567767 dinov2 helpers.py:102]   [530/634]  eta: 0:07:00    time: 3.983173  data: 0.002701  max mem: 4109
I20241204 10:55:08 2567767 dinov2 helpers.py:102]   [540/634]  eta: 0:06:20    time: 3.988723  data: 0.001997  max mem: 4109
I20241204 10:55:48 2567767 dinov2 helpers.py:102]   [550/634]  eta: 0:05:39    time: 3.991527  data: 0.001156  max mem: 4109
I20241204 10:56:28 2567767 dinov2 helpers.py:102]   [560/634]  eta: 0:04:59    time: 3.988854  data: 0.001671  max mem: 4109
I20241204 10:57:08 2567767 dinov2 helpers.py:102]   [570/634]  eta: 0:04:18    time: 3.988729  data: 0.002082  max mem: 4109
I20241204 10:57:48 2567767 dinov2 helpers.py:102]   [580/634]  eta: 0:03:38    time: 3.989538  data: 0.001299  max mem: 4109
I20241204 10:58:28 2567767 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.988662  data: 0.001557  max mem: 4109
I20241204 10:59:08 2567767 dinov2 helpers.py:102]   [600/634]  eta: 0:02:17    time: 3.986899  data: 0.002173  max mem: 4109
I20241204 10:59:47 2567767 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.984992  data: 0.001771  max mem: 4109
I20241204 11:00:27 2567767 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.986971  data: 0.001176  max mem: 4109
I20241204 11:01:05 2567767 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.859671  data: 0.001191  max mem: 4109
I20241204 11:01:19 2567767 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.004543  data: 0.001088  max mem: 4109
I20241204 11:01:20 2567767 dinov2 helpers.py:130]  Total time: 0:42:40 (4.038980 s / it)
I20241204 11:01:20 2567767 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:01:20 2567767 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:01:20 2567767 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:01:20 2567767 dinov2 loaders.py:151] sampler: distributed
I20241204 11:01:20 2567767 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:01:20 2567767 dinov2 loaders.py:223] # of batches: 78
I20241204 11:01:20 2567767 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:01:37 2567767 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:20:35    time: 15.841957  data: 11.942389  max mem: 4109
I20241204 11:02:17 2567767 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:45    time: 5.081212  data: 1.088087  max mem: 4109
I20241204 11:02:57 2567767 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:24    time: 4.003565  data: 0.003718  max mem: 4109
I20241204 11:03:37 2567767 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:30    time: 3.996810  data: 0.004809  max mem: 4109
I20241204 11:04:17 2567767 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:43    time: 3.998525  data: 0.007412  max mem: 4109
I20241204 11:04:57 2567767 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:58    time: 4.005620  data: 0.011120  max mem: 4109
I20241204 11:05:37 2567767 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:15    time: 4.002552  data: 0.009195  max mem: 4109
I20241204 11:06:08 2567767 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:32    time: 3.514994  data: 0.005556  max mem: 4109
I20241204 11:06:16 2567767 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.550591  data: 0.003304  max mem: 4109
I20241204 11:06:16 2567767 dinov2 helpers.py:130] Test: Total time: 0:04:55 (3.782197 s / it)
I20241204 11:06:16 2567767 dinov2 utils.py:79] Averaged stats: 
I20241204 11:06:17 2567767 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:06:17 2567767 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:06:17 2567767 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:06:17 2567767 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:06:17,443) - Job completed successfully
I20241204 11:06:17 2567767 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:06:17,445) - Exiting after successful completion
I20241204 11:06:17 2567767 submitit submission.py:61] Exiting after successful completion
