submitit INFO (2024-12-04 10:17:37,550) - Starting with JobEnvironment(job_id=2567762, hostname=tars, local_rank=0(8), node=0(1), global_rank=0(8))
submitit INFO (2024-12-04 10:17:37,551) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender/2567762_submitted.pkl
I20241204 10:17:46 2567763 dinov2 config.py:59] git:
  sha: 4c4cfbb972cf0b759288a3e90703e8753dba7c6a, status: has uncommitted changes, branch: main

I20241204 10:17:46 2567763 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAOriginalVal
I20241204 10:17:46 2567763 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241204 10:17:46 2567763 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241204 10:17:46 2567763 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241204 10:18:20 2567763 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241204 10:18:25 2567763 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241204 10:18:25 2567763 dinov2 loaders.py:88] using dataset: "CelebAOriginalTrain"
Load image list
I20241204 10:18:30 2567763 dinov2 loaders.py:93] # of dataset samples: 162,127
I20241204 10:18:30 2567763 dinov2 loaders.py:88] using dataset: "CelebAOriginalVal"
Load image list
I20241204 10:18:31 2567763 dinov2 loaders.py:93] # of dataset samples: 19,792
I20241204 10:18:31 2567763 dinov2 knn.py:260] Extracting features for train set...
I20241204 10:18:31 2567763 dinov2 loaders.py:151] sampler: distributed
I20241204 10:18:31 2567763 dinov2 loaders.py:210] using PyTorch data loader
W20241204 10:18:31 2567763 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241204 10:18:31 2567763 dinov2 loaders.py:223] # of batches: 634
I20241204 10:19:05 2567763 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241204 10:19:05 2567763 dinov2 helpers.py:102]   [  0/634]  eta: 5:57:04    time: 33.792610  data: 15.207079  max mem: 3463
I20241204 10:19:11 2567763 dinov2 helpers.py:102]   [ 10/634]  eta: 0:38:07    time: 3.666008  data: 1.385405  max mem: 4109
I20241204 10:19:43 2567763 dinov2 helpers.py:102]   [ 20/634]  eta: 0:34:49    time: 1.884079  data: 0.001981  max mem: 4109
I20241204 10:20:22 2567763 dinov2 helpers.py:102]   [ 30/634]  eta: 0:35:53    time: 3.509635  data: 0.000671  max mem: 4109
I20241204 10:21:01 2567763 dinov2 helpers.py:102]   [ 40/634]  eta: 0:36:13    time: 3.927497  data: 0.000562  max mem: 4109
I20241204 10:21:41 2567763 dinov2 helpers.py:102]   [ 50/634]  eta: 0:36:12    time: 3.960510  data: 0.001147  max mem: 4109
I20241204 10:22:21 2567763 dinov2 helpers.py:102]   [ 60/634]  eta: 0:35:59    time: 3.972761  data: 0.001282  max mem: 4109
I20241204 10:23:00 2567763 dinov2 helpers.py:102]   [ 70/634]  eta: 0:35:39    time: 3.977212  data: 0.002596  max mem: 4109
I20241204 10:23:40 2567763 dinov2 helpers.py:102]   [ 80/634]  eta: 0:35:13    time: 3.976836  data: 0.002468  max mem: 4109
I20241204 10:24:20 2567763 dinov2 helpers.py:102]   [ 90/634]  eta: 0:34:45    time: 3.976960  data: 0.000526  max mem: 4109
I20241204 10:25:00 2567763 dinov2 helpers.py:102]   [100/634]  eta: 0:34:14    time: 3.978815  data: 0.000664  max mem: 4109
I20241204 10:25:39 2567763 dinov2 helpers.py:102]   [110/634]  eta: 0:33:42    time: 3.976006  data: 0.000699  max mem: 4109
I20241204 10:26:19 2567763 dinov2 helpers.py:102]   [120/634]  eta: 0:33:08    time: 3.976835  data: 0.000820  max mem: 4109
I20241204 10:26:59 2567763 dinov2 helpers.py:102]   [130/634]  eta: 0:32:34    time: 3.978697  data: 0.000920  max mem: 4109
I20241204 10:27:39 2567763 dinov2 helpers.py:102]   [140/634]  eta: 0:31:58    time: 3.978627  data: 0.000853  max mem: 4109
I20241204 10:28:19 2567763 dinov2 helpers.py:102]   [150/634]  eta: 0:31:23    time: 3.979661  data: 0.001254  max mem: 4109
I20241204 10:28:58 2567763 dinov2 helpers.py:102]   [160/634]  eta: 0:30:46    time: 3.979766  data: 0.001083  max mem: 4109
I20241204 10:29:38 2567763 dinov2 helpers.py:102]   [170/634]  eta: 0:30:10    time: 3.978801  data: 0.000657  max mem: 4109
I20241204 10:30:18 2567763 dinov2 helpers.py:102]   [180/634]  eta: 0:29:33    time: 3.978589  data: 0.000784  max mem: 4109
I20241204 10:30:58 2567763 dinov2 helpers.py:102]   [190/634]  eta: 0:28:55    time: 3.979457  data: 0.001004  max mem: 4109
I20241204 10:31:38 2567763 dinov2 helpers.py:102]   [200/634]  eta: 0:28:18    time: 3.977926  data: 0.001270  max mem: 4109
I20241204 10:32:17 2567763 dinov2 helpers.py:102]   [210/634]  eta: 0:27:40    time: 3.976999  data: 0.000958  max mem: 4109
I20241204 10:32:57 2567763 dinov2 helpers.py:102]   [220/634]  eta: 0:27:02    time: 3.978633  data: 0.001938  max mem: 4109
I20241204 10:33:37 2567763 dinov2 helpers.py:102]   [230/634]  eta: 0:26:24    time: 3.979768  data: 0.001981  max mem: 4109
I20241204 10:34:17 2567763 dinov2 helpers.py:102]   [240/634]  eta: 0:25:45    time: 3.976998  data: 0.000751  max mem: 4109
I20241204 10:34:56 2567763 dinov2 helpers.py:102]   [250/634]  eta: 0:25:07    time: 3.975875  data: 0.000769  max mem: 4109
I20241204 10:35:36 2567763 dinov2 helpers.py:102]   [260/634]  eta: 0:24:28    time: 3.976470  data: 0.001410  max mem: 4109
I20241204 10:36:16 2567763 dinov2 helpers.py:102]   [270/634]  eta: 0:23:50    time: 3.976358  data: 0.001470  max mem: 4109
I20241204 10:36:56 2567763 dinov2 helpers.py:102]   [280/634]  eta: 0:23:11    time: 3.978558  data: 0.000819  max mem: 4109
I20241204 10:37:36 2567763 dinov2 helpers.py:102]   [290/634]  eta: 0:22:32    time: 3.977807  data: 0.000981  max mem: 4109
I20241204 10:38:15 2567763 dinov2 helpers.py:102]   [300/634]  eta: 0:21:54    time: 3.978555  data: 0.001366  max mem: 4109
I20241204 10:38:55 2567763 dinov2 helpers.py:102]   [310/634]  eta: 0:21:15    time: 3.980458  data: 0.001889  max mem: 4109
I20241204 10:39:35 2567763 dinov2 helpers.py:102]   [320/634]  eta: 0:20:36    time: 3.978842  data: 0.001518  max mem: 4109
I20241204 10:40:15 2567763 dinov2 helpers.py:102]   [330/634]  eta: 0:19:57    time: 3.979463  data: 0.001056  max mem: 4109
I20241204 10:40:55 2567763 dinov2 helpers.py:102]   [340/634]  eta: 0:19:18    time: 3.981207  data: 0.000942  max mem: 4109
I20241204 10:41:34 2567763 dinov2 helpers.py:102]   [350/634]  eta: 0:18:39    time: 3.978706  data: 0.001159  max mem: 4109
I20241204 10:42:14 2567763 dinov2 helpers.py:102]   [360/634]  eta: 0:18:00    time: 3.981466  data: 0.001158  max mem: 4109
I20241204 10:42:54 2567763 dinov2 helpers.py:102]   [370/634]  eta: 0:17:20    time: 3.984114  data: 0.000915  max mem: 4109
I20241204 10:43:34 2567763 dinov2 helpers.py:102]   [380/634]  eta: 0:16:41    time: 3.981474  data: 0.000960  max mem: 4109
I20241204 10:44:14 2567763 dinov2 helpers.py:102]   [390/634]  eta: 0:16:02    time: 3.983309  data: 0.000718  max mem: 4109
I20241204 10:44:53 2567763 dinov2 helpers.py:102]   [400/634]  eta: 0:15:23    time: 3.983264  data: 0.000799  max mem: 4109
I20241204 10:45:33 2567763 dinov2 helpers.py:102]   [410/634]  eta: 0:14:44    time: 3.982526  data: 0.001297  max mem: 4109
I20241204 10:46:13 2567763 dinov2 helpers.py:102]   [420/634]  eta: 0:14:04    time: 3.984415  data: 0.001247  max mem: 4109
I20241204 10:46:53 2567763 dinov2 helpers.py:102]   [430/634]  eta: 0:13:25    time: 3.985144  data: 0.000728  max mem: 4109
I20241204 10:47:33 2567763 dinov2 helpers.py:102]   [440/634]  eta: 0:12:46    time: 3.983250  data: 0.000843  max mem: 4109
I20241204 10:48:13 2567763 dinov2 helpers.py:102]   [450/634]  eta: 0:12:06    time: 3.978672  data: 0.000752  max mem: 4109
I20241204 10:48:52 2567763 dinov2 helpers.py:102]   [460/634]  eta: 0:11:27    time: 3.976597  data: 0.000550  max mem: 4109
I20241204 10:49:32 2567763 dinov2 helpers.py:102]   [470/634]  eta: 0:10:47    time: 3.975578  data: 0.000652  max mem: 4109
I20241204 10:50:12 2567763 dinov2 helpers.py:102]   [480/634]  eta: 0:10:08    time: 3.974980  data: 0.001023  max mem: 4109
I20241204 10:50:52 2567763 dinov2 helpers.py:102]   [490/634]  eta: 0:09:29    time: 3.979420  data: 0.001101  max mem: 4109
I20241204 10:51:32 2567763 dinov2 helpers.py:102]   [500/634]  eta: 0:08:49    time: 3.983228  data: 0.001134  max mem: 4109
I20241204 10:52:11 2567763 dinov2 helpers.py:102]   [510/634]  eta: 0:08:10    time: 3.983482  data: 0.001336  max mem: 4109
I20241204 10:52:51 2567763 dinov2 helpers.py:102]   [520/634]  eta: 0:07:30    time: 3.979694  data: 0.001078  max mem: 4109
I20241204 10:53:31 2567763 dinov2 helpers.py:102]   [530/634]  eta: 0:06:51    time: 3.981478  data: 0.001293  max mem: 4109
I20241204 10:54:11 2567763 dinov2 helpers.py:102]   [540/634]  eta: 0:06:11    time: 3.983240  data: 0.003050  max mem: 4109
I20241204 10:54:51 2567763 dinov2 helpers.py:102]   [550/634]  eta: 0:05:32    time: 3.985091  data: 0.002562  max mem: 4109
I20241204 10:55:31 2567763 dinov2 helpers.py:102]   [560/634]  eta: 0:04:52    time: 3.988792  data: 0.000641  max mem: 4109
I20241204 10:56:10 2567763 dinov2 helpers.py:102]   [570/634]  eta: 0:04:13    time: 3.986984  data: 0.001010  max mem: 4109
I20241204 10:56:50 2567763 dinov2 helpers.py:102]   [580/634]  eta: 0:03:33    time: 3.989068  data: 0.001061  max mem: 4109
I20241204 10:57:30 2567763 dinov2 helpers.py:102]   [590/634]  eta: 0:02:54    time: 3.987897  data: 0.000729  max mem: 4109
I20241204 10:58:10 2567763 dinov2 helpers.py:102]   [600/634]  eta: 0:02:14    time: 3.986725  data: 0.000711  max mem: 4109
I20241204 10:58:50 2567763 dinov2 helpers.py:102]   [610/634]  eta: 0:01:35    time: 3.984988  data: 0.000651  max mem: 4109
I20241204 10:59:30 2567763 dinov2 helpers.py:102]   [620/634]  eta: 0:00:55    time: 3.981181  data: 0.000635  max mem: 4109
I20241204 11:00:10 2567763 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.982267  data: 0.000884  max mem: 4109
I20241204 11:00:29 2567763 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.359021  data: 0.000812  max mem: 4109
I20241204 11:00:29 2567763 dinov2 helpers.py:130]  Total time: 0:41:58 (3.972123 s / it)
I20241204 11:00:29 2567763 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241204 11:00:29 2567763 dinov2 utils.py:142] Labels shape: (162127,)
I20241204 11:00:30 2567763 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241204 11:00:30 2567763 dinov2 loaders.py:151] sampler: distributed
I20241204 11:00:30 2567763 dinov2 loaders.py:210] using PyTorch data loader
I20241204 11:00:30 2567763 dinov2 loaders.py:223] # of batches: 78
I20241204 11:00:30 2567763 dinov2 knn.py:299] Start the k-NN classification.
I20241204 11:00:40 2567763 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:11:26    time: 8.803094  data: 4.702225  max mem: 4109
I20241204 11:01:16 2567763 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:04:37    time: 4.082534  data: 0.431878  max mem: 4109
I20241204 11:01:50 2567763 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:39    time: 3.529480  data: 0.007511  max mem: 4109
I20241204 11:02:30 2567763 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:04    time: 3.727724  data: 0.010515  max mem: 4109
I20241204 11:03:11 2567763 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:28    time: 4.015337  data: 0.006987  max mem: 4109
I20241204 11:03:51 2567763 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:49    time: 4.005191  data: 0.004155  max mem: 4109
I20241204 11:04:31 2567763 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:10    time: 3.994840  data: 0.005412  max mem: 4109
I20241204 11:05:11 2567763 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:31    time: 4.009650  data: 0.003883  max mem: 4109
I20241204 11:05:37 2567763 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.907552  data: 0.001599  max mem: 4109
I20241204 11:05:37 2567763 dinov2 helpers.py:130] Test: Total time: 0:05:05 (3.920941 s / it)
I20241204 11:05:37 2567763 dinov2 utils.py:79] Averaged stats: 
I20241204 11:05:38 2567763 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 91.76
I20241204 11:05:38 2567763 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 92.01
I20241204 11:05:38 2567763 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 91.68
I20241204 11:05:38 2567763 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 91.44
submitit INFO (2024-12-04 11:05:39,218) - Job completed successfully
I20241204 11:05:39 2567763 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-04 11:05:39,238) - Exiting after successful completion
I20241204 11:05:39 2567763 submitit submission.py:61] Exiting after successful completion
