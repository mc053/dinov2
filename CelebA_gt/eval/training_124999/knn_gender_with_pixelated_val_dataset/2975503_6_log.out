submitit INFO (2024-12-05 07:22:37,828) - Starting with JobEnvironment(job_id=2975503, hostname=tars, local_rank=6(8), node=0(1), global_rank=6(8))
submitit INFO (2024-12-05 07:22:37,828) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset/2975503_submitted.pkl
I20241205 07:22:46 2975512 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 07:22:46 2975512 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 07:22:46 2975512 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 07:22:46 2975512 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 07:22:46 2975512 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 07:23:22 2975512 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 07:23:26 2975512 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 07:23:26 2975512 dinov2 loaders.py:94] using dataset: "CelebAOriginalTrain"
Load image list
I20241205 07:23:41 2975512 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 07:23:41 2975512 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 07:23:45 2975512 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 07:23:45 2975512 dinov2 knn.py:260] Extracting features for train set...
I20241205 07:23:45 2975512 dinov2 loaders.py:157] sampler: distributed
I20241205 07:23:45 2975512 dinov2 loaders.py:216] using PyTorch data loader
W20241205 07:23:45 2975512 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 07:23:45 2975512 dinov2 loaders.py:229] # of batches: 634
I20241205 07:24:43 2975512 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 07:24:43 2975512 dinov2 helpers.py:102]   [  0/634]  eta: 10:17:37    time: 58.449585  data: 11.088099  max mem: 3463
I20241205 07:25:07 2975512 dinov2 helpers.py:102]   [ 10/634]  eta: 1:18:05    time: 7.509361  data: 1.010847  max mem: 4109
I20241205 07:25:47 2975512 dinov2 helpers.py:102]   [ 20/634]  eta: 0:59:30    time: 3.183248  data: 0.002043  max mem: 4109
I20241205 07:26:27 2975512 dinov2 helpers.py:102]   [ 30/634]  eta: 0:52:30    time: 3.955990  data: 0.001137  max mem: 4109
I20241205 07:27:06 2975512 dinov2 helpers.py:102]   [ 40/634]  eta: 0:48:36    time: 3.960357  data: 0.001012  max mem: 4109
I20241205 07:27:46 2975512 dinov2 helpers.py:102]   [ 50/634]  eta: 0:45:59    time: 3.961017  data: 0.000892  max mem: 4109
I20241205 07:28:25 2975512 dinov2 helpers.py:102]   [ 60/634]  eta: 0:44:00    time: 3.962921  data: 0.000825  max mem: 4109
I20241205 07:29:05 2975512 dinov2 helpers.py:102]   [ 70/634]  eta: 0:42:23    time: 3.961949  data: 0.000780  max mem: 4109
I20241205 07:29:45 2975512 dinov2 helpers.py:102]   [ 80/634]  eta: 0:41:00    time: 3.957043  data: 0.001829  max mem: 4109
I20241205 07:30:24 2975512 dinov2 helpers.py:102]   [ 90/634]  eta: 0:39:47    time: 3.956787  data: 0.001716  max mem: 4109
I20241205 07:31:04 2975512 dinov2 helpers.py:102]   [100/634]  eta: 0:38:40    time: 3.958604  data: 0.000868  max mem: 4109
I20241205 07:31:43 2975512 dinov2 helpers.py:102]   [110/634]  eta: 0:37:38    time: 3.954993  data: 0.000824  max mem: 4109
I20241205 07:32:23 2975512 dinov2 helpers.py:102]   [120/634]  eta: 0:36:40    time: 3.956874  data: 0.000757  max mem: 4109
I20241205 07:33:02 2975512 dinov2 helpers.py:102]   [130/634]  eta: 0:35:45    time: 3.959612  data: 0.000790  max mem: 4109
I20241205 07:33:42 2975512 dinov2 helpers.py:102]   [140/634]  eta: 0:34:52    time: 3.955154  data: 0.000809  max mem: 4109
I20241205 07:34:22 2975512 dinov2 helpers.py:102]   [150/634]  eta: 0:34:00    time: 3.954251  data: 0.000743  max mem: 4109
I20241205 07:35:01 2975512 dinov2 helpers.py:102]   [160/634]  eta: 0:33:10    time: 3.954302  data: 0.000749  max mem: 4109
I20241205 07:35:41 2975512 dinov2 helpers.py:102]   [170/634]  eta: 0:32:22    time: 3.952438  data: 0.001040  max mem: 4109
I20241205 07:36:20 2975512 dinov2 helpers.py:102]   [180/634]  eta: 0:31:34    time: 3.954369  data: 0.000897  max mem: 4109
I20241205 07:37:00 2975512 dinov2 helpers.py:102]   [190/634]  eta: 0:30:47    time: 3.954518  data: 0.000546  max mem: 4109
I20241205 07:37:39 2975512 dinov2 helpers.py:102]   [200/634]  eta: 0:30:01    time: 3.952455  data: 0.001255  max mem: 4109
I20241205 07:38:19 2975512 dinov2 helpers.py:102]   [210/634]  eta: 0:29:16    time: 3.952683  data: 0.002111  max mem: 4109
I20241205 07:38:58 2975512 dinov2 helpers.py:102]   [220/634]  eta: 0:28:31    time: 3.952539  data: 0.001518  max mem: 4109
I20241205 07:39:38 2975512 dinov2 helpers.py:102]   [230/634]  eta: 0:27:46    time: 3.952052  data: 0.000784  max mem: 4109
I20241205 07:40:17 2975512 dinov2 helpers.py:102]   [240/634]  eta: 0:27:02    time: 3.954285  data: 0.001026  max mem: 4109
I20241205 07:40:57 2975512 dinov2 helpers.py:102]   [250/634]  eta: 0:26:18    time: 3.954565  data: 0.001140  max mem: 4109
I20241205 07:41:36 2975512 dinov2 helpers.py:102]   [260/634]  eta: 0:25:35    time: 3.952523  data: 0.000873  max mem: 4109
I20241205 07:42:16 2975512 dinov2 helpers.py:102]   [270/634]  eta: 0:24:52    time: 3.952456  data: 0.001088  max mem: 4109
I20241205 07:42:55 2975512 dinov2 helpers.py:102]   [280/634]  eta: 0:24:09    time: 3.952248  data: 0.001146  max mem: 4109
I20241205 07:43:35 2975512 dinov2 helpers.py:102]   [290/634]  eta: 0:23:26    time: 3.954187  data: 0.001094  max mem: 4109
I20241205 07:44:15 2975512 dinov2 helpers.py:102]   [300/634]  eta: 0:22:44    time: 3.954911  data: 0.001279  max mem: 4109
I20241205 07:44:54 2975512 dinov2 helpers.py:102]   [310/634]  eta: 0:22:02    time: 3.953379  data: 0.001271  max mem: 4109
I20241205 07:45:34 2975512 dinov2 helpers.py:102]   [320/634]  eta: 0:21:20    time: 3.955664  data: 0.001425  max mem: 4109
I20241205 07:46:13 2975512 dinov2 helpers.py:102]   [330/634]  eta: 0:20:38    time: 3.955364  data: 0.002521  max mem: 4109
I20241205 07:46:53 2975512 dinov2 helpers.py:102]   [340/634]  eta: 0:19:56    time: 3.952639  data: 0.002151  max mem: 4109
I20241205 07:47:32 2975512 dinov2 helpers.py:102]   [350/634]  eta: 0:19:14    time: 3.954284  data: 0.001008  max mem: 4109
I20241205 07:48:12 2975512 dinov2 helpers.py:102]   [360/634]  eta: 0:18:33    time: 3.953166  data: 0.001311  max mem: 4109
I20241205 07:48:51 2975512 dinov2 helpers.py:102]   [370/634]  eta: 0:17:51    time: 3.951922  data: 0.001336  max mem: 4109
I20241205 07:49:31 2975512 dinov2 helpers.py:102]   [380/634]  eta: 0:17:10    time: 3.952754  data: 0.001149  max mem: 4109
I20241205 07:50:10 2975512 dinov2 helpers.py:102]   [390/634]  eta: 0:16:29    time: 3.952000  data: 0.001352  max mem: 4109
I20241205 07:50:50 2975512 dinov2 helpers.py:102]   [400/634]  eta: 0:15:48    time: 3.954114  data: 0.001405  max mem: 4109
I20241205 07:51:30 2975512 dinov2 helpers.py:102]   [410/634]  eta: 0:15:07    time: 3.956299  data: 0.001129  max mem: 4109
I20241205 07:52:09 2975512 dinov2 helpers.py:102]   [420/634]  eta: 0:14:26    time: 3.954354  data: 0.000971  max mem: 4109
I20241205 07:52:49 2975512 dinov2 helpers.py:102]   [430/634]  eta: 0:13:45    time: 3.958652  data: 0.001027  max mem: 4109
I20241205 07:53:28 2975512 dinov2 helpers.py:102]   [440/634]  eta: 0:13:04    time: 3.964562  data: 0.001246  max mem: 4109
I20241205 07:54:08 2975512 dinov2 helpers.py:102]   [450/634]  eta: 0:12:23    time: 3.966125  data: 0.001165  max mem: 4109
I20241205 07:54:48 2975512 dinov2 helpers.py:102]   [460/634]  eta: 0:11:43    time: 3.968286  data: 0.001074  max mem: 4109
I20241205 07:55:27 2975512 dinov2 helpers.py:102]   [470/634]  eta: 0:11:02    time: 3.970076  data: 0.001312  max mem: 4109
I20241205 07:56:07 2975512 dinov2 helpers.py:102]   [480/634]  eta: 0:10:21    time: 3.972245  data: 0.001243  max mem: 4109
I20241205 07:56:47 2975512 dinov2 helpers.py:102]   [490/634]  eta: 0:09:41    time: 3.972190  data: 0.001009  max mem: 4109
I20241205 07:57:27 2975512 dinov2 helpers.py:102]   [500/634]  eta: 0:09:00    time: 3.971743  data: 0.001094  max mem: 4109
I20241205 07:58:06 2975512 dinov2 helpers.py:102]   [510/634]  eta: 0:08:20    time: 3.970009  data: 0.001290  max mem: 4109
I20241205 07:58:46 2975512 dinov2 helpers.py:102]   [520/634]  eta: 0:07:39    time: 3.968229  data: 0.001071  max mem: 4109
I20241205 07:59:26 2975512 dinov2 helpers.py:102]   [530/634]  eta: 0:06:59    time: 3.970100  data: 0.000817  max mem: 4109
I20241205 08:00:05 2975512 dinov2 helpers.py:102]   [540/634]  eta: 0:06:18    time: 3.972141  data: 0.000907  max mem: 4109
I20241205 08:00:45 2975512 dinov2 helpers.py:102]   [550/634]  eta: 0:05:38    time: 3.971678  data: 0.001101  max mem: 4109
I20241205 08:01:25 2975512 dinov2 helpers.py:102]   [560/634]  eta: 0:04:58    time: 3.971799  data: 0.001305  max mem: 4109
I20241205 08:02:05 2975512 dinov2 helpers.py:102]   [570/634]  eta: 0:04:17    time: 3.972850  data: 0.001188  max mem: 4109
I20241205 08:02:44 2975512 dinov2 helpers.py:102]   [580/634]  eta: 0:03:37    time: 3.971309  data: 0.001239  max mem: 4109
I20241205 08:03:24 2975512 dinov2 helpers.py:102]   [590/634]  eta: 0:02:57    time: 3.971136  data: 0.001201  max mem: 4109
I20241205 08:04:02 2975512 dinov2 helpers.py:102]   [600/634]  eta: 0:02:16    time: 3.884216  data: 0.001459  max mem: 4109
I20241205 08:04:41 2975512 dinov2 helpers.py:102]   [610/634]  eta: 0:01:36    time: 3.853232  data: 0.001223  max mem: 4109
I20241205 08:05:19 2975512 dinov2 helpers.py:102]   [620/634]  eta: 0:00:56    time: 3.866057  data: 0.000525  max mem: 4109
I20241205 08:05:59 2975512 dinov2 helpers.py:102]   [630/634]  eta: 0:00:16    time: 3.873030  data: 0.000557  max mem: 4109
I20241205 08:06:18 2975512 dinov2 helpers.py:102]   [633/634]  eta: 0:00:04    time: 4.261550  data: 0.000501  max mem: 4109
I20241205 08:06:18 2975512 dinov2 helpers.py:130]  Total time: 0:42:33 (4.027098 s / it)
I20241205 08:06:18 2975512 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:06:18 2975512 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:06:19 2975512 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:06:19 2975512 dinov2 loaders.py:157] sampler: distributed
I20241205 08:06:19 2975512 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:06:19 2975512 dinov2 loaders.py:229] # of batches: 78
I20241205 08:06:19 2975512 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:06:28 2975512 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:10:28    time: 8.063604  data: 4.000826  max mem: 4109
I20241205 08:06:58 2975512 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:03:58    time: 3.504349  data: 0.374092  max mem: 4109
I20241205 08:07:39 2975512 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:03:37    time: 3.541228  data: 0.009542  max mem: 4109
I20241205 08:08:19 2975512 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:04    time: 4.029410  data: 0.005972  max mem: 4109
I20241205 08:08:58 2975512 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:26    time: 3.983540  data: 0.006643  max mem: 4109
I20241205 08:09:29 2975512 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:43    time: 3.504030  data: 0.006965  max mem: 4109
I20241205 08:09:56 2975512 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:03    time: 2.903924  data: 0.005042  max mem: 4109
I20241205 08:10:21 2975512 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:27    time: 2.620572  data: 0.005073  max mem: 4109
I20241205 08:10:38 2975512 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 2.436340  data: 0.003240  max mem: 4109
I20241205 08:10:38 2975512 dinov2 helpers.py:130] Test: Total time: 0:04:17 (3.306695 s / it)
I20241205 08:10:38 2975512 dinov2 utils.py:79] Averaged stats: 
I20241205 08:10:38 2975512 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 75.06
I20241205 08:10:38 2975512 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 75.91
I20241205 08:10:38 2975512 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 77.50
I20241205 08:10:38 2975512 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 78.01
submitit INFO (2024-12-05 08:10:39,206) - Job completed successfully
I20241205 08:10:39 2975512 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 08:10:39,208) - Exiting after successful completion
I20241205 08:10:39 2975512 submitit submission.py:61] Exiting after successful completion
