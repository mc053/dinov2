submitit INFO (2024-12-05 07:22:37,855) - Starting with JobEnvironment(job_id=2975503, hostname=tars, local_rank=1(8), node=0(1), global_rank=1(8))
submitit INFO (2024-12-05 07:22:37,855) - Loading pickle: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset/2975503_submitted.pkl
I20241205 07:22:47 2975507 dinov2 config.py:59] git:
  sha: 1514d8883ab0f94a8e16e2f31e7880cd543d6e95, status: has uncommitted changes, branch: main

I20241205 07:22:47 2975507 dinov2 config.py:60] batch_size: 256
comment: 
config_file: CelebA_gt/config.yaml
exclude: 
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
ngpus: 8
nodes: 1
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset
partition: learnlab
pretrained_weights: CelebA_gt/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
timeout: 2800
train_dataset_str: CelebAOriginalTrain
use_volta32: False
val_dataset_str: CelebAPixelatedVal
I20241205 07:22:47 2975507 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20241205 07:22:47 2975507 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAOriginalTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_gt/eval/training_124999/knn_gender_with_pixelated_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20241205 07:22:47 2975507 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241205 07:23:16 2975507 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241205 07:23:19 2975507 dinov2 utils.py:33] Pretrained weights found at CelebA_gt/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241205 07:23:19 2975507 dinov2 loaders.py:94] using dataset: "CelebAOriginalTrain"
Load image list
I20241205 07:23:30 2975507 dinov2 loaders.py:99] # of dataset samples: 162,127
I20241205 07:23:30 2975507 dinov2 loaders.py:94] using dataset: "CelebAPixelatedVal"
Load image list
I20241205 07:23:34 2975507 dinov2 loaders.py:99] # of dataset samples: 19,792
I20241205 07:23:34 2975507 dinov2 knn.py:260] Extracting features for train set...
I20241205 07:23:34 2975507 dinov2 loaders.py:157] sampler: distributed
I20241205 07:23:34 2975507 dinov2 loaders.py:216] using PyTorch data loader
W20241205 07:23:34 2975507 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241205 07:23:34 2975507 dinov2 loaders.py:229] # of batches: 634
I20241205 07:24:12 2975507 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20241205 07:24:12 2975507 dinov2 helpers.py:102]   [  0/634]  eta: 6:45:03    time: 38.333195  data: 9.282597  max mem: 3463
I20241205 07:24:21 2975507 dinov2 helpers.py:102]   [ 10/634]  eta: 0:44:29    time: 4.277843  data: 0.848963  max mem: 4109
I20241205 07:24:36 2975507 dinov2 helpers.py:102]   [ 20/634]  eta: 0:30:16    time: 1.189938  data: 0.004145  max mem: 4109
I20241205 07:24:56 2975507 dinov2 helpers.py:102]   [ 30/634]  eta: 0:26:40    time: 1.753791  data: 0.001587  max mem: 4109
I20241205 07:25:36 2975507 dinov2 helpers.py:102]   [ 40/634]  eta: 0:29:21    time: 2.974350  data: 0.000827  max mem: 4109
I20241205 07:26:15 2975507 dinov2 helpers.py:102]   [ 50/634]  eta: 0:30:45    time: 3.951491  data: 0.001272  max mem: 4109
I20241205 07:26:55 2975507 dinov2 helpers.py:102]   [ 60/634]  eta: 0:31:28    time: 3.956248  data: 0.001218  max mem: 4109
I20241205 07:27:34 2975507 dinov2 helpers.py:102]   [ 70/634]  eta: 0:31:49    time: 3.960998  data: 0.001195  max mem: 4109
I20241205 07:28:14 2975507 dinov2 helpers.py:102]   [ 80/634]  eta: 0:31:54    time: 3.961068  data: 0.001046  max mem: 4109
I20241205 07:28:54 2975507 dinov2 helpers.py:102]   [ 90/634]  eta: 0:31:50    time: 3.959229  data: 0.001025  max mem: 4109
I20241205 07:29:33 2975507 dinov2 helpers.py:102]   [100/634]  eta: 0:31:38    time: 3.959890  data: 0.001114  max mem: 4109
I20241205 07:30:13 2975507 dinov2 helpers.py:102]   [110/634]  eta: 0:31:22    time: 3.960529  data: 0.000870  max mem: 4109
I20241205 07:30:52 2975507 dinov2 helpers.py:102]   [120/634]  eta: 0:31:02    time: 3.958763  data: 0.000668  max mem: 4109
I20241205 07:31:32 2975507 dinov2 helpers.py:102]   [130/634]  eta: 0:30:38    time: 3.956970  data: 0.000543  max mem: 4109
I20241205 07:32:11 2975507 dinov2 helpers.py:102]   [140/634]  eta: 0:30:12    time: 3.954857  data: 0.000707  max mem: 4109
I20241205 07:32:51 2975507 dinov2 helpers.py:102]   [150/634]  eta: 0:29:45    time: 3.952314  data: 0.001150  max mem: 4109
I20241205 07:33:31 2975507 dinov2 helpers.py:102]   [160/634]  eta: 0:29:16    time: 3.957230  data: 0.001121  max mem: 4109
I20241205 07:34:10 2975507 dinov2 helpers.py:102]   [170/634]  eta: 0:28:46    time: 3.956810  data: 0.000961  max mem: 4109
I20241205 07:34:50 2975507 dinov2 helpers.py:102]   [180/634]  eta: 0:28:14    time: 3.956688  data: 0.001114  max mem: 4109
I20241205 07:35:29 2975507 dinov2 helpers.py:102]   [190/634]  eta: 0:27:42    time: 3.956945  data: 0.001085  max mem: 4109
I20241205 07:36:09 2975507 dinov2 helpers.py:102]   [200/634]  eta: 0:27:09    time: 3.955376  data: 0.001563  max mem: 4109
I20241205 07:36:48 2975507 dinov2 helpers.py:102]   [210/634]  eta: 0:26:36    time: 3.957357  data: 0.001428  max mem: 4109
I20241205 07:37:28 2975507 dinov2 helpers.py:102]   [220/634]  eta: 0:26:02    time: 3.954312  data: 0.001825  max mem: 4109
I20241205 07:38:07 2975507 dinov2 helpers.py:102]   [230/634]  eta: 0:25:27    time: 3.952348  data: 0.002062  max mem: 4109
I20241205 07:38:47 2975507 dinov2 helpers.py:102]   [240/634]  eta: 0:24:52    time: 3.952370  data: 0.001580  max mem: 4109
I20241205 07:39:26 2975507 dinov2 helpers.py:102]   [250/634]  eta: 0:24:17    time: 3.952474  data: 0.001745  max mem: 4109
I20241205 07:40:06 2975507 dinov2 helpers.py:102]   [260/634]  eta: 0:23:41    time: 3.954423  data: 0.001412  max mem: 4109
I20241205 07:40:46 2975507 dinov2 helpers.py:102]   [270/634]  eta: 0:23:05    time: 3.954408  data: 0.002222  max mem: 4109
I20241205 07:41:25 2975507 dinov2 helpers.py:102]   [280/634]  eta: 0:22:29    time: 3.952504  data: 0.001972  max mem: 4109
I20241205 07:42:05 2975507 dinov2 helpers.py:102]   [290/634]  eta: 0:21:52    time: 3.952430  data: 0.001182  max mem: 4109
I20241205 07:42:44 2975507 dinov2 helpers.py:102]   [300/634]  eta: 0:21:16    time: 3.952334  data: 0.001210  max mem: 4109
I20241205 07:43:24 2975507 dinov2 helpers.py:102]   [310/634]  eta: 0:20:39    time: 3.952461  data: 0.000896  max mem: 4109
I20241205 07:44:03 2975507 dinov2 helpers.py:102]   [320/634]  eta: 0:20:02    time: 3.955618  data: 0.000934  max mem: 4109
I20241205 07:44:43 2975507 dinov2 helpers.py:102]   [330/634]  eta: 0:19:25    time: 3.955777  data: 0.000714  max mem: 4109
I20241205 07:45:22 2975507 dinov2 helpers.py:102]   [340/634]  eta: 0:18:47    time: 3.952987  data: 0.000698  max mem: 4109
I20241205 07:46:02 2975507 dinov2 helpers.py:102]   [350/634]  eta: 0:18:10    time: 3.954901  data: 0.001366  max mem: 4109
I20241205 07:46:41 2975507 dinov2 helpers.py:102]   [360/634]  eta: 0:17:33    time: 3.954464  data: 0.001531  max mem: 4109
I20241205 07:47:21 2975507 dinov2 helpers.py:102]   [370/634]  eta: 0:16:55    time: 3.954353  data: 0.001290  max mem: 4109
I20241205 07:48:01 2975507 dinov2 helpers.py:102]   [380/634]  eta: 0:16:17    time: 3.954253  data: 0.001321  max mem: 4109
I20241205 07:48:40 2975507 dinov2 helpers.py:102]   [390/634]  eta: 0:15:39    time: 3.951937  data: 0.001293  max mem: 4109
I20241205 07:49:20 2975507 dinov2 helpers.py:102]   [400/634]  eta: 0:15:01    time: 3.952001  data: 0.001428  max mem: 4109
I20241205 07:49:59 2975507 dinov2 helpers.py:102]   [410/634]  eta: 0:14:23    time: 3.951937  data: 0.001344  max mem: 4109
I20241205 07:50:39 2975507 dinov2 helpers.py:102]   [420/634]  eta: 0:13:45    time: 3.954016  data: 0.001355  max mem: 4109
I20241205 07:51:18 2975507 dinov2 helpers.py:102]   [430/634]  eta: 0:13:07    time: 3.956288  data: 0.001279  max mem: 4109
I20241205 07:51:58 2975507 dinov2 helpers.py:102]   [440/634]  eta: 0:12:29    time: 3.954318  data: 0.000953  max mem: 4109
I20241205 07:52:37 2975507 dinov2 helpers.py:102]   [450/634]  eta: 0:11:51    time: 3.958537  data: 0.000716  max mem: 4109
I20241205 07:53:17 2975507 dinov2 helpers.py:102]   [460/634]  eta: 0:11:12    time: 3.962590  data: 0.001021  max mem: 4109
I20241205 07:53:57 2975507 dinov2 helpers.py:102]   [470/634]  eta: 0:10:34    time: 3.965962  data: 0.001365  max mem: 4109
I20241205 07:54:36 2975507 dinov2 helpers.py:102]   [480/634]  eta: 0:09:56    time: 3.971886  data: 0.001021  max mem: 4109
I20241205 07:55:16 2975507 dinov2 helpers.py:102]   [490/634]  eta: 0:09:17    time: 3.969307  data: 0.000728  max mem: 4109
I20241205 07:55:56 2975507 dinov2 helpers.py:102]   [500/634]  eta: 0:08:39    time: 3.965618  data: 0.000613  max mem: 4109
I20241205 07:56:35 2975507 dinov2 helpers.py:102]   [510/634]  eta: 0:08:00    time: 3.966745  data: 0.000531  max mem: 4109
I20241205 07:57:15 2975507 dinov2 helpers.py:102]   [520/634]  eta: 0:07:22    time: 3.969321  data: 0.000742  max mem: 4109
I20241205 07:57:55 2975507 dinov2 helpers.py:102]   [530/634]  eta: 0:06:43    time: 3.970037  data: 0.000980  max mem: 4109
I20241205 07:58:34 2975507 dinov2 helpers.py:102]   [540/634]  eta: 0:06:04    time: 3.969110  data: 0.001036  max mem: 4109
I20241205 07:59:14 2975507 dinov2 helpers.py:102]   [550/634]  eta: 0:05:26    time: 3.968319  data: 0.001201  max mem: 4109
I20241205 07:59:54 2975507 dinov2 helpers.py:102]   [560/634]  eta: 0:04:47    time: 3.970332  data: 0.001240  max mem: 4109
I20241205 08:00:34 2975507 dinov2 helpers.py:102]   [570/634]  eta: 0:04:08    time: 3.972462  data: 0.001176  max mem: 4109
I20241205 08:01:13 2975507 dinov2 helpers.py:102]   [580/634]  eta: 0:03:29    time: 3.972641  data: 0.000978  max mem: 4109
I20241205 08:01:53 2975507 dinov2 helpers.py:102]   [590/634]  eta: 0:02:51    time: 3.971502  data: 0.001019  max mem: 4109
I20241205 08:02:33 2975507 dinov2 helpers.py:102]   [600/634]  eta: 0:02:12    time: 3.971390  data: 0.001742  max mem: 4109
I20241205 08:03:13 2975507 dinov2 helpers.py:102]   [610/634]  eta: 0:01:33    time: 3.972406  data: 0.001791  max mem: 4109
I20241205 08:03:51 2975507 dinov2 helpers.py:102]   [620/634]  eta: 0:00:54    time: 3.920423  data: 0.001054  max mem: 4109
I20241205 08:04:30 2975507 dinov2 helpers.py:102]   [630/634]  eta: 0:00:15    time: 3.858601  data: 0.001135  max mem: 4109
I20241205 08:04:49 2975507 dinov2 helpers.py:102]   [633/634]  eta: 0:00:03    time: 4.228787  data: 0.001029  max mem: 4109
I20241205 08:04:49 2975507 dinov2 helpers.py:130]  Total time: 0:41:15 (3.904380 s / it)
I20241205 08:04:49 2975507 dinov2 utils.py:141] Features shape: (162127, 1024)
I20241205 08:04:49 2975507 dinov2 utils.py:142] Labels shape: (162127,)
I20241205 08:04:50 2975507 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20241205 08:04:50 2975507 dinov2 loaders.py:157] sampler: distributed
I20241205 08:04:50 2975507 dinov2 loaders.py:216] using PyTorch data loader
I20241205 08:04:50 2975507 dinov2 loaders.py:229] # of batches: 78
I20241205 08:04:50 2975507 dinov2 knn.py:299] Start the k-NN classification.
I20241205 08:05:01 2975507 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:13:30    time: 10.385409  data: 6.314748  max mem: 4109
I20241205 08:05:42 2975507 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:05:16    time: 4.651481  data: 0.580887  max mem: 4109
I20241205 08:06:22 2975507 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:04:12    time: 4.050295  data: 0.007168  max mem: 4109
I20241205 08:06:53 2975507 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:03:08    time: 3.537388  data: 0.007684  max mem: 4109
I20241205 08:07:33 2975507 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:02:29    time: 3.519888  data: 0.005656  max mem: 4109
I20241205 08:08:13 2975507 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:01:50    time: 3.985160  data: 0.003645  max mem: 4109
I20241205 08:08:52 2975507 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:01:11    time: 3.985469  data: 0.004513  max mem: 4109
I20241205 08:09:24 2975507 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:30    time: 3.586013  data: 0.004187  max mem: 4109
I20241205 08:09:44 2975507 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:03    time: 3.167444  data: 0.002764  max mem: 4109
I20241205 08:09:44 2975507 dinov2 helpers.py:130] Test: Total time: 0:04:52 (3.755095 s / it)
I20241205 08:09:44 2975507 dinov2 utils.py:79] Averaged stats: 
I20241205 08:09:45 2975507 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 75.06
I20241205 08:09:45 2975507 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 75.91
I20241205 08:09:45 2975507 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 77.50
I20241205 08:09:45 2975507 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 78.01
submitit INFO (2024-12-05 08:09:45,790) - Job completed successfully
I20241205 08:09:45 2975507 submitit submission.py:56] Job completed successfully
submitit INFO (2024-12-05 08:09:45,800) - Exiting after successful completion
I20241205 08:09:45 2975507 submitit submission.py:61] Exiting after successful completion
