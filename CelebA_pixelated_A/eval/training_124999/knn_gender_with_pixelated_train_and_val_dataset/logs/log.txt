I20250116 17:42:33 2013922 dinov2 config.py:59] git:
  sha: 90f3b9435f92e0479b8e93adaefed55eb315da87, status: has uncommitted changes, branch: main

I20250116 17:42:33 2013922 dinov2 config.py:60] batch_size: 256
config_file: CelebA_pixelated_A/config.yaml
gather_on_cpu: False
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
opts: ['train.output_dir=/home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_A/eval/training_124999/knn_gender_with_pixelated_train_and_val_dataset']
output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_A/eval/training_124999/knn_gender_with_pixelated_train_and_val_dataset
pretrained_weights: CelebA_pixelated_A/eval/training_124999/teacher_checkpoint.pth
temperature: 0.07
train_dataset_str: CelebAPixelatedTrain
val_dataset_str: CelebAPixelatedVal
I20250116 17:42:33 2013922 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0004330127018922193
I20250116 17:42:33 2013922 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 12
  dataset_path: CelebAPixelatedABTrain
  output_dir: /home/stud/m/mc085/mounted_home/dinov2/CelebA_pixelated_A/eval/training_124999/knn_gender_with_pixelated_train_and_val_dataset
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
  a_b_training: A
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
  in_chans: 3
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0004330127018922193
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250116 17:42:33 2013922 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250116 17:42:36 2013922 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20250116 17:42:36 2013922 dinov2 utils.py:33] Pretrained weights found at CelebA_pixelated_A/eval/training_124999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20250116 17:42:36 2013922 dinov2 loaders.py:110] using dataset: "CelebAPixelatedTrain"
I20250116 17:42:38 2013922 dinov2 loaders.py:115] # of dataset samples: 162,127
I20250116 17:42:38 2013922 dinov2 loaders.py:110] using dataset: "CelebAPixelatedVal"
I20250116 17:42:39 2013922 dinov2 loaders.py:115] # of dataset samples: 19,792
I20250116 17:42:39 2013922 dinov2 knn.py:260] Extracting features for train set...
I20250116 17:42:39 2013922 dinov2 loaders.py:173] sampler: distributed
I20250116 17:42:39 2013922 dinov2 loaders.py:232] using PyTorch data loader
W20250116 17:42:39 2013922 py.warnings warnings.py:109] /home/stud/m/mc085/mounted_home/dinov2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20250116 17:42:39 2013922 dinov2 loaders.py:245] # of batches: 634
I20250116 17:42:45 2013922 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([162127, 1024])
I20250116 17:42:45 2013922 dinov2 helpers.py:102]   [  0/634]  eta: 1:03:04    time: 5.969315  data: 3.406573  max mem: 3463
I20250116 17:42:48 2013922 dinov2 helpers.py:102]   [ 10/634]  eta: 0:09:06    time: 0.875782  data: 0.309972  max mem: 4109
I20250116 17:42:53 2013922 dinov2 helpers.py:102]   [ 20/634]  eta: 0:06:52    time: 0.406838  data: 0.000293  max mem: 4109
I20250116 17:42:57 2013922 dinov2 helpers.py:102]   [ 30/634]  eta: 0:06:02    time: 0.447637  data: 0.000298  max mem: 4109
I20250116 17:43:02 2013922 dinov2 helpers.py:102]   [ 40/634]  eta: 0:05:34    time: 0.448504  data: 0.000297  max mem: 4109
I20250116 17:43:06 2013922 dinov2 helpers.py:102]   [ 50/634]  eta: 0:05:15    time: 0.449290  data: 0.000261  max mem: 4109
I20250116 17:43:11 2013922 dinov2 helpers.py:102]   [ 60/634]  eta: 0:05:01    time: 0.449941  data: 0.000263  max mem: 4109
I20250116 17:43:15 2013922 dinov2 helpers.py:102]   [ 70/634]  eta: 0:04:50    time: 0.450455  data: 0.000292  max mem: 4109
I20250116 17:43:20 2013922 dinov2 helpers.py:102]   [ 80/634]  eta: 0:04:41    time: 0.451436  data: 0.000315  max mem: 4109
I20250116 17:43:24 2013922 dinov2 helpers.py:102]   [ 90/634]  eta: 0:04:32    time: 0.452493  data: 0.000312  max mem: 4109
I20250116 17:43:29 2013922 dinov2 helpers.py:102]   [100/634]  eta: 0:04:25    time: 0.452931  data: 0.000293  max mem: 4109
I20250116 17:43:33 2013922 dinov2 helpers.py:102]   [110/634]  eta: 0:04:18    time: 0.453341  data: 0.000264  max mem: 4109
I20250116 17:43:38 2013922 dinov2 helpers.py:102]   [120/634]  eta: 0:04:11    time: 0.453633  data: 0.000243  max mem: 4109
I20250116 17:43:42 2013922 dinov2 helpers.py:102]   [130/634]  eta: 0:04:05    time: 0.453813  data: 0.000242  max mem: 4109
I20250116 17:43:47 2013922 dinov2 helpers.py:102]   [140/634]  eta: 0:03:59    time: 0.453974  data: 0.000260  max mem: 4109
I20250116 17:43:51 2013922 dinov2 helpers.py:102]   [150/634]  eta: 0:03:53    time: 0.454103  data: 0.000268  max mem: 4109
I20250116 17:43:56 2013922 dinov2 helpers.py:102]   [160/634]  eta: 0:03:47    time: 0.454199  data: 0.000276  max mem: 4109
I20250116 17:44:01 2013922 dinov2 helpers.py:102]   [170/634]  eta: 0:03:42    time: 0.454292  data: 0.000322  max mem: 4109
I20250116 17:44:05 2013922 dinov2 helpers.py:102]   [180/634]  eta: 0:03:36    time: 0.454325  data: 0.000313  max mem: 4109
I20250116 17:44:10 2013922 dinov2 helpers.py:102]   [190/634]  eta: 0:03:31    time: 0.454343  data: 0.000276  max mem: 4109
I20250116 17:44:14 2013922 dinov2 helpers.py:102]   [200/634]  eta: 0:03:26    time: 0.454501  data: 0.000260  max mem: 4109
I20250116 17:44:19 2013922 dinov2 helpers.py:102]   [210/634]  eta: 0:03:21    time: 0.454643  data: 0.000283  max mem: 4109
I20250116 17:44:23 2013922 dinov2 helpers.py:102]   [220/634]  eta: 0:03:16    time: 0.454713  data: 0.000292  max mem: 4109
I20250116 17:44:28 2013922 dinov2 helpers.py:102]   [230/634]  eta: 0:03:11    time: 0.454888  data: 0.000267  max mem: 4109
I20250116 17:44:32 2013922 dinov2 helpers.py:102]   [240/634]  eta: 0:03:05    time: 0.455111  data: 0.000277  max mem: 4109
I20250116 17:44:37 2013922 dinov2 helpers.py:102]   [250/634]  eta: 0:03:01    time: 0.455222  data: 0.000308  max mem: 4109
I20250116 17:44:41 2013922 dinov2 helpers.py:102]   [260/634]  eta: 0:02:56    time: 0.455261  data: 0.000321  max mem: 4109
I20250116 17:44:46 2013922 dinov2 helpers.py:102]   [270/634]  eta: 0:02:51    time: 0.455390  data: 0.000275  max mem: 4109
I20250116 17:44:51 2013922 dinov2 helpers.py:102]   [280/634]  eta: 0:02:46    time: 0.455588  data: 0.000277  max mem: 4109
I20250116 17:44:55 2013922 dinov2 helpers.py:102]   [290/634]  eta: 0:02:41    time: 0.455607  data: 0.000308  max mem: 4109
I20250116 17:45:00 2013922 dinov2 helpers.py:102]   [300/634]  eta: 0:02:36    time: 0.455612  data: 0.000285  max mem: 4109
I20250116 17:45:04 2013922 dinov2 helpers.py:102]   [310/634]  eta: 0:02:31    time: 0.455747  data: 0.000252  max mem: 4109
I20250116 17:45:09 2013922 dinov2 helpers.py:102]   [320/634]  eta: 0:02:26    time: 0.455756  data: 0.000263  max mem: 4109
I20250116 17:45:13 2013922 dinov2 helpers.py:102]   [330/634]  eta: 0:02:22    time: 0.455794  data: 0.000274  max mem: 4109
I20250116 17:45:18 2013922 dinov2 helpers.py:102]   [340/634]  eta: 0:02:17    time: 0.455987  data: 0.000285  max mem: 4109
I20250116 17:45:23 2013922 dinov2 helpers.py:102]   [350/634]  eta: 0:02:12    time: 0.456067  data: 0.000285  max mem: 4109
I20250116 17:45:27 2013922 dinov2 helpers.py:102]   [360/634]  eta: 0:02:07    time: 0.456095  data: 0.000254  max mem: 4109
I20250116 17:45:32 2013922 dinov2 helpers.py:102]   [370/634]  eta: 0:02:03    time: 0.456181  data: 0.000253  max mem: 4109
I20250116 17:45:36 2013922 dinov2 helpers.py:102]   [380/634]  eta: 0:01:58    time: 0.456249  data: 0.000261  max mem: 4109
I20250116 17:45:41 2013922 dinov2 helpers.py:102]   [390/634]  eta: 0:01:53    time: 0.456289  data: 0.000251  max mem: 4109
I20250116 17:45:45 2013922 dinov2 helpers.py:102]   [400/634]  eta: 0:01:48    time: 0.456321  data: 0.000252  max mem: 4109
I20250116 17:45:50 2013922 dinov2 helpers.py:102]   [410/634]  eta: 0:01:44    time: 0.456495  data: 0.000257  max mem: 4109
I20250116 17:45:54 2013922 dinov2 helpers.py:102]   [420/634]  eta: 0:01:39    time: 0.456565  data: 0.000245  max mem: 4109
I20250116 17:45:59 2013922 dinov2 helpers.py:102]   [430/634]  eta: 0:01:34    time: 0.456492  data: 0.000256  max mem: 4109
I20250116 17:46:04 2013922 dinov2 helpers.py:102]   [440/634]  eta: 0:01:30    time: 0.456552  data: 0.000279  max mem: 4109
I20250116 17:46:08 2013922 dinov2 helpers.py:102]   [450/634]  eta: 0:01:25    time: 0.456521  data: 0.000274  max mem: 4109
I20250116 17:46:13 2013922 dinov2 helpers.py:102]   [460/634]  eta: 0:01:20    time: 0.456582  data: 0.000248  max mem: 4109
I20250116 17:46:17 2013922 dinov2 helpers.py:102]   [470/634]  eta: 0:01:16    time: 0.456694  data: 0.000245  max mem: 4109
I20250116 17:46:22 2013922 dinov2 helpers.py:102]   [480/634]  eta: 0:01:11    time: 0.456778  data: 0.000265  max mem: 4109
I20250116 17:46:26 2013922 dinov2 helpers.py:102]   [490/634]  eta: 0:01:06    time: 0.456967  data: 0.000268  max mem: 4109
I20250116 17:46:31 2013922 dinov2 helpers.py:102]   [500/634]  eta: 0:01:02    time: 0.456994  data: 0.000295  max mem: 4109
I20250116 17:46:36 2013922 dinov2 helpers.py:102]   [510/634]  eta: 0:00:57    time: 0.456988  data: 0.000288  max mem: 4109
I20250116 17:46:40 2013922 dinov2 helpers.py:102]   [520/634]  eta: 0:00:52    time: 0.457035  data: 0.000257  max mem: 4109
I20250116 17:46:45 2013922 dinov2 helpers.py:102]   [530/634]  eta: 0:00:48    time: 0.456990  data: 0.000247  max mem: 4109
I20250116 17:46:49 2013922 dinov2 helpers.py:102]   [540/634]  eta: 0:00:43    time: 0.456912  data: 0.000260  max mem: 4109
I20250116 17:46:54 2013922 dinov2 helpers.py:102]   [550/634]  eta: 0:00:38    time: 0.456862  data: 0.000378  max mem: 4109
I20250116 17:46:58 2013922 dinov2 helpers.py:102]   [560/634]  eta: 0:00:34    time: 0.456866  data: 0.000352  max mem: 4109
I20250116 17:47:03 2013922 dinov2 helpers.py:102]   [570/634]  eta: 0:00:29    time: 0.456890  data: 0.000252  max mem: 4109
I20250116 17:47:08 2013922 dinov2 helpers.py:102]   [580/634]  eta: 0:00:24    time: 0.456800  data: 0.000259  max mem: 4109
I20250116 17:47:12 2013922 dinov2 helpers.py:102]   [590/634]  eta: 0:00:20    time: 0.456795  data: 0.000249  max mem: 4109
I20250116 17:47:17 2013922 dinov2 helpers.py:102]   [600/634]  eta: 0:00:15    time: 0.456820  data: 0.000246  max mem: 4109
I20250116 17:47:21 2013922 dinov2 helpers.py:102]   [610/634]  eta: 0:00:11    time: 0.456802  data: 0.000268  max mem: 4109
I20250116 17:47:26 2013922 dinov2 helpers.py:102]   [620/634]  eta: 0:00:06    time: 0.456716  data: 0.000267  max mem: 4109
I20250116 17:47:30 2013922 dinov2 helpers.py:102]   [630/634]  eta: 0:00:01    time: 0.456524  data: 0.000319  max mem: 4109
I20250116 17:47:33 2013922 dinov2 helpers.py:102]   [633/634]  eta: 0:00:00    time: 0.498958  data: 0.000299  max mem: 4109
I20250116 17:47:33 2013922 dinov2 helpers.py:130]  Total time: 0:04:54 (0.463983 s / it)
I20250116 17:47:33 2013922 dinov2 utils.py:141] Features shape: (162127, 1024)
I20250116 17:47:33 2013922 dinov2 utils.py:142] Labels shape: (162127,)
I20250116 17:47:33 2013922 dinov2 knn.py:264] Train features created, shape torch.Size([162127, 1024]).
I20250116 17:47:33 2013922 dinov2 loaders.py:173] sampler: distributed
I20250116 17:47:33 2013922 dinov2 loaders.py:232] using PyTorch data loader
I20250116 17:47:33 2013922 dinov2 loaders.py:245] # of batches: 78
I20250116 17:47:33 2013922 dinov2 knn.py:299] Start the k-NN classification.
I20250116 17:47:35 2013922 dinov2 helpers.py:102] Test:  [ 0/78]  eta: 0:02:45    time: 2.123768  data: 1.660351  max mem: 4109
I20250116 17:47:40 2013922 dinov2 helpers.py:102] Test:  [10/78]  eta: 0:00:41    time: 0.612172  data: 0.151212  max mem: 4109
I20250116 17:47:44 2013922 dinov2 helpers.py:102] Test:  [20/78]  eta: 0:00:31    time: 0.460797  data: 0.000253  max mem: 4109
I20250116 17:47:49 2013922 dinov2 helpers.py:102] Test:  [30/78]  eta: 0:00:24    time: 0.460615  data: 0.000207  max mem: 4109
I20250116 17:47:53 2013922 dinov2 helpers.py:102] Test:  [40/78]  eta: 0:00:19    time: 0.460682  data: 0.000200  max mem: 4109
I20250116 17:47:58 2013922 dinov2 helpers.py:102] Test:  [50/78]  eta: 0:00:13    time: 0.460693  data: 0.000184  max mem: 4109
I20250116 17:48:03 2013922 dinov2 helpers.py:102] Test:  [60/78]  eta: 0:00:08    time: 0.460649  data: 0.000180  max mem: 4109
I20250116 17:48:07 2013922 dinov2 helpers.py:102] Test:  [70/78]  eta: 0:00:03    time: 0.460662  data: 0.000193  max mem: 4109
I20250116 17:48:10 2013922 dinov2 helpers.py:102] Test:  [77/78]  eta: 0:00:00    time: 0.447707  data: 0.000177  max mem: 4109
I20250116 17:48:10 2013922 dinov2 helpers.py:130] Test: Total time: 0:00:37 (0.478765 s / it)
I20250116 17:48:10 2013922 dinov2 utils.py:79] Averaged stats: 
I20250116 17:48:10 2013922 dinov2 knn.py:368] ('full', 10) classifier result: Top1: 85.62
I20250116 17:48:10 2013922 dinov2 knn.py:368] ('full', 20) classifier result: Top1: 85.75
I20250116 17:48:10 2013922 dinov2 knn.py:368] ('full', 100) classifier result: Top1: 85.90
I20250116 17:48:10 2013922 dinov2 knn.py:368] ('full', 200) classifier result: Top1: 85.92
